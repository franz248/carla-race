2023-12-30 01:08:40.746453: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 01:08:40.931824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 01:08:40.931956: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 01:08:40.955246: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 01:08:41.007787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 01:08:41.629131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 01:08:42.270188: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:42.443683: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:42.444406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:42.460257: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:42.460961: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:42.461211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:43.453204: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:43.453336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:43.453433: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:08:43.453590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4395 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API 
WARNING: Client API version     = 0.9.15 
WARNING: Simulator API version  = 0.9.14 
2023-12-30 01:09:02.285379: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2023-12-30 01:09:03.832721: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fcd835aa690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-12-30 01:09:03.832735: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5
2023-12-30 01:09:03.839974: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1703916543.922047    2429 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  0%|          | 0/1000 [00:00<?, ?episode/s]
Started episode 1 of 1000
Finished training first epoch.
  0%|          | 0/1000 [02:32<?, ?episode/s]
Error message: 'Failed to add concrete function \'b\'__inference_predict_function_567252\'\' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<ResourceHandle(name="Variable/31", device="/job:localhost/replica:0/task:0/device:GPU:0", container="Anonymous", type="tensorflow::Var", dtype and shapes : "[ DType enum: 9, Shape: [] ]")>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).'
Error during episode 1
2023-12-30 01:12:10.136732: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 01:12:10.158649: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 01:12:10.158665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 01:12:10.159447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 01:12:10.162786: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 01:12:10.581169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 01:12:10.882163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:10.909659: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:10.909830: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:10.912459: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:10.912759: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:10.912971: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:11.649520: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:11.649671: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:11.649849: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 01:12:11.649930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4395 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API 
WARNING: Client API version     = 0.9.15 
WARNING: Simulator API version  = 0.9.14 
2023-12-30 01:12:24.350995: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2023-12-30 01:12:25.262343: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa650344340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-12-30 01:12:25.262357: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5
2023-12-30 01:12:25.265196: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1703916745.340169   12451 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  0%|          | 0/1000 [00:00<?, ?episode/s]
Started episode 1 of 1000
Finished training first epoch.
Finished episode 1 of 1000
Saved model from episode 1. Count of epochs trained: 252
  0%|          | 1/1000 [03:56<65:33:24, 236.24s/episode]
Started episode 2 of 1000
Finished episode 2 of 1000
Saved model from episode 2. Count of epochs trained: 588
  0%|          | 2/1000 [07:51<65:21:29, 235.76s/episode]
Started episode 3 of 1000
Finished episode 3 of 1000
Saved model from episode 3. Count of epochs trained: 917
  0%|          | 3/1000 [11:45<65:00:58, 234.76s/episode]
Started episode 4 of 1000
Finished episode 4 of 1000
Saved model from episode 4. Count of epochs trained: 1242
  0%|          | 4/1000 [15:38<64:44:18, 233.99s/episode]
Started episode 5 of 1000
Finished episode 5 of 1000
Saved model from episode 5. Count of epochs trained: 1563
  0%|          | 5/1000 [19:30<64:29:11, 233.32s/episode]
Started episode 6 of 1000
Finished episode 6 of 1000
Count of epochs trained: 1881	Goal: 2340
Count of epochs trained: 1968	Goal: 2340
Count of epochs trained: 2056	Goal: 2340
Count of epochs trained: 2144	Goal: 2340
Count of epochs trained: 2232	Goal: 2340
Count of epochs trained: 2320	Goal: 2340
Saved model from episode 6. Count of epochs trained: 2408
  1%|          | 6/1000 [29:22<98:08:13, 355.43s/episode]
Started episode 7 of 1000
Finished episode 7 of 1000
Count of epochs trained: 2713	Goal: 3185
Count of epochs trained: 2801	Goal: 3185
Count of epochs trained: 2889	Goal: 3185
Count of epochs trained: 2976	Goal: 3185
Count of epochs trained: 3060	Goal: 3185
Count of epochs trained: 3148	Goal: 3185
Saved model from episode 7. Count of epochs trained: 3237
  1%|          | 7/1000 [39:13<119:16:35, 432.42s/episode]
Started episode 8 of 1000
Finished episode 8 of 1000
Count of epochs trained: 3262	Goal: 4014
Count of epochs trained: 3350	Goal: 4014
Count of epochs trained: 3439	Goal: 4014
Count of epochs trained: 3528	Goal: 4014
Count of epochs trained: 3615	Goal: 4014
Count of epochs trained: 3704	Goal: 4014
Count of epochs trained: 3792	Goal: 4014
Count of epochs trained: 3880	Goal: 4014
Count of epochs trained: 3966	Goal: 4014
Saved model from episode 8. Count of epochs trained: 4055
  1%|          | 8/1000 [48:33<130:18:51, 472.91s/episode]
Started episode 9 of 1000
Finished episode 9 of 1000
Count of epochs trained: 4152	Goal: 4832
Count of epochs trained: 4240	Goal: 4832
Count of epochs trained: 4329	Goal: 4832
Count of epochs trained: 4417	Goal: 4832
Count of epochs trained: 4506	Goal: 4832
Count of epochs trained: 4594	Goal: 4832
Count of epochs trained: 4678	Goal: 4832
Count of epochs trained: 4766	Goal: 4832
Saved model from episode 9. Count of epochs trained: 4854
  1%|          | 9/1000 [57:47<137:09:00, 498.22s/episode]
Started episode 10 of 1000
Finished episode 10 of 1000
Count of epochs trained: 4981	Goal: 5631
Count of epochs trained: 5065	Goal: 5631
Count of epochs trained: 5150	Goal: 5631
Count of epochs trained: 5238	Goal: 5631
Count of epochs trained: 5325	Goal: 5631
Count of epochs trained: 5413	Goal: 5631
Count of epochs trained: 5500	Goal: 5631
Count of epochs trained: 5588	Goal: 5631
Saved model from episode 10. Count of epochs trained: 5676
  1%|1         | 10/1000 [1:07:24<143:44:07, 522.67s/episode]
Started episode 11 of 1000
Finished episode 11 of 1000
Count of epochs trained: 5695	Goal: 6453
Count of epochs trained: 5783	Goal: 6453
Count of epochs trained: 5870	Goal: 6453
Count of epochs trained: 5957	Goal: 6453
Count of epochs trained: 6045	Goal: 6453
Count of epochs trained: 6133	Goal: 6453
Count of epochs trained: 6221	Goal: 6453
Count of epochs trained: 6309	Goal: 6453
Count of epochs trained: 6397	Goal: 6453
Saved model from episode 11. Count of epochs trained: 6485
  1%|1         | 11/1000 [1:16:41<146:26:15, 533.04s/episode]
Started episode 12 of 1000
Finished episode 12 of 1000
Count of epochs trained: 6630	Goal: 7262
Count of epochs trained: 6718	Goal: 7262
Count of epochs trained: 6807	Goal: 7262
Count of epochs trained: 6894	Goal: 7262
Count of epochs trained: 6983	Goal: 7262
Count of epochs trained: 7070	Goal: 7262
Count of epochs trained: 7158	Goal: 7262
Count of epochs trained: 7246	Goal: 7262
Saved model from episode 12. Count of epochs trained: 7336
  1%|1         | 12/1000 [1:26:29<150:54:57, 549.90s/episode]
Started episode 13 of 1000
Finished episode 13 of 1000
Count of epochs trained: 7341	Goal: 8113
Count of epochs trained: 7429	Goal: 8113
Count of epochs trained: 7518	Goal: 8113
Count of epochs trained: 7606	Goal: 8113
Count of epochs trained: 7694	Goal: 8113
Count of epochs trained: 7782	Goal: 8113
Count of epochs trained: 7870	Goal: 8113
Count of epochs trained: 7958	Goal: 8113
Count of epochs trained: 8045	Goal: 8113
Saved model from episode 13. Count of epochs trained: 8134
  1%|1         | 13/1000 [1:35:34<150:23:30, 548.54s/episode]
Started episode 14 of 1000
Finished episode 14 of 1000
Count of epochs trained: 8142	Goal: 8911
Count of epochs trained: 8230	Goal: 8911
Count of epochs trained: 8318	Goal: 8911
Count of epochs trained: 8406	Goal: 8911
Count of epochs trained: 8494	Goal: 8911
Count of epochs trained: 8582	Goal: 8911
Count of epochs trained: 8669	Goal: 8911
Count of epochs trained: 8757	Goal: 8911
Count of epochs trained: 8845	Goal: 8911
Saved model from episode 14. Count of epochs trained: 8933
  1%|1         | 14/1000 [1:44:42<150:07:49, 548.14s/episode]
Started episode 15 of 1000
Finished episode 15 of 1000
Count of epochs trained: 9027	Goal: 9710
Count of epochs trained: 9115	Goal: 9710
Count of epochs trained: 9202	Goal: 9710
Count of epochs trained: 9290	Goal: 9710
Count of epochs trained: 9377	Goal: 9710
Count of epochs trained: 9465	Goal: 9710
Count of epochs trained: 9552	Goal: 9710
Count of epochs trained: 9640	Goal: 9710
Saved model from episode 15. Count of epochs trained: 9729
  2%|1         | 15/1000 [1:53:52<150:09:52, 548.82s/episode]
Started episode 16 of 1000
Finished episode 16 of 1000
Count of epochs trained: 9733	Goal: 10506
Count of epochs trained: 9821	Goal: 10506
Count of epochs trained: 9908	Goal: 10506
Count of epochs trained: 9997	Goal: 10506
Count of epochs trained: 10084	Goal: 10506
Count of epochs trained: 10172	Goal: 10506
Count of epochs trained: 10259	Goal: 10506
Count of epochs trained: 10346	Goal: 10506
Count of epochs trained: 10434	Goal: 10506
Saved model from episode 16. Count of epochs trained: 10522
  2%|1         | 16/1000 [2:02:57<149:39:54, 547.56s/episode]
Started episode 17 of 1000
Finished episode 17 of 1000
Count of epochs trained: 10528	Goal: 11299
Count of epochs trained: 10616	Goal: 11299
Count of epochs trained: 10704	Goal: 11299
Count of epochs trained: 10791	Goal: 11299
Count of epochs trained: 10879	Goal: 11299
Count of epochs trained: 10966	Goal: 11299
Count of epochs trained: 11053	Goal: 11299
Count of epochs trained: 11141	Goal: 11299
Count of epochs trained: 11229	Goal: 11299
Saved model from episode 17. Count of epochs trained: 11317
  2%|1         | 17/1000 [2:12:02<149:21:37, 547.00s/episode]
Started episode 18 of 1000
Finished episode 18 of 1000
Count of epochs trained: 11329	Goal: 12094
Count of epochs trained: 11416	Goal: 12094
Count of epochs trained: 11504	Goal: 12094
Count of epochs trained: 11592	Goal: 12094
Count of epochs trained: 11680	Goal: 12094
Count of epochs trained: 11767	Goal: 12094
Count of epochs trained: 11855	Goal: 12094
Count of epochs trained: 11942	Goal: 12094
Count of epochs trained: 12029	Goal: 12094
Saved model from episode 18. Count of epochs trained: 12118
  2%|1         | 18/1000 [2:21:12<149:27:03, 547.89s/episode]
Started episode 19 of 1000
Finished episode 19 of 1000
Count of epochs trained: 12225	Goal: 12895
Count of epochs trained: 12307	Goal: 12895
Count of epochs trained: 12394	Goal: 12895
Count of epochs trained: 12481	Goal: 12895
Count of epochs trained: 12569	Goal: 12895
Count of epochs trained: 12655	Goal: 12895
Count of epochs trained: 12743	Goal: 12895
Count of epochs trained: 12830	Goal: 12895
Saved model from episode 19. Count of epochs trained: 12919
  2%|1         | 19/1000 [2:30:37<150:38:25, 552.81s/episode]
Started episode 20 of 1000
Finished episode 20 of 1000
Count of epochs trained: 12928	Goal: 13696
Count of epochs trained: 13016	Goal: 13696
Count of epochs trained: 13103	Goal: 13696
Count of epochs trained: 13191	Goal: 13696
Count of epochs trained: 13279	Goal: 13696
Count of epochs trained: 13365	Goal: 13696
Count of epochs trained: 13452	Goal: 13696
Count of epochs trained: 13540	Goal: 13696
Count of epochs trained: 13629	Goal: 13696
Saved model from episode 20. Count of epochs trained: 13717
  2%|2         | 20/1000 [2:39:45<150:10:07, 551.64s/episode]
Started episode 21 of 1000
Finished episode 21 of 1000
Count of epochs trained: 13723	Goal: 14494
Count of epochs trained: 13812	Goal: 14494
Count of epochs trained: 13900	Goal: 14494
Count of epochs trained: 13989	Goal: 14494
Count of epochs trained: 14077	Goal: 14494
Count of epochs trained: 14165	Goal: 14494
Count of epochs trained: 14253	Goal: 14494
Count of epochs trained: 14342	Goal: 14494
Count of epochs trained: 14430	Goal: 14494
Saved model from episode 21. Count of epochs trained: 14519
  2%|2         | 21/1000 [2:48:52<149:35:09, 550.06s/episode]
Started episode 22 of 1000
Finished episode 22 of 1000
Count of epochs trained: 14548	Goal: 15296
Count of epochs trained: 14636	Goal: 15296
Count of epochs trained: 14723	Goal: 15296
Count of epochs trained: 14812	Goal: 15296
Count of epochs trained: 14900	Goal: 15296
Count of epochs trained: 14988	Goal: 15296
Count of epochs trained: 15074	Goal: 15296
Count of epochs trained: 15161	Goal: 15296
Count of epochs trained: 15249	Goal: 15296
Saved model from episode 22. Count of epochs trained: 15338
  2%|2         | 22/1000 [2:58:16<150:34:23, 554.26s/episode]
Started episode 23 of 1000
Finished episode 23 of 1000
Count of epochs trained: 15344	Goal: 16115
Count of epochs trained: 15432	Goal: 16115
Count of epochs trained: 15520	Goal: 16115
Count of epochs trained: 15608	Goal: 16115
Count of epochs trained: 15696	Goal: 16115
Count of epochs trained: 15784	Goal: 16115
Count of epochs trained: 15871	Goal: 16115
Count of epochs trained: 15958	Goal: 16115
Count of epochs trained: 16046	Goal: 16115
Saved model from episode 23. Count of epochs trained: 16134
  2%|2         | 23/1000 [3:07:23<149:48:17, 551.99s/episode]
Started episode 24 of 1000
Finished episode 24 of 1000
Count of epochs trained: 16141	Goal: 16911
Count of epochs trained: 16229	Goal: 16911
Count of epochs trained: 16316	Goal: 16911
Count of epochs trained: 16404	Goal: 16911
Count of epochs trained: 16492	Goal: 16911
Count of epochs trained: 16579	Goal: 16911
Count of epochs trained: 16667	Goal: 16911
Count of epochs trained: 16754	Goal: 16911
Count of epochs trained: 16842	Goal: 16911
Saved model from episode 24. Count of epochs trained: 16930
  2%|2         | 24/1000 [3:16:29<149:13:23, 550.41s/episode]
Started episode 25 of 1000
Finished episode 25 of 1000
Count of epochs trained: 16938	Goal: 17707
Count of epochs trained: 17025	Goal: 17707
Count of epochs trained: 17112	Goal: 17707
Count of epochs trained: 17200	Goal: 17707
Count of epochs trained: 17288	Goal: 17707
Count of epochs trained: 17375	Goal: 17707
Count of epochs trained: 17462	Goal: 17707
Count of epochs trained: 17550	Goal: 17707
Count of epochs trained: 17638	Goal: 17707
Saved model from episode 25. Count of epochs trained: 17726
  2%|2         | 25/1000 [3:25:37<148:48:23, 549.44s/episode]
Started episode 26 of 1000
Finished episode 26 of 1000
Count of epochs trained: 17734	Goal: 18503
Count of epochs trained: 17821	Goal: 18503
Count of epochs trained: 17909	Goal: 18503
Count of epochs trained: 17997	Goal: 18503
Count of epochs trained: 18085	Goal: 18503
Count of epochs trained: 18173	Goal: 18503
Count of epochs trained: 18262	Goal: 18503
Count of epochs trained: 18350	Goal: 18503
Count of epochs trained: 18438	Goal: 18503
Saved model from episode 26. Count of epochs trained: 18527
  3%|2         | 26/1000 [3:34:43<148:26:23, 548.65s/episode]
Started episode 27 of 1000
Finished episode 27 of 1000
Count of epochs trained: 18823	Goal: 19304
Count of epochs trained: 18911	Goal: 19304
Count of epochs trained: 18999	Goal: 19304
Count of epochs trained: 19087	Goal: 19304
Count of epochs trained: 19175	Goal: 19304
Count of epochs trained: 19263	Goal: 19304
Saved model from episode 27. Count of epochs trained: 19351
  3%|2         | 27/1000 [3:44:30<151:21:38, 560.02s/episode]
Started episode 28 of 1000
Finished episode 28 of 1000
Count of epochs trained: 19646	Goal: 20128
Count of epochs trained: 19734	Goal: 20128
Count of epochs trained: 19822	Goal: 20128
Count of epochs trained: 19910	Goal: 20128
Count of epochs trained: 19997	Goal: 20128
Count of epochs trained: 20085	Goal: 20128
Saved model from episode 28. Count of epochs trained: 20173
  3%|2         | 28/1000 [3:54:16<153:19:09, 567.85s/episode]
Started episode 29 of 1000
Finished episode 29 of 1000
Count of epochs trained: 20341	Goal: 20950
Count of epochs trained: 20429	Goal: 20950
Count of epochs trained: 20516	Goal: 20950
Count of epochs trained: 20604	Goal: 20950
Count of epochs trained: 20692	Goal: 20950
Count of epochs trained: 20780	Goal: 20950
Count of epochs trained: 20868	Goal: 20950
Saved model from episode 29. Count of epochs trained: 20956
  3%|2         | 29/1000 [4:03:26<151:43:39, 562.53s/episode]
Started episode 30 of 1000
  3%|2         | 29/1000 [4:04:53<136:39:46, 506.68s/episode]
Error message: 'Failed to add concrete function \'b\'__inference_predict_function_76357216\'\' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<ResourceHandle(name="Variable/31", device="/job:localhost/replica:0/task:0/device:GPU:0", container="Anonymous", type="tensorflow::Var", dtype and shapes : "[ DType enum: 9, Shape: [] ]")>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).'
Error during episode 30
2023-12-30 05:17:52.243444: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 05:17:52.265278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 05:17:52.265297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 05:17:52.265838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 05:17:52.269152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 05:17:52.670312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 05:17:52.979909: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.008264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.008469: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.011496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.011796: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.011999: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.729573: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.729704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.729800: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 05:17:53.729909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4422 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Models in tmp ['tmp/001.252.model', 'tmp/002.587.model', 'tmp/003.916.model', 'tmp/004.1241.model', 'tmp/005.1562.model', 'tmp/006.2408.model', 'tmp/007.3237.model', 'tmp/008.4054.model', 'tmp/009.4853.model', 'tmp/010.5676.model', 'tmp/011.6484.model', 'tmp/012.7335.model', 'tmp/013.8133.model', 'tmp/014.8932.model', 'tmp/015.9728.model', 'tmp/016.10521.model', 'tmp/017.11317.model', 'tmp/018.12117.model', 'tmp/019.12918.model', 'tmp/020.13717.model', 'tmp/021.14518.model', 'tmp/022.15337.model', 'tmp/023.16134.model', 'tmp/024.16930.model', 'tmp/025.17726.model', 'tmp/026.18526.model', 'tmp/027.19351.model', 'tmp/028.20173.model', 'tmp/029.20956.model']
Load model tmp/029.20956.model
Leftover images from failed episode: ['_out_16rl_custom2/030_037919.png', '_out_16rl_custom2/030_037920.png', '_out_16rl_custom2/030_037921.png', '_out_16rl_custom2/030_037922.png', '_out_16rl_custom2/030_037923.png', '_out_16rl_custom2/030_037924.png', '_out_16rl_custom2/030_037925.png', '_out_16rl_custom2/030_037927.png', '_out_16rl_custom2/030_037928.png', '_out_16rl_custom2/030_037929.png', '_out_16rl_custom2/030_037930.png', '_out_16rl_custom2/030_037931.png', '_out_16rl_custom2/030_037932.png', '_out_16rl_custom2/030_037934.png', '_out_16rl_custom2/030_037935.png', '_out_16rl_custom2/030_037936.png', '_out_16rl_custom2/030_037938.png', '_out_16rl_custom2/030_037939.png', '_out_16rl_custom2/030_037940.png', '_out_16rl_custom2/030_037941.png', '_out_16rl_custom2/030_037942.png', '_out_16rl_custom2/030_037944.png', '_out_16rl_custom2/030_037945.png', '_out_16rl_custom2/030_037947.png', '_out_16rl_custom2/030_037949.png', '_out_16rl_custom2/030_037950.png', '_out_16rl_custom2/030_037951.png', '_out_16rl_custom2/030_037952.png', '_out_16rl_custom2/030_037953.png', '_out_16rl_custom2/030_037954.png', '_out_16rl_custom2/030_037955.png', '_out_16rl_custom2/030_037956.png', '_out_16rl_custom2/030_037957.png', '_out_16rl_custom2/030_037959.png', '_out_16rl_custom2/030_037960.png', '_out_16rl_custom2/030_037961.png', '_out_16rl_custom2/030_037963.png', '_out_16rl_custom2/030_037964.png', '_out_16rl_custom2/030_037965.png', '_out_16rl_custom2/030_037966.png', '_out_16rl_custom2/030_037967.png', '_out_16rl_custom2/030_037969.png', '_out_16rl_custom2/030_037970.png', '_out_16rl_custom2/030_037971.png', '_out_16rl_custom2/030_037972.png', '_out_16rl_custom2/030_037974.png', '_out_16rl_custom2/030_037975.png', '_out_16rl_custom2/030_037977.png', '_out_16rl_custom2/030_037979.png', '_out_16rl_custom2/030_037980.png', '_out_16rl_custom2/030_037981.png', '_out_16rl_custom2/030_037983.png', '_out_16rl_custom2/030_037984.png', '_out_16rl_custom2/030_037985.png', '_out_16rl_custom2/030_037986.png', '_out_16rl_custom2/030_037987.png', '_out_16rl_custom2/030_037988.png', '_out_16rl_custom2/030_037990.png', '_out_16rl_custom2/030_037991.png', '_out_16rl_custom2/030_037992.png', '_out_16rl_custom2/030_037993.png', '_out_16rl_custom2/030_037994.png', '_out_16rl_custom2/030_037995.png', '_out_16rl_custom2/030_037997.png', '_out_16rl_custom2/030_037998.png', '_out_16rl_custom2/030_037999.png', '_out_16rl_custom2/030_038000.png', '_out_16rl_custom2/030_038001.png', '_out_16rl_custom2/030_038002.png', '_out_16rl_custom2/030_038003.png', '_out_16rl_custom2/030_038004.png', '_out_16rl_custom2/030_038006.png', '_out_16rl_custom2/030_038007.png', '_out_16rl_custom2/030_038008.png', '_out_16rl_custom2/030_038009.png', '_out_16rl_custom2/030_038011.png', '_out_16rl_custom2/030_038013.png', '_out_16rl_custom2/030_038014.png', '_out_16rl_custom2/030_038016.png', '_out_16rl_custom2/030_038018.png', '_out_16rl_custom2/030_038019.png', '_out_16rl_custom2/030_038020.png', '_out_16rl_custom2/030_038021.png', '_out_16rl_custom2/030_038022.png', '_out_16rl_custom2/030_038023.png', '_out_16rl_custom2/030_038024.png', '_out_16rl_custom2/030_038025.png', '_out_16rl_custom2/030_038026.png', '_out_16rl_custom2/030_038027.png', '_out_16rl_custom2/030_038028.png', '_out_16rl_custom2/030_038029.png', '_out_16rl_custom2/030_038031.png', '_out_16rl_custom2/030_038032.png', '_out_16rl_custom2/030_038033.png', '_out_16rl_custom2/030_038034.png', '_out_16rl_custom2/030_038035.png', '_out_16rl_custom2/030_038036.png', '_out_16rl_custom2/030_038037.png', '_out_16rl_custom2/030_038039.png', '_out_16rl_custom2/030_038040.png', '_out_16rl_custom2/030_038041.png', '_out_16rl_custom2/030_038042.png', '_out_16rl_custom2/030_038043.png', '_out_16rl_custom2/030_038045.png', '_out_16rl_custom2/030_038046.png', '_out_16rl_custom2/030_038047.png', '_out_16rl_custom2/030_038048.png', '_out_16rl_custom2/030_038049.png', '_out_16rl_custom2/030_038050.png', '_out_16rl_custom2/030_038051.png', '_out_16rl_custom2/030_038053.png', '_out_16rl_custom2/030_038054.png', '_out_16rl_custom2/030_038056.png', '_out_16rl_custom2/030_038057.png', '_out_16rl_custom2/030_038058.png', '_out_16rl_custom2/030_038059.png', '_out_16rl_custom2/030_038060.png', '_out_16rl_custom2/030_038061.png', '_out_16rl_custom2/030_038063.png', '_out_16rl_custom2/030_038064.png', '_out_16rl_custom2/030_038066.png', '_out_16rl_custom2/030_038067.png', '_out_16rl_custom2/030_038068.png', '_out_16rl_custom2/030_038069.png', '_out_16rl_custom2/030_038071.png', '_out_16rl_custom2/030_038072.png', '_out_16rl_custom2/030_038073.png', '_out_16rl_custom2/030_038074.png', '_out_16rl_custom2/030_038075.png', '_out_16rl_custom2/030_038077.png', '_out_16rl_custom2/030_038078.png', '_out_16rl_custom2/030_038079.png', '_out_16rl_custom2/030_038080.png', '_out_16rl_custom2/030_038081.png', '_out_16rl_custom2/030_038082.png', '_out_16rl_custom2/030_038083.png', '_out_16rl_custom2/030_038084.png', '_out_16rl_custom2/030_038086.png', '_out_16rl_custom2/030_038088.png', '_out_16rl_custom2/030_038089.png', '_out_16rl_custom2/030_038091.png', '_out_16rl_custom2/030_038092.png', '_out_16rl_custom2/030_038093.png', '_out_16rl_custom2/030_038094.png', '_out_16rl_custom2/030_038095.png', '_out_16rl_custom2/030_038096.png', '_out_16rl_custom2/030_038097.png', '_out_16rl_custom2/030_038099.png', '_out_16rl_custom2/030_038100.png', '_out_16rl_custom2/030_038101.png', '_out_16rl_custom2/030_038102.png', '_out_16rl_custom2/030_038103.png', '_out_16rl_custom2/030_038104.png', '_out_16rl_custom2/030_038106.png', '_out_16rl_custom2/030_038107.png', '_out_16rl_custom2/030_038108.png', '_out_16rl_custom2/030_038109.png', '_out_16rl_custom2/030_038110.png', '_out_16rl_custom2/030_038111.png', '_out_16rl_custom2/030_038113.png', '_out_16rl_custom2/030_038114.png', '_out_16rl_custom2/030_038115.png', '_out_16rl_custom2/030_038116.png', '_out_16rl_custom2/030_038117.png', '_out_16rl_custom2/030_038118.png', '_out_16rl_custom2/030_038120.png', '_out_16rl_custom2/030_038122.png', '_out_16rl_custom2/030_038123.png', '_out_16rl_custom2/030_038125.png', '_out_16rl_custom2/030_038126.png', '_out_16rl_custom2/030_038127.png', '_out_16rl_custom2/030_038128.png', '_out_16rl_custom2/030_038129.png', '_out_16rl_custom2/030_038130.png', '_out_16rl_custom2/030_038131.png', '_out_16rl_custom2/030_038132.png', '_out_16rl_custom2/030_038134.png', '_out_16rl_custom2/030_038135.png', '_out_16rl_custom2/030_038136.png', '_out_16rl_custom2/030_038137.png', '_out_16rl_custom2/030_038138.png', '_out_16rl_custom2/030_038139.png', '_out_16rl_custom2/030_038140.png', '_out_16rl_custom2/030_038141.png', '_out_16rl_custom2/030_038142.png', '_out_16rl_custom2/030_038143.png', '_out_16rl_custom2/030_038144.png', '_out_16rl_custom2/030_038145.png', '_out_16rl_custom2/030_038146.png', '_out_16rl_custom2/030_038147.png', '_out_16rl_custom2/030_038148.png', '_out_16rl_custom2/030_038149.png', '_out_16rl_custom2/030_038150.png', '_out_16rl_custom2/030_038151.png', '_out_16rl_custom2/030_038152.png', '_out_16rl_custom2/030_038153.png', '_out_16rl_custom2/030_038154.png', '_out_16rl_custom2/030_038155.png', '_out_16rl_custom2/030_038156.png', '_out_16rl_custom2/030_038158.png', '_out_16rl_custom2/030_038159.png', '_out_16rl_custom2/030_038160.png', '_out_16rl_custom2/030_038161.png', '_out_16rl_custom2/030_038162.png', '_out_16rl_custom2/030_038163.png', '_out_16rl_custom2/030_038164.png', '_out_16rl_custom2/030_038165.png', '_out_16rl_custom2/030_038166.png', '_out_16rl_custom2/030_038167.png', '_out_16rl_custom2/030_038168.png', '_out_16rl_custom2/030_038170.png', '_out_16rl_custom2/030_038171.png', '_out_16rl_custom2/030_038172.png', '_out_16rl_custom2/030_038174.png', '_out_16rl_custom2/030_038175.png', '_out_16rl_custom2/030_038177.png', '_out_16rl_custom2/030_038178.png', '_out_16rl_custom2/030_038180.png', '_out_16rl_custom2/030_038181.png', '_out_16rl_custom2/030_038182.png', '_out_16rl_custom2/030_038183.png', '_out_16rl_custom2/030_038184.png', '_out_16rl_custom2/030_038185.png', '_out_16rl_custom2/030_038186.png', '_out_16rl_custom2/030_038188.png', '_out_16rl_custom2/030_038189.png', '_out_16rl_custom2/030_038190.png', '_out_16rl_custom2/030_038191.png', '_out_16rl_custom2/030_038192.png', '_out_16rl_custom2/030_038193.png', '_out_16rl_custom2/030_038194.png', '_out_16rl_custom2/030_038196.png', '_out_16rl_custom2/030_038197.png', '_out_16rl_custom2/030_038199.png', '_out_16rl_custom2/030_038201.png', '_out_16rl_custom2/030_038202.png', '_out_16rl_custom2/030_038203.png', '_out_16rl_custom2/030_038204.png', '_out_16rl_custom2/030_038205.png', '_out_16rl_custom2/030_038206.png', '_out_16rl_custom2/030_038207.png', '_out_16rl_custom2/030_038208.png', '_out_16rl_custom2/030_038209.png', '_out_16rl_custom2/030_038210.png', '_out_16rl_custom2/030_038212.png', '_out_16rl_custom2/030_038213.png', '_out_16rl_custom2/030_038214.png', '_out_16rl_custom2/030_038216.png', '_out_16rl_custom2/030_038217.png', '_out_16rl_custom2/030_038218.png', '_out_16rl_custom2/030_038219.png', '_out_16rl_custom2/030_038220.png', '_out_16rl_custom2/030_038221.png', '_out_16rl_custom2/030_038222.png', '_out_16rl_custom2/030_038223.png', '_out_16rl_custom2/030_038224.png', '_out_16rl_custom2/030_038225.png', '_out_16rl_custom2/030_038226.png', '_out_16rl_custom2/030_038227.png', '_out_16rl_custom2/030_038228.png', '_out_16rl_custom2/030_038230.png', '_out_16rl_custom2/030_038231.png', '_out_16rl_custom2/030_038232.png', '_out_16rl_custom2/030_038233.png', '_out_16rl_custom2/030_038234.png', '_out_16rl_custom2/030_038235.png', '_out_16rl_custom2/030_038236.png', '_out_16rl_custom2/030_038238.png', '_out_16rl_custom2/030_038239.png', '_out_16rl_custom2/030_038240.png', '_out_16rl_custom2/030_038241.png', '_out_16rl_custom2/030_038242.png', '_out_16rl_custom2/030_038244.png', '_out_16rl_custom2/030_038245.png', '_out_16rl_custom2/030_038246.png', '_out_16rl_custom2/030_038247.png', '_out_16rl_custom2/030_038248.png', '_out_16rl_custom2/030_038249.png', '_out_16rl_custom2/030_038250.png', '_out_16rl_custom2/030_038251.png', '_out_16rl_custom2/030_038252.png', '_out_16rl_custom2/030_038253.png', '_out_16rl_custom2/030_038254.png', '_out_16rl_custom2/030_038255.png', '_out_16rl_custom2/030_038256.png', '_out_16rl_custom2/030_038257.png', '_out_16rl_custom2/030_038259.png', '_out_16rl_custom2/030_038260.png', '_out_16rl_custom2/030_038261.png', '_out_16rl_custom2/030_038263.png', '_out_16rl_custom2/030_038264.png', '_out_16rl_custom2/030_038265.png', '_out_16rl_custom2/030_038267.png', '_out_16rl_custom2/030_038268.png', '_out_16rl_custom2/030_038269.png', '_out_16rl_custom2/030_038270.png', '_out_16rl_custom2/030_038271.png', '_out_16rl_custom2/030_038272.png', '_out_16rl_custom2/030_038273.png', '_out_16rl_custom2/030_038274.png', '_out_16rl_custom2/030_038275.png', '_out_16rl_custom2/030_038276.png', '_out_16rl_custom2/030_038277.png', '_out_16rl_custom2/030_038278.png', '_out_16rl_custom2/030_038279.png', '_out_16rl_custom2/030_038280.png', '_out_16rl_custom2/030_038281.png', '_out_16rl_custom2/030_038283.png', '_out_16rl_custom2/030_038284.png', '_out_16rl_custom2/030_038285.png', '_out_16rl_custom2/030_038286.png', '_out_16rl_custom2/030_038288.png', '_out_16rl_custom2/030_038289.png', '_out_16rl_custom2/030_038290.png', '_out_16rl_custom2/030_038291.png', '_out_16rl_custom2/030_038292.png', '_out_16rl_custom2/030_038293.png', '_out_16rl_custom2/030_038294.png', '_out_16rl_custom2/030_038295.png', '_out_16rl_custom2/030_038297.png', '_out_16rl_custom2/030_038299.png', '_out_16rl_custom2/030_038300.png', '_out_16rl_custom2/030_038301.png', '_out_16rl_custom2/030_038302.png', '_out_16rl_custom2/030_038304.png', '_out_16rl_custom2/030_038305.png', '_out_16rl_custom2/030_038306.png', '_out_16rl_custom2/030_038307.png', '_out_16rl_custom2/030_038308.png', '_out_16rl_custom2/030_038309.png', '_out_16rl_custom2/030_038310.png', '_out_16rl_custom2/030_038311.png', '_out_16rl_custom2/030_038313.png', '_out_16rl_custom2/030_038314.png', '_out_16rl_custom2/030_038315.png', '_out_16rl_custom2/030_038316.png', '_out_16rl_custom2/030_038317.png', '_out_16rl_custom2/030_038318.png', '_out_16rl_custom2/030_038319.png', '_out_16rl_custom2/030_038320.png', '_out_16rl_custom2/030_038321.png', '_out_16rl_custom2/030_038322.png', '_out_16rl_custom2/030_038324.png', '_out_16rl_custom2/030_038325.png', '_out_16rl_custom2/030_038326.png', '_out_16rl_custom2/030_038327.png', '_out_16rl_custom2/030_038328.png', '_out_16rl_custom2/030_038330.png', '_out_16rl_custom2/030_038331.png', '_out_16rl_custom2/030_038332.png', '_out_16rl_custom2/030_038333.png', '_out_16rl_custom2/030_038334.png', '_out_16rl_custom2/030_038335.png', '_out_16rl_custom2/030_038336.png', '_out_16rl_custom2/030_038337.png', '_out_16rl_custom2/030_038338.png', '_out_16rl_custom2/030_038339.png', '_out_16rl_custom2/030_038340.png', '_out_16rl_custom2/030_038341.png', '_out_16rl_custom2/030_038343.png', '_out_16rl_custom2/030_038344.png', '_out_16rl_custom2/030_038345.png', '_out_16rl_custom2/030_038346.png', '_out_16rl_custom2/030_038347.png', '_out_16rl_custom2/030_038348.png', '_out_16rl_custom2/030_038349.png', '_out_16rl_custom2/030_038350.png', '_out_16rl_custom2/030_038351.png', '_out_16rl_custom2/030_038352.png', '_out_16rl_custom2/030_038353.png', '_out_16rl_custom2/030_038355.png', '_out_16rl_custom2/030_038356.png', '_out_16rl_custom2/030_038357.png', '_out_16rl_custom2/030_038358.png', '_out_16rl_custom2/030_038359.png', '_out_16rl_custom2/030_038360.png', '_out_16rl_custom2/030_038361.png', '_out_16rl_custom2/030_038362.png', '_out_16rl_custom2/030_038363.png', '_out_16rl_custom2/030_038364.png', '_out_16rl_custom2/030_038366.png', '_out_16rl_custom2/030_038367.png', '_out_16rl_custom2/030_038369.png', '_out_16rl_custom2/030_038370.png', '_out_16rl_custom2/030_038372.png', '_out_16rl_custom2/030_038373.png', '_out_16rl_custom2/030_038375.png', '_out_16rl_custom2/030_038376.png', '_out_16rl_custom2/030_038377.png', '_out_16rl_custom2/030_038378.png', '_out_16rl_custom2/030_038379.png', '_out_16rl_custom2/030_038381.png', '_out_16rl_custom2/030_038382.png', '_out_16rl_custom2/030_038383.png', '_out_16rl_custom2/030_038385.png', '_out_16rl_custom2/030_038387.png', '_out_16rl_custom2/030_038388.png', '_out_16rl_custom2/030_038390.png', '_out_16rl_custom2/030_038391.png', '_out_16rl_custom2/030_038392.png', '_out_16rl_custom2/030_038393.png', '_out_16rl_custom2/030_038394.png', '_out_16rl_custom2/030_038396.png', '_out_16rl_custom2/030_038397.png', '_out_16rl_custom2/030_038398.png', '_out_16rl_custom2/030_038400.png', '_out_16rl_custom2/030_038401.png', '_out_16rl_custom2/030_038402.png', '_out_16rl_custom2/030_038403.png', '_out_16rl_custom2/030_038404.png', '_out_16rl_custom2/030_038405.png', '_out_16rl_custom2/030_038406.png', '_out_16rl_custom2/030_038407.png', '_out_16rl_custom2/030_038408.png', '_out_16rl_custom2/030_038410.png', '_out_16rl_custom2/030_038412.png', '_out_16rl_custom2/030_038413.png', '_out_16rl_custom2/030_038414.png', '_out_16rl_custom2/030_038415.png', '_out_16rl_custom2/030_038416.png', '_out_16rl_custom2/030_038418.png', '_out_16rl_custom2/030_038420.png', '_out_16rl_custom2/030_038421.png', '_out_16rl_custom2/030_038422.png', '_out_16rl_custom2/030_038423.png', '_out_16rl_custom2/030_038425.png', '_out_16rl_custom2/030_038427.png', '_out_16rl_custom2/030_038428.png', '_out_16rl_custom2/030_038429.png', '_out_16rl_custom2/030_038430.png', '_out_16rl_custom2/030_038432.png', '_out_16rl_custom2/030_038433.png', '_out_16rl_custom2/030_038434.png', '_out_16rl_custom2/030_038435.png', '_out_16rl_custom2/030_038436.png', '_out_16rl_custom2/030_038437.png', '_out_16rl_custom2/030_038438.png', '_out_16rl_custom2/030_038439.png', '_out_16rl_custom2/030_038440.png', '_out_16rl_custom2/030_038441.png', '_out_16rl_custom2/030_038442.png', '_out_16rl_custom2/030_038443.png', '_out_16rl_custom2/030_038444.png', '_out_16rl_custom2/030_038445.png', '_out_16rl_custom2/030_038446.png', '_out_16rl_custom2/030_038447.png', '_out_16rl_custom2/030_038448.png', '_out_16rl_custom2/030_038449.png', '_out_16rl_custom2/030_038450.png', '_out_16rl_custom2/030_038451.png', '_out_16rl_custom2/030_038452.png', '_out_16rl_custom2/030_038453.png', '_out_16rl_custom2/030_038455.png', '_out_16rl_custom2/030_038456.png', '_out_16rl_custom2/030_038457.png', '_out_16rl_custom2/030_038458.png', '_out_16rl_custom2/030_038459.png', '_out_16rl_custom2/030_038460.png', '_out_16rl_custom2/030_038461.png', '_out_16rl_custom2/030_038463.png', '_out_16rl_custom2/030_038464.png', '_out_16rl_custom2/030_038465.png', '_out_16rl_custom2/030_038466.png', '_out_16rl_custom2/030_038467.png', '_out_16rl_custom2/030_038468.png', '_out_16rl_custom2/030_038469.png', '_out_16rl_custom2/030_038471.png', '_out_16rl_custom2/030_038472.png', '_out_16rl_custom2/030_038473.png', '_out_16rl_custom2/030_038474.png', '_out_16rl_custom2/030_038475.png', '_out_16rl_custom2/030_038476.png', '_out_16rl_custom2/030_038477.png', '_out_16rl_custom2/030_038478.png', '_out_16rl_custom2/030_038479.png', '_out_16rl_custom2/030_038480.png', '_out_16rl_custom2/030_038481.png', '_out_16rl_custom2/030_038482.png', '_out_16rl_custom2/030_038483.png', '_out_16rl_custom2/030_038484.png', '_out_16rl_custom2/030_038485.png', '_out_16rl_custom2/030_038486.png', '_out_16rl_custom2/030_038487.png', '_out_16rl_custom2/030_038488.png', '_out_16rl_custom2/030_038489.png', '_out_16rl_custom2/030_038490.png', '_out_16rl_custom2/030_038491.png', '_out_16rl_custom2/030_038492.png', '_out_16rl_custom2/030_038493.png', '_out_16rl_custom2/030_038495.png', '_out_16rl_custom2/030_038496.png', '_out_16rl_custom2/030_038497.png', '_out_16rl_custom2/030_038498.png', '_out_16rl_custom2/030_038499.png', '_out_16rl_custom2/030_038500.png', '_out_16rl_custom2/030_038501.png', '_out_16rl_custom2/030_038502.png', '_out_16rl_custom2/030_038503.png', '_out_16rl_custom2/030_038505.png', '_out_16rl_custom2/030_038506.png', '_out_16rl_custom2/030_038508.png', '_out_16rl_custom2/030_038509.png', '_out_16rl_custom2/030_038510.png', '_out_16rl_custom2/030_038511.png', '_out_16rl_custom2/030_038512.png', '_out_16rl_custom2/030_038513.png', '_out_16rl_custom2/030_038514.png', '_out_16rl_custom2/030_038516.png', '_out_16rl_custom2/030_038517.png', '_out_16rl_custom2/030_038518.png', '_out_16rl_custom2/030_038519.png', '_out_16rl_custom2/030_038520.png', '_out_16rl_custom2/030_038521.png', '_out_16rl_custom2/030_038522.png', '_out_16rl_custom2/030_038523.png', '_out_16rl_custom2/030_038524.png', '_out_16rl_custom2/030_038526.png', '_out_16rl_custom2/030_038528.png', '_out_16rl_custom2/030_038529.png', '_out_16rl_custom2/030_038530.png', '_out_16rl_custom2/030_038531.png', '_out_16rl_custom2/030_038532.png', '_out_16rl_custom2/030_038533.png', '_out_16rl_custom2/030_038535.png', '_out_16rl_custom2/030_038536.png', '_out_16rl_custom2/030_038537.png', '_out_16rl_custom2/030_038538.png', '_out_16rl_custom2/030_038540.png', '_out_16rl_custom2/030_038541.png', '_out_16rl_custom2/030_038542.png', '_out_16rl_custom2/030_038543.png', '_out_16rl_custom2/030_038544.png', '_out_16rl_custom2/030_038546.png', '_out_16rl_custom2/030_038548.png', '_out_16rl_custom2/030_038549.png', '_out_16rl_custom2/030_038550.png', '_out_16rl_custom2/030_038552.png', '_out_16rl_custom2/030_038553.png', '_out_16rl_custom2/030_038555.png', '_out_16rl_custom2/030_038556.png', '_out_16rl_custom2/030_038557.png', '_out_16rl_custom2/030_038558.png', '_out_16rl_custom2/030_038559.png', '_out_16rl_custom2/030_038560.png', '_out_16rl_custom2/030_038561.png', '_out_16rl_custom2/030_038562.png', '_out_16rl_custom2/030_038564.png', '_out_16rl_custom2/030_038565.png', '_out_16rl_custom2/030_038566.png', '_out_16rl_custom2/030_038568.png', '_out_16rl_custom2/030_038570.png', '_out_16rl_custom2/030_038571.png', '_out_16rl_custom2/030_038573.png', '_out_16rl_custom2/030_038574.png', '_out_16rl_custom2/030_038575.png', '_out_16rl_custom2/030_038576.png', '_out_16rl_custom2/030_038577.png', '_out_16rl_custom2/030_038578.png', '_out_16rl_custom2/030_038579.png', '_out_16rl_custom2/030_038580.png', '_out_16rl_custom2/030_038581.png', '_out_16rl_custom2/030_038583.png', '_out_16rl_custom2/030_038585.png', '_out_16rl_custom2/030_038586.png', '_out_16rl_custom2/030_038587.png', '_out_16rl_custom2/030_038588.png', '_out_16rl_custom2/030_038590.png', '_out_16rl_custom2/030_038591.png', '_out_16rl_custom2/030_038593.png', '_out_16rl_custom2/030_038594.png', '_out_16rl_custom2/030_038596.png', '_out_16rl_custom2/030_038597.png', '_out_16rl_custom2/030_038598.png', '_out_16rl_custom2/030_038599.png', '_out_16rl_custom2/030_038600.png', '_out_16rl_custom2/030_038601.png', '_out_16rl_custom2/030_038603.png', '_out_16rl_custom2/030_038604.png', '_out_16rl_custom2/030_038605.png', '_out_16rl_custom2/030_038606.png', '_out_16rl_custom2/030_038607.png', '_out_16rl_custom2/030_038608.png', '_out_16rl_custom2/030_038610.png', '_out_16rl_custom2/030_038611.png', '_out_16rl_custom2/030_038613.png', '_out_16rl_custom2/030_038615.png', '_out_16rl_custom2/030_038616.png', '_out_16rl_custom2/030_038617.png', '_out_16rl_custom2/030_038618.png', '_out_16rl_custom2/030_038620.png', '_out_16rl_custom2/030_038621.png', '_out_16rl_custom2/030_038622.png', '_out_16rl_custom2/030_038624.png', '_out_16rl_custom2/030_038625.png', '_out_16rl_custom2/030_038626.png', '_out_16rl_custom2/030_038627.png', '_out_16rl_custom2/030_038628.png', '_out_16rl_custom2/030_038629.png', '_out_16rl_custom2/030_038630.png', '_out_16rl_custom2/030_038631.png', '_out_16rl_custom2/030_038632.png', '_out_16rl_custom2/030_038633.png', '_out_16rl_custom2/030_038634.png', '_out_16rl_custom2/030_038636.png', '_out_16rl_custom2/030_038637.png', '_out_16rl_custom2/030_038638.png', '_out_16rl_custom2/030_038639.png', '_out_16rl_custom2/030_038640.png', '_out_16rl_custom2/030_038641.png', '_out_16rl_custom2/030_038642.png', '_out_16rl_custom2/030_038643.png', '_out_16rl_custom2/030_038644.png', '_out_16rl_custom2/030_038645.png', '_out_16rl_custom2/030_038647.png', '_out_16rl_custom2/030_038648.png', '_out_16rl_custom2/030_038649.png', '_out_16rl_custom2/030_038650.png', '_out_16rl_custom2/030_038652.png', '_out_16rl_custom2/030_038653.png', '_out_16rl_custom2/030_038655.png', '_out_16rl_custom2/030_038656.png', '_out_16rl_custom2/030_038657.png', '_out_16rl_custom2/030_038659.png', '_out_16rl_custom2/030_038660.png', '_out_16rl_custom2/030_038661.png', '_out_16rl_custom2/030_038662.png', '_out_16rl_custom2/030_038663.png', '_out_16rl_custom2/030_038664.png', '_out_16rl_custom2/030_038665.png', '_out_16rl_custom2/030_038666.png', '_out_16rl_custom2/030_038667.png', '_out_16rl_custom2/030_038669.png', '_out_16rl_custom2/030_038670.png', '_out_16rl_custom2/030_038671.png', '_out_16rl_custom2/030_038673.png', '_out_16rl_custom2/030_038674.png', '_out_16rl_custom2/030_038675.png', '_out_16rl_custom2/030_038676.png', '_out_16rl_custom2/030_038677.png', '_out_16rl_custom2/030_038678.png', '_out_16rl_custom2/030_038679.png', '_out_16rl_custom2/030_038680.png', '_out_16rl_custom2/030_038682.png', '_out_16rl_custom2/030_038683.png', '_out_16rl_custom2/030_038685.png', '_out_16rl_custom2/030_038686.png', '_out_16rl_custom2/030_038687.png', '_out_16rl_custom2/030_038688.png', '_out_16rl_custom2/030_038689.png', '_out_16rl_custom2/030_038690.png', '_out_16rl_custom2/030_038691.png', '_out_16rl_custom2/030_038692.png', '_out_16rl_custom2/030_038693.png', '_out_16rl_custom2/030_038694.png', '_out_16rl_custom2/030_038695.png', '_out_16rl_custom2/030_038696.png', '_out_16rl_custom2/030_038697.png', '_out_16rl_custom2/030_038698.png', '_out_16rl_custom2/030_038700.png', '_out_16rl_custom2/030_038701.png', '_out_16rl_custom2/030_038702.png', '_out_16rl_custom2/030_038704.png', '_out_16rl_custom2/030_038705.png', '_out_16rl_custom2/030_038706.png', '_out_16rl_custom2/030_038707.png', '_out_16rl_custom2/030_038708.png', '_out_16rl_custom2/030_038709.png', '_out_16rl_custom2/030_038710.png', '_out_16rl_custom2/030_038711.png', '_out_16rl_custom2/030_038712.png', '_out_16rl_custom2/030_038713.png', '_out_16rl_custom2/030_038715.png', '_out_16rl_custom2/030_038717.png', '_out_16rl_custom2/030_038718.png', '_out_16rl_custom2/030_038720.png', '_out_16rl_custom2/030_038721.png', '_out_16rl_custom2/030_038722.png', '_out_16rl_custom2/030_038723.png', '_out_16rl_custom2/030_038724.png', '_out_16rl_custom2/030_038725.png', '_out_16rl_custom2/030_038726.png', '_out_16rl_custom2/030_038727.png', '_out_16rl_custom2/030_038728.png', '_out_16rl_custom2/030_038729.png', '_out_16rl_custom2/030_038731.png', '_out_16rl_custom2/030_038732.png', '_out_16rl_custom2/030_038734.png', '_out_16rl_custom2/030_038735.png', '_out_16rl_custom2/030_038736.png', '_out_16rl_custom2/030_038737.png', '_out_16rl_custom2/030_038738.png', '_out_16rl_custom2/030_038740.png', '_out_16rl_custom2/030_038741.png', '_out_16rl_custom2/030_038742.png', '_out_16rl_custom2/030_038743.png', '_out_16rl_custom2/030_038744.png', '_out_16rl_custom2/030_038745.png', '_out_16rl_custom2/030_038746.png', '_out_16rl_custom2/030_038747.png', '_out_16rl_custom2/030_038748.png', '_out_16rl_custom2/030_038749.png', '_out_16rl_custom2/030_038750.png', '_out_16rl_custom2/030_038751.png', '_out_16rl_custom2/030_038753.png', '_out_16rl_custom2/030_038754.png', '_out_16rl_custom2/030_038755.png', '_out_16rl_custom2/030_038756.png', '_out_16rl_custom2/030_038757.png', '_out_16rl_custom2/030_038759.png', '_out_16rl_custom2/030_038760.png', '_out_16rl_custom2/030_038761.png', '_out_16rl_custom2/030_038762.png', '_out_16rl_custom2/030_038764.png', '_out_16rl_custom2/030_038765.png', '_out_16rl_custom2/030_038766.png', '_out_16rl_custom2/030_038768.png', '_out_16rl_custom2/030_038769.png', '_out_16rl_custom2/030_038770.png', '_out_16rl_custom2/030_038771.png', '_out_16rl_custom2/030_038772.png', '_out_16rl_custom2/030_038773.png', '_out_16rl_custom2/030_038774.png', '_out_16rl_custom2/030_038775.png', '_out_16rl_custom2/030_038776.png', '_out_16rl_custom2/030_038777.png', '_out_16rl_custom2/030_038778.png', '_out_16rl_custom2/030_038780.png', '_out_16rl_custom2/030_038781.png', '_out_16rl_custom2/030_038782.png', '_out_16rl_custom2/030_038783.png', '_out_16rl_custom2/030_038784.png', '_out_16rl_custom2/030_038785.png', '_out_16rl_custom2/030_038786.png', '_out_16rl_custom2/030_038787.png', '_out_16rl_custom2/030_038788.png', '_out_16rl_custom2/030_038789.png', '_out_16rl_custom2/030_038790.png', '_out_16rl_custom2/030_038791.png', '_out_16rl_custom2/030_038792.png', '_out_16rl_custom2/030_038793.png', '_out_16rl_custom2/030_038794.png', '_out_16rl_custom2/030_038795.png', '_out_16rl_custom2/030_038796.png', '_out_16rl_custom2/030_038797.png', '_out_16rl_custom2/030_038799.png', '_out_16rl_custom2/030_038800.png', '_out_16rl_custom2/030_038801.png', '_out_16rl_custom2/030_038802.png', '_out_16rl_custom2/030_038804.png', '_out_16rl_custom2/030_038805.png', '_out_16rl_custom2/030_038807.png', '_out_16rl_custom2/030_038809.png', '_out_16rl_custom2/030_038810.png', '_out_16rl_custom2/030_038812.png', '_out_16rl_custom2/030_038813.png', '_out_16rl_custom2/030_038814.png', '_out_16rl_custom2/030_038815.png', '_out_16rl_custom2/030_038816.png', '_out_16rl_custom2/030_038817.png', '_out_16rl_custom2/030_038818.png', '_out_16rl_custom2/030_038819.png', '_out_16rl_custom2/030_038820.png', '_out_16rl_custom2/030_038822.png', '_out_16rl_custom2/030_038823.png', '_out_16rl_custom2/030_038824.png', '_out_16rl_custom2/030_038825.png', '_out_16rl_custom2/030_038827.png', '_out_16rl_custom2/030_038828.png', '_out_16rl_custom2/030_038830.png', '_out_16rl_custom2/030_038831.png', '_out_16rl_custom2/030_038832.png', '_out_16rl_custom2/030_038833.png', '_out_16rl_custom2/030_038835.png', '_out_16rl_custom2/030_038836.png', '_out_16rl_custom2/030_038837.png', '_out_16rl_custom2/030_038838.png', '_out_16rl_custom2/030_038839.png', '_out_16rl_custom2/030_038840.png', '_out_16rl_custom2/030_038841.png', '_out_16rl_custom2/030_038842.png', '_out_16rl_custom2/030_038843.png', '_out_16rl_custom2/030_038844.png', '_out_16rl_custom2/030_038845.png', '_out_16rl_custom2/030_038846.png', '_out_16rl_custom2/030_038847.png', '_out_16rl_custom2/030_038848.png', '_out_16rl_custom2/030_038849.png', '_out_16rl_custom2/030_038850.png', '_out_16rl_custom2/030_038851.png', '_out_16rl_custom2/030_038853.png', '_out_16rl_custom2/030_038854.png', '_out_16rl_custom2/030_038855.png', '_out_16rl_custom2/030_038856.png', '_out_16rl_custom2/030_038857.png', '_out_16rl_custom2/030_038858.png', '_out_16rl_custom2/030_038859.png', '_out_16rl_custom2/030_038860.png', '_out_16rl_custom2/030_038861.png', '_out_16rl_custom2/030_038862.png', '_out_16rl_custom2/030_038863.png', '_out_16rl_custom2/030_038864.png', '_out_16rl_custom2/030_038866.png', '_out_16rl_custom2/030_038868.png', '_out_16rl_custom2/030_038869.png', '_out_16rl_custom2/030_038870.png', '_out_16rl_custom2/030_038871.png', '_out_16rl_custom2/030_038872.png', '_out_16rl_custom2/030_038873.png', '_out_16rl_custom2/030_038874.png', '_out_16rl_custom2/030_038876.png', '_out_16rl_custom2/030_038877.png', '_out_16rl_custom2/030_038878.png', '_out_16rl_custom2/030_038879.png', '_out_16rl_custom2/030_038880.png', '_out_16rl_custom2/030_038882.png', '_out_16rl_custom2/030_038883.png', '_out_16rl_custom2/030_038884.png', '_out_16rl_custom2/030_038885.png', '_out_16rl_custom2/030_038887.png', '_out_16rl_custom2/030_038888.png', '_out_16rl_custom2/030_038889.png', '_out_16rl_custom2/030_038890.png', '_out_16rl_custom2/030_038892.png', '_out_16rl_custom2/030_038893.png', '_out_16rl_custom2/030_038894.png', '_out_16rl_custom2/030_038895.png', '_out_16rl_custom2/030_038896.png', '_out_16rl_custom2/030_038897.png', '_out_16rl_custom2/030_038898.png', '_out_16rl_custom2/030_038900.png', '_out_16rl_custom2/030_038901.png', '_out_16rl_custom2/030_038902.png', '_out_16rl_custom2/030_038903.png', '_out_16rl_custom2/030_038904.png', '_out_16rl_custom2/030_038905.png', '_out_16rl_custom2/030_038906.png', '_out_16rl_custom2/030_038907.png', '_out_16rl_custom2/030_038909.png', '_out_16rl_custom2/030_038910.png', '_out_16rl_custom2/030_038911.png', '_out_16rl_custom2/030_038913.png', '_out_16rl_custom2/030_038914.png', '_out_16rl_custom2/030_038915.png', '_out_16rl_custom2/030_038917.png', '_out_16rl_custom2/030_038918.png', '_out_16rl_custom2/030_038919.png', '_out_16rl_custom2/030_038920.png', '_out_16rl_custom2/030_038921.png', '_out_16rl_custom2/030_038922.png', '_out_16rl_custom2/030_038924.png', '_out_16rl_custom2/030_038925.png', '_out_16rl_custom2/030_038926.png', '_out_16rl_custom2/030_038927.png', '_out_16rl_custom2/030_038928.png', '_out_16rl_custom2/030_038929.png', '_out_16rl_custom2/030_038930.png', '_out_16rl_custom2/030_038931.png', '_out_16rl_custom2/030_038932.png', '_out_16rl_custom2/030_038934.png', '_out_16rl_custom2/030_038935.png', '_out_16rl_custom2/030_038936.png', '_out_16rl_custom2/030_038937.png', '_out_16rl_custom2/030_038938.png', '_out_16rl_custom2/030_038939.png', '_out_16rl_custom2/030_038940.png', '_out_16rl_custom2/030_038942.png', '_out_16rl_custom2/030_038943.png', '_out_16rl_custom2/030_038944.png', '_out_16rl_custom2/030_038946.png', '_out_16rl_custom2/030_038947.png', '_out_16rl_custom2/030_038948.png', '_out_16rl_custom2/030_038949.png', '_out_16rl_custom2/030_038950.png', '_out_16rl_custom2/030_038952.png', '_out_16rl_custom2/030_038953.png', '_out_16rl_custom2/030_038954.png', '_out_16rl_custom2/030_038955.png', '_out_16rl_custom2/030_038956.png', '_out_16rl_custom2/030_038957.png', '_out_16rl_custom2/030_038958.png', '_out_16rl_custom2/030_038959.png', '_out_16rl_custom2/030_038960.png', '_out_16rl_custom2/030_038961.png', '_out_16rl_custom2/030_038962.png', '_out_16rl_custom2/030_038963.png', '_out_16rl_custom2/030_038964.png', '_out_16rl_custom2/030_038965.png', '_out_16rl_custom2/030_038966.png', '_out_16rl_custom2/030_038967.png', '_out_16rl_custom2/030_038968.png', '_out_16rl_custom2/030_038969.png', '_out_16rl_custom2/030_038970.png', '_out_16rl_custom2/030_038971.png', '_out_16rl_custom2/030_038973.png', '_out_16rl_custom2/030_038974.png', '_out_16rl_custom2/030_038975.png', '_out_16rl_custom2/030_038976.png', '_out_16rl_custom2/030_038978.png', '_out_16rl_custom2/030_038979.png', '_out_16rl_custom2/030_038980.png', '_out_16rl_custom2/030_038982.png', '_out_16rl_custom2/030_038983.png', '_out_16rl_custom2/030_038984.png', '_out_16rl_custom2/030_038985.png', '_out_16rl_custom2/030_038986.png', '_out_16rl_custom2/030_038988.png', '_out_16rl_custom2/030_038989.png', '_out_16rl_custom2/030_038990.png', '_out_16rl_custom2/030_038991.png', '_out_16rl_custom2/030_038992.png', '_out_16rl_custom2/030_038993.png', '_out_16rl_custom2/030_038994.png', '_out_16rl_custom2/030_038996.png', '_out_16rl_custom2/030_038998.png', '_out_16rl_custom2/030_038999.png', '_out_16rl_custom2/030_039000.png', '_out_16rl_custom2/030_039001.png', '_out_16rl_custom2/030_039002.png', '_out_16rl_custom2/030_039003.png', '_out_16rl_custom2/030_039004.png', '_out_16rl_custom2/030_039005.png', '_out_16rl_custom2/030_039006.png', '_out_16rl_custom2/030_039007.png', '_out_16rl_custom2/030_039009.png', '_out_16rl_custom2/030_039010.png', '_out_16rl_custom2/030_039011.png', '_out_16rl_custom2/030_039013.png', '_out_16rl_custom2/030_039014.png', '_out_16rl_custom2/030_039015.png', '_out_16rl_custom2/030_039016.png', '_out_16rl_custom2/030_039017.png', '_out_16rl_custom2/030_039018.png', '_out_16rl_custom2/030_039020.png', '_out_16rl_custom2/030_039021.png', '_out_16rl_custom2/030_039022.png', '_out_16rl_custom2/030_039023.png', '_out_16rl_custom2/030_039024.png', '_out_16rl_custom2/030_039026.png', '_out_16rl_custom2/030_039027.png', '_out_16rl_custom2/030_039028.png', '_out_16rl_custom2/030_039029.png', '_out_16rl_custom2/030_039031.png', '_out_16rl_custom2/030_039032.png', '_out_16rl_custom2/030_039033.png', '_out_16rl_custom2/030_039034.png', '_out_16rl_custom2/030_039035.png', '_out_16rl_custom2/030_039037.png', '_out_16rl_custom2/030_039038.png', '_out_16rl_custom2/030_039039.png', '_out_16rl_custom2/030_039040.png', '_out_16rl_custom2/030_039042.png', '_out_16rl_custom2/030_039043.png', '_out_16rl_custom2/030_039044.png', '_out_16rl_custom2/030_039045.png', '_out_16rl_custom2/030_039047.png', '_out_16rl_custom2/030_039048.png', '_out_16rl_custom2/030_039049.png', '_out_16rl_custom2/030_039050.png', '_out_16rl_custom2/030_039052.png', '_out_16rl_custom2/030_039053.png', '_out_16rl_custom2/030_039054.png', '_out_16rl_custom2/030_039055.png', '_out_16rl_custom2/030_039056.png', '_out_16rl_custom2/030_039057.png', '_out_16rl_custom2/030_039058.png', '_out_16rl_custom2/030_039059.png', '_out_16rl_custom2/030_039060.png', '_out_16rl_custom2/030_039062.png', '_out_16rl_custom2/030_039064.png', '_out_16rl_custom2/030_039065.png', '_out_16rl_custom2/030_039067.png', '_out_16rl_custom2/030_039068.png', '_out_16rl_custom2/030_039069.png', '_out_16rl_custom2/030_039071.png', '_out_16rl_custom2/030_039072.png', '_out_16rl_custom2/030_039073.png', '_out_16rl_custom2/030_039075.png', '_out_16rl_custom2/030_039077.png', '_out_16rl_custom2/030_039078.png', '_out_16rl_custom2/030_039080.png', '_out_16rl_custom2/030_039082.png', '_out_16rl_custom2/030_039083.png', '_out_16rl_custom2/030_039085.png', '_out_16rl_custom2/030_039086.png', '_out_16rl_custom2/030_039087.png', '_out_16rl_custom2/030_039088.png', '_out_16rl_custom2/030_039089.png', '_out_16rl_custom2/030_039090.png', '_out_16rl_custom2/030_039092.png', '_out_16rl_custom2/030_039093.png', '_out_16rl_custom2/030_039094.png', '_out_16rl_custom2/030_039095.png', '_out_16rl_custom2/030_039096.png', '_out_16rl_custom2/030_039097.png', '_out_16rl_custom2/030_039098.png', '_out_16rl_custom2/030_039099.png', '_out_16rl_custom2/030_039100.png', '_out_16rl_custom2/030_039101.png', '_out_16rl_custom2/030_039102.png', '_out_16rl_custom2/030_039103.png', '_out_16rl_custom2/030_039104.png', '_out_16rl_custom2/030_039106.png', '_out_16rl_custom2/030_039108.png', '_out_16rl_custom2/030_039109.png', '_out_16rl_custom2/030_039110.png', '_out_16rl_custom2/030_039111.png', '_out_16rl_custom2/030_039112.png', '_out_16rl_custom2/030_039113.png', '_out_16rl_custom2/030_039114.png', '_out_16rl_custom2/030_039115.png', '_out_16rl_custom2/030_039116.png', '_out_16rl_custom2/030_039117.png', '_out_16rl_custom2/030_039118.png', '_out_16rl_custom2/030_039119.png', '_out_16rl_custom2/030_039120.png', '_out_16rl_custom2/030_039121.png', '_out_16rl_custom2/030_039122.png', '_out_16rl_custom2/030_039123.png', '_out_16rl_custom2/030_039124.png', '_out_16rl_custom2/030_039126.png', '_out_16rl_custom2/030_039127.png', '_out_16rl_custom2/030_039128.png', '_out_16rl_custom2/030_039130.png']
WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API 
WARNING: Client API version     = 0.9.15 
WARNING: Simulator API version  = 0.9.14 
2023-12-30 05:18:02.407186: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2023-12-30 05:18:03.320499: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f1f8f483460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-12-30 05:18:03.320515: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5
2023-12-30 05:18:03.323311: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1703931483.393600 1549988 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  0%|          | 0/971 [00:00<?, ?episode/s]
Started episode 30 of 1000
2023-12-30 05:19:02.088770: W external/local_tsl/tsl/framework/bfc_allocator.cc:366] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
Finished episode 30 of 1000
Saved model from episode 30. Count of epochs trained: 21176
  0%|          | 1/971 [03:33<57:24:29, 213.06s/episode]
Started episode 31 of 1000
Finished episode 31 of 1000
Saved model from episode 31. Count of epochs trained: 21477
  0%|          | 2/971 [07:04<57:06:55, 212.19s/episode]
Started episode 32 of 1000
Finished episode 32 of 1000
Saved model from episode 32. Count of epochs trained: 21737
  0%|          | 3/971 [10:10<53:45:45, 199.94s/episode]
Started episode 33 of 1000
Finished episode 33 of 1000
Saved model from episode 33. Count of epochs trained: 21930
  0%|          | 4/971 [12:28<47:09:50, 175.58s/episode]
Started episode 34 of 1000
Finished episode 34 of 1000
Saved model from episode 34. Count of epochs trained: 22059
  1%|          | 5/971 [14:01<39:06:18, 145.73s/episode]
Started episode 35 of 1000
Finished episode 35 of 1000
Saved model from episode 35. Count of epochs trained: 22070
  1%|          | 6/971 [14:09<26:35:25, 99.20s/episode] 
Started episode 36 of 1000
Finished episode 36 of 1000
Saved model from episode 36. Count of epochs trained: 22145
  1%|          | 7/971 [15:05<22:42:46, 84.82s/episode]
Started episode 37 of 1000
Finished episode 37 of 1000
Saved model from episode 37. Count of epochs trained: 22153
  1%|          | 8/971 [15:11<15:58:29, 59.72s/episode]
Started episode 38 of 1000
Finished episode 38 of 1000
Saved model from episode 38. Count of epochs trained: 22159
  1%|          | 9/971 [15:16<11:23:54, 42.66s/episode]
Started episode 39 of 1000
Finished episode 39 of 1000
Saved model from episode 39. Count of epochs trained: 22205
  1%|1         | 10/971 [15:49<10:39:08, 39.90s/episode]
Started episode 40 of 1000
Finished episode 40 of 1000
Saved model from episode 40. Count of epochs trained: 22212
  1%|1         | 11/971 [15:56<7:56:44, 29.80s/episode] 
Started episode 41 of 1000
Finished episode 41 of 1000
Saved model from episode 41. Count of epochs trained: 22256
  1%|1         | 12/971 [16:29<8:08:52, 30.59s/episode]
Started episode 42 of 1000
Finished episode 42 of 1000
Saved model from episode 42. Count of epochs trained: 22260
  1%|1         | 13/971 [16:33<5:59:33, 22.52s/episode]
Started episode 43 of 1000
Finished episode 43 of 1000
Saved model from episode 43. Count of epochs trained: 22265
  1%|1         | 14/971 [16:37<4:30:09, 16.94s/episode]
Started episode 44 of 1000
Finished episode 44 of 1000
Saved model from episode 44. Count of epochs trained: 22270
  2%|1         | 15/971 [16:41<3:30:26, 13.21s/episode]
Started episode 45 of 1000
Finished episode 45 of 1000
Saved model from episode 45. Count of epochs trained: 22277
  2%|1         | 16/971 [16:46<2:51:33, 10.78s/episode]
Started episode 46 of 1000
Finished episode 46 of 1000
Saved model from episode 46. Count of epochs trained: 22291
  2%|1         | 17/971 [16:57<2:51:38, 10.79s/episode]
Started episode 47 of 1000
Finished episode 47 of 1000
Saved model from episode 47. Count of epochs trained: 22296
  2%|1         | 18/971 [17:02<2:20:55,  8.87s/episode]
Started episode 48 of 1000
Finished episode 48 of 1000
Saved model from episode 48. Count of epochs trained: 22301
  2%|1         | 19/971 [17:06<1:58:19,  7.46s/episode]
Started episode 49 of 1000
Finished episode 49 of 1000
Saved model from episode 49. Count of epochs trained: 22305
  2%|2         | 20/971 [17:10<1:40:06,  6.32s/episode]
Started episode 50 of 1000
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 459, in train_in_loop
    self.train()
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 379, in train
    current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE, verbose=0)
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/site-packages/keras/src/engine/training.py", line 2655, in predict
    tmp_batch_outputs = self.predict_function(iterator)
TypeError: 'NoneType' object is not callable
Finished episode 50 of 1000
Saved model from episode 50. Count of epochs trained: 22309
  2%|2         | 21/971 [17:14<1:30:48,  5.73s/episode]
Started episode 51 of 1000
Finished episode 51 of 1000
Saved model from episode 51. Count of epochs trained: 22309
  2%|2         | 22/971 [17:18<1:21:10,  5.13s/episode]
Started episode 52 of 1000
Finished episode 52 of 1000
Saved model from episode 52. Count of epochs trained: 22309
  2%|2         | 23/971 [17:21<1:12:56,  4.62s/episode]
Started episode 53 of 1000
Finished episode 53 of 1000
Saved model from episode 53. Count of epochs trained: 22309
  2%|2         | 24/971 [17:25<1:07:36,  4.28s/episode]
Started episode 54 of 1000
Finished episode 54 of 1000
Saved model from episode 54. Count of epochs trained: 22309
  3%|2         | 25/971 [17:28<1:04:02,  4.06s/episode]
Started episode 55 of 1000
Finished episode 55 of 1000
Saved model from episode 55. Count of epochs trained: 22309
  3%|2         | 26/971 [17:35<1:17:34,  4.93s/episode]
Started episode 56 of 1000
Finished episode 56 of 1000
Saved model from episode 56. Count of epochs trained: 22309
  3%|2         | 27/971 [17:39<1:11:18,  4.53s/episode]
Started episode 57 of 1000
Finished episode 57 of 1000
Saved model from episode 57. Count of epochs trained: 22309
  3%|2         | 28/971 [17:43<1:08:45,  4.38s/episode]
Started episode 58 of 1000
Finished episode 58 of 1000
Saved model from episode 58. Count of epochs trained: 22309
  3%|2         | 29/971 [17:47<1:08:10,  4.34s/episode]
Started episode 59 of 1000
Finished episode 59 of 1000
Saved model from episode 59. Count of epochs trained: 22309
  3%|3         | 30/971 [17:50<1:02:42,  4.00s/episode]
Started episode 60 of 1000
Finished episode 60 of 1000
Saved model from episode 60. Count of epochs trained: 22309
  3%|3         | 31/971 [17:55<1:06:17,  4.23s/episode]
Started episode 61 of 1000
Finished episode 61 of 1000
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
Count of epochs trained: 22309	Goal: 23086
2023-12-30 13:44:23.600696: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 13:44:23.622812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 13:44:23.622829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 13:44:23.623371: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 13:44:23.626761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 13:44:24.042479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 13:44:24.353910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:24.387275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:24.387478: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:24.390555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:24.390826: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:24.391049: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:25.350300: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:25.350449: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:25.350590: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 13:44:25.350717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4407 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Models in tmp ['tmp/029.20956.model', 'tmp/030.21176.model', 'tmp/031.21476.model', 'tmp/032.21737.model', 'tmp/033.21930.model', 'tmp/034.22058.model', 'tmp/035.22070.model', 'tmp/036.22145.model', 'tmp/037.22152.model', 'tmp/038.22159.model', 'tmp/039.22204.model', 'tmp/040.22212.model', 'tmp/041.22255.model', 'tmp/042.22260.model', 'tmp/043.22264.model', 'tmp/044.22270.model', 'tmp/045.22276.model', 'tmp/046.22290.model', 'tmp/047.22295.model', 'tmp/048.22300.model', 'tmp/049.22305.model', 'tmp/050.22309.model', 'tmp/051.22309.model', 'tmp/052.22309.model', 'tmp/053.22309.model', 'tmp/054.22309.model', 'tmp/055.22309.model', 'tmp/056.22309.model', 'tmp/057.22309.model', 'tmp/058.22309.model', 'tmp/059.22309.model', 'tmp/060.22309.model']
Load model tmp/060.22309.model
Leftover images from failed episode: ['_out_16rl_custom2/061_016005.png', '_out_16rl_custom2/061_016007.png', '_out_16rl_custom2/061_016008.png', '_out_16rl_custom2/061_016010.png', '_out_16rl_custom2/061_016011.png', '_out_16rl_custom2/061_016012.png', '_out_16rl_custom2/061_016014.png', '_out_16rl_custom2/061_016015.png', '_out_16rl_custom2/061_016016.png', '_out_16rl_custom2/061_016017.png', '_out_16rl_custom2/061_016019.png', '_out_16rl_custom2/061_016021.png', '_out_16rl_custom2/061_016022.png', '_out_16rl_custom2/061_016023.png', '_out_16rl_custom2/061_016024.png', '_out_16rl_custom2/061_016025.png', '_out_16rl_custom2/061_016027.png', '_out_16rl_custom2/061_016028.png', '_out_16rl_custom2/061_016030.png', '_out_16rl_custom2/061_016031.png', '_out_16rl_custom2/061_016032.png', '_out_16rl_custom2/061_016034.png', '_out_16rl_custom2/061_016036.png', '_out_16rl_custom2/061_016038.png', '_out_16rl_custom2/061_016039.png', '_out_16rl_custom2/061_016040.png', '_out_16rl_custom2/061_016041.png', '_out_16rl_custom2/061_016042.png', '_out_16rl_custom2/061_016043.png', '_out_16rl_custom2/061_016044.png', '_out_16rl_custom2/061_016045.png', '_out_16rl_custom2/061_016046.png', '_out_16rl_custom2/061_016047.png', '_out_16rl_custom2/061_016048.png']
WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API 
WARNING: Client API version     = 0.9.15 
WARNING: Simulator API version  = 0.9.14 
2023-12-30 13:44:39.141829: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2023-12-30 13:44:40.097486: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fe3141dc300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-12-30 13:44:40.097501: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5
2023-12-30 13:44:40.100325: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1703961880.176129 1681744 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  0%|          | 0/940 [00:00<?, ?episode/s]
Started episode 61 of 1000
Finished episode 61 of 1000
Saved model from episode 61. Count of epochs trained: 22548
  0%|          | 1/940 [02:37<41:06:59, 157.63s/episode]
Started episode 62 of 1000
Finished episode 62 of 1000
Saved model from episode 62. Count of epochs trained: 22848
  0%|          | 2/940 [05:14<40:56:00, 157.10s/episode]
Started episode 63 of 1000
Finished episode 63 of 1000
Saved model from episode 63. Count of epochs trained: 23105
  0%|          | 3/940 [07:28<38:07:16, 146.46s/episode]
Started episode 64 of 1000
Finished episode 64 of 1000
Saved model from episode 64. Count of epochs trained: 23383
  0%|          | 4/940 [09:51<37:44:36, 145.17s/episode]
Started episode 65 of 1000
Finished episode 65 of 1000
Saved model from episode 65. Count of epochs trained: 23560
  1%|          | 5/940 [11:23<32:45:37, 126.14s/episode]
Started episode 66 of 1000
Finished episode 66 of 1000
Saved model from episode 66. Count of epochs trained: 23575
  1%|          | 6/940 [11:32<22:20:06, 86.09s/episode] 
Started episode 67 of 1000
Finished episode 67 of 1000
Saved model from episode 67. Count of epochs trained: 23638
  1%|          | 7/940 [12:05<17:52:07, 68.95s/episode]
Started episode 68 of 1000
Finished episode 68 of 1000
Saved model from episode 68. Count of epochs trained: 23647
  1%|          | 8/940 [12:11<12:36:45, 48.72s/episode]
Started episode 69 of 1000
Finished episode 69 of 1000
Saved model from episode 69. Count of epochs trained: 23656
  1%|          | 9/940 [12:16<9:04:12, 35.07s/episode] 
Started episode 70 of 1000
Finished episode 70 of 1000
Saved model from episode 70. Count of epochs trained: 23697
  1%|1         | 10/940 [12:39<8:07:19, 31.44s/episode]
Started episode 71 of 1000
Finished episode 71 of 1000
Saved model from episode 71. Count of epochs trained: 23703
  1%|1         | 11/940 [12:43<5:55:44, 22.98s/episode]
Started episode 72 of 1000
Finished episode 72 of 1000
Saved model from episode 72. Count of epochs trained: 23709
  1%|1         | 12/940 [12:46<4:24:25, 17.10s/episode]
Started episode 73 of 1000
Finished episode 73 of 1000
Saved model from episode 73. Count of epochs trained: 23742
  1%|1         | 13/940 [13:05<4:31:11, 17.55s/episode]
Started episode 74 of 1000
Finished episode 74 of 1000
Count of epochs trained: 23748	Goal: 24981
Count of epochs trained: 23872	Goal: 24981
Count of epochs trained: 23997	Goal: 24981
Count of epochs trained: 24121	Goal: 24981
Count of epochs trained: 24245	Goal: 24981
Count of epochs trained: 24369	Goal: 24981
Count of epochs trained: 24493	Goal: 24981
Count of epochs trained: 24617	Goal: 24981
Count of epochs trained: 24740	Goal: 24981
Count of epochs trained: 24864	Goal: 24981
Saved model from episode 74. Count of epochs trained: 24988
  1%|1         | 14/940 [23:10<50:08:28, 194.93s/episode]
Started episode 75 of 1000
Finished episode 75 of 1000
Count of epochs trained: 25010	Goal: 26227
Count of epochs trained: 25133	Goal: 26227
Count of epochs trained: 25257	Goal: 26227
Count of epochs trained: 25381	Goal: 26227
Count of epochs trained: 25505	Goal: 26227
Count of epochs trained: 25630	Goal: 26227
Count of epochs trained: 25754	Goal: 26227
Count of epochs trained: 25879	Goal: 26227
Count of epochs trained: 26004	Goal: 26227
Count of epochs trained: 26128	Goal: 26227
Saved model from episode 75. Count of epochs trained: 26253
  2%|1         | 15/940 [33:24<82:31:15, 321.16s/episode]
Started episode 76 of 1000
Finished episode 76 of 1000
Count of epochs trained: 26271	Goal: 27492
Count of epochs trained: 26396	Goal: 27492
Count of epochs trained: 26520	Goal: 27492
Count of epochs trained: 26644	Goal: 27492
Count of epochs trained: 26769	Goal: 27492
Count of epochs trained: 26893	Goal: 27492
Count of epochs trained: 27017	Goal: 27492
Count of epochs trained: 27142	Goal: 27492
Count of epochs trained: 27266	Goal: 27492
Count of epochs trained: 27390	Goal: 27492
Saved model from episode 76. Count of epochs trained: 27516
  2%|1         | 16/940 [43:35<104:52:12, 408.59s/episode]
Started episode 77 of 1000
Finished episode 77 of 1000
Count of epochs trained: 27521	Goal: 28755
Count of epochs trained: 27645	Goal: 28755
Count of epochs trained: 27769	Goal: 28755
Count of epochs trained: 27893	Goal: 28755
Count of epochs trained: 28016	Goal: 28755
Count of epochs trained: 28140	Goal: 28755
Count of epochs trained: 28264	Goal: 28755
Count of epochs trained: 28388	Goal: 28755
Count of epochs trained: 28511	Goal: 28755
Count of epochs trained: 28635	Goal: 28755
Saved model from episode 77. Count of epochs trained: 28760
  2%|1         | 17/940 [53:39<119:49:36, 467.36s/episode]
Started episode 78 of 1000
Finished episode 78 of 1000
Count of epochs trained: 28779	Goal: 29999
Count of epochs trained: 28903	Goal: 29999
Count of epochs trained: 29027	Goal: 29999
Count of epochs trained: 29151	Goal: 29999
Count of epochs trained: 29276	Goal: 29999
Count of epochs trained: 29400	Goal: 29999
Count of epochs trained: 29524	Goal: 29999
Count of epochs trained: 29648	Goal: 29999
Count of epochs trained: 29772	Goal: 29999
Count of epochs trained: 29896	Goal: 29999
Saved model from episode 78. Count of epochs trained: 30021
  2%|1         | 18/940 [1:03:51<130:49:13, 510.80s/episode]
Started episode 79 of 1000
Finished episode 79 of 1000
Count of epochs trained: 30291	Goal: 31260
Count of epochs trained: 30415	Goal: 31260
Count of epochs trained: 30540	Goal: 31260
Count of epochs trained: 30663	Goal: 31260
Count of epochs trained: 30787	Goal: 31260
Count of epochs trained: 30911	Goal: 31260
Count of epochs trained: 31035	Goal: 31260
Count of epochs trained: 31159	Goal: 31260
Saved model from episode 79. Count of epochs trained: 31283
  2%|2         | 19/940 [1:14:24<140:01:38, 547.34s/episode]
Started episode 80 of 1000
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 459, in train_in_loop
    self.train()
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 379, in train
    current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE, verbose=0)
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/site-packages/keras/src/engine/training.py", line 2655, in predict
    tmp_batch_outputs = self.predict_function(iterator)
TypeError: 'NoneType' object is not callable
Finished episode 80 of 1000
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
Count of epochs trained: 31289	Goal: 32522
2023-12-30 16:50:24.564167: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 16:50:24.586771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 16:50:24.586790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 16:50:24.587367: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 16:50:24.590949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 16:50:25.026270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 16:50:25.371915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:25.406758: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:25.407119: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:25.410463: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:25.410661: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:25.410861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:26.255728: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:26.255873: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:26.255985: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 16:50:26.256107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4406 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Models in tmp ['tmp/060.22309.model', 'tmp/061.22548.model', 'tmp/062.22848.model', 'tmp/063.23104.model', 'tmp/064.23382.model', 'tmp/065.23559.model', 'tmp/066.23574.model', 'tmp/067.23637.model', 'tmp/068.23646.model', 'tmp/069.23655.model', 'tmp/070.23696.model', 'tmp/071.23702.model', 'tmp/072.23708.model', 'tmp/073.23741.model', 'tmp/074.24987.model', 'tmp/075.26252.model', 'tmp/076.27515.model', 'tmp/077.28759.model', 'tmp/078.30021.model', 'tmp/079.31282.model']
Load model tmp/079.31282.model
Leftover images from failed episode: ['_out_16rl_custom2/080_020266.png', '_out_16rl_custom2/080_020267.png', '_out_16rl_custom2/080_020268.png', '_out_16rl_custom2/080_020269.png', '_out_16rl_custom2/080_020270.png', '_out_16rl_custom2/080_020271.png', '_out_16rl_custom2/080_020272.png', '_out_16rl_custom2/080_020274.png', '_out_16rl_custom2/080_020275.png', '_out_16rl_custom2/080_020276.png', '_out_16rl_custom2/080_020278.png', '_out_16rl_custom2/080_020279.png', '_out_16rl_custom2/080_020280.png', '_out_16rl_custom2/080_020281.png', '_out_16rl_custom2/080_020282.png', '_out_16rl_custom2/080_020283.png', '_out_16rl_custom2/080_020285.png', '_out_16rl_custom2/080_020287.png', '_out_16rl_custom2/080_020289.png', '_out_16rl_custom2/080_020290.png', '_out_16rl_custom2/080_020291.png', '_out_16rl_custom2/080_020292.png', '_out_16rl_custom2/080_020293.png', '_out_16rl_custom2/080_020295.png', '_out_16rl_custom2/080_020296.png', '_out_16rl_custom2/080_020298.png', '_out_16rl_custom2/080_020299.png', '_out_16rl_custom2/080_020301.png', '_out_16rl_custom2/080_020302.png', '_out_16rl_custom2/080_020303.png', '_out_16rl_custom2/080_020304.png', '_out_16rl_custom2/080_020305.png', '_out_16rl_custom2/080_020307.png', '_out_16rl_custom2/080_020308.png', '_out_16rl_custom2/080_020309.png', '_out_16rl_custom2/080_020311.png', '_out_16rl_custom2/080_020312.png', '_out_16rl_custom2/080_020313.png', '_out_16rl_custom2/080_020315.png', '_out_16rl_custom2/080_020316.png', '_out_16rl_custom2/080_020317.png', '_out_16rl_custom2/080_020319.png', '_out_16rl_custom2/080_020320.png', '_out_16rl_custom2/080_020321.png', '_out_16rl_custom2/080_020322.png', '_out_16rl_custom2/080_020323.png', '_out_16rl_custom2/080_020324.png', '_out_16rl_custom2/080_020325.png', '_out_16rl_custom2/080_020326.png', '_out_16rl_custom2/080_020327.png', '_out_16rl_custom2/080_020328.png', '_out_16rl_custom2/080_020329.png', '_out_16rl_custom2/080_020331.png']
WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API 
WARNING: Client API version     = 0.9.15 
WARNING: Simulator API version  = 0.9.14 
2023-12-30 16:50:39.645425: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2023-12-30 16:50:40.602861: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f8fd355e120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-12-30 16:50:40.602876: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5
2023-12-30 16:50:40.605785: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1703973040.682264 2306450 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  0%|          | 0/921 [00:00<?, ?episode/s]
Started episode 80 of 1000
Finished episode 80 of 1000
Saved model from episode 80. Count of epochs trained: 31530
  0%|          | 1/921 [02:37<40:20:37, 157.87s/episode]
Started episode 81 of 1000
Finished episode 81 of 1000
Saved model from episode 81. Count of epochs trained: 31830
  0%|          | 2/921 [05:14<40:07:25, 157.18s/episode]
Started episode 82 of 1000
Finished episode 82 of 1000
Saved model from episode 82. Count of epochs trained: 32126
  0%|          | 3/921 [07:50<39:55:50, 156.59s/episode]
Started episode 83 of 1000
Finished episode 83 of 1000
Saved model from episode 83. Count of epochs trained: 32417
  0%|          | 4/921 [10:25<39:44:35, 156.03s/episode]
Started episode 84 of 1000
Finished episode 84 of 1000
Saved model from episode 84. Count of epochs trained: 32704
  1%|          | 5/921 [12:59<39:32:38, 155.41s/episode]
Started episode 85 of 1000
Finished episode 85 of 1000
Count of epochs trained: 32987	Goal: 33943
Count of epochs trained: 33110	Goal: 33943
Count of epochs trained: 33234	Goal: 33943
Count of epochs trained: 33358	Goal: 33943
Count of epochs trained: 33483	Goal: 33943
Count of epochs trained: 33608	Goal: 33943
Count of epochs trained: 33733	Goal: 33943
Count of epochs trained: 33858	Goal: 33943
Saved model from episode 85. Count of epochs trained: 33984
  1%|          | 6/921 [23:34<80:55:39, 318.40s/episode]
Started episode 86 of 1000
Finished episode 86 of 1000
Count of epochs trained: 34277	Goal: 35223
Count of epochs trained: 34403	Goal: 35223
Count of epochs trained: 34528	Goal: 35223
Count of epochs trained: 34653	Goal: 35223
Count of epochs trained: 34778	Goal: 35223
Count of epochs trained: 34902	Goal: 35223
Count of epochs trained: 35028	Goal: 35223
Count of epochs trained: 35153	Goal: 35223
Saved model from episode 86. Count of epochs trained: 35279
  1%|          | 7/921 [34:10<107:10:02, 422.10s/episode]
Started episode 87 of 1000
Finished episode 87 of 1000
Count of epochs trained: 35302	Goal: 36518
Count of epochs trained: 35427	Goal: 36518
Count of epochs trained: 35553	Goal: 36518
Count of epochs trained: 35678	Goal: 36518
Count of epochs trained: 35803	Goal: 36518
Count of epochs trained: 35929	Goal: 36518
Count of epochs trained: 36054	Goal: 36518
Count of epochs trained: 36179	Goal: 36518
Count of epochs trained: 36304	Goal: 36518
Count of epochs trained: 36429	Goal: 36518
Saved model from episode 87. Count of epochs trained: 36555
  1%|          | 8/921 [44:23<122:29:42, 483.00s/episode]
Started episode 88 of 1000
Finished episode 88 of 1000
Count of epochs trained: 36655	Goal: 37794
Count of epochs trained: 36779	Goal: 37794
Count of epochs trained: 36904	Goal: 37794
Count of epochs trained: 37028	Goal: 37794
Count of epochs trained: 37152	Goal: 37794
Count of epochs trained: 37276	Goal: 37794
Count of epochs trained: 37401	Goal: 37794
Count of epochs trained: 37525	Goal: 37794
Count of epochs trained: 37649	Goal: 37794
Count of epochs trained: 37773	Goal: 37794
Saved model from episode 88. Count of epochs trained: 37898
  1%|          | 9/921 [55:17<135:55:13, 536.53s/episode]
Started episode 89 of 1000
Finished episode 89 of 1000
Count of epochs trained: 37991	Goal: 39137
Count of epochs trained: 38116	Goal: 39137
Count of epochs trained: 38240	Goal: 39137
Count of epochs trained: 38365	Goal: 39137
Count of epochs trained: 38490	Goal: 39137
Count of epochs trained: 38615	Goal: 39137
Count of epochs trained: 38739	Goal: 39137
Count of epochs trained: 38864	Goal: 39137
Count of epochs trained: 38988	Goal: 39137
Count of epochs trained: 39113	Goal: 39137
Saved model from episode 89. Count of epochs trained: 39238
  1%|1         | 10/921 [1:06:09<144:43:25, 571.90s/episode]
Started episode 90 of 1000
Finished episode 90 of 1000
Count of epochs trained: 39336	Goal: 40477
Count of epochs trained: 39461	Goal: 40477
Count of epochs trained: 39585	Goal: 40477
Count of epochs trained: 39710	Goal: 40477
Count of epochs trained: 39834	Goal: 40477
Count of epochs trained: 39959	Goal: 40477
Count of epochs trained: 40083	Goal: 40477
Count of epochs trained: 40208	Goal: 40477
Count of epochs trained: 40332	Goal: 40477
Count of epochs trained: 40447	Goal: 40477
Saved model from episode 90. Count of epochs trained: 40565
  1%|1         | 11/921 [1:17:03<150:58:54, 597.29s/episode]
Started episode 91 of 1000
Finished episode 91 of 1000
Count of epochs trained: 40620	Goal: 41804
Count of epochs trained: 40744	Goal: 41804
Count of epochs trained: 40868	Goal: 41804
Count of epochs trained: 40993	Goal: 41804
Count of epochs trained: 41117	Goal: 41804
Count of epochs trained: 41241	Goal: 41804
Count of epochs trained: 41365	Goal: 41804
Count of epochs trained: 41490	Goal: 41804
Count of epochs trained: 41614	Goal: 41804
Count of epochs trained: 41738	Goal: 41804
Saved model from episode 91. Count of epochs trained: 41862
  1%|1         | 12/921 [1:27:35<153:26:03, 607.66s/episode]
Started episode 92 of 1000
Finished episode 92 of 1000
Count of epochs trained: 41898	Goal: 43101
Count of epochs trained: 42021	Goal: 43101
Count of epochs trained: 42146	Goal: 43101
Count of epochs trained: 42269	Goal: 43101
Count of epochs trained: 42393	Goal: 43101
Count of epochs trained: 42517	Goal: 43101
Count of epochs trained: 42641	Goal: 43101
Count of epochs trained: 42766	Goal: 43101
Count of epochs trained: 42890	Goal: 43101
Count of epochs trained: 43015	Goal: 43101
Saved model from episode 92. Count of epochs trained: 43140
  1%|1         | 13/921 [1:37:56<154:16:03, 611.63s/episode]
Started episode 93 of 1000
Finished episode 93 of 1000
Count of epochs trained: 43261	Goal: 44379
Count of epochs trained: 43385	Goal: 44379
Count of epochs trained: 43510	Goal: 44379
Count of epochs trained: 43634	Goal: 44379
Count of epochs trained: 43759	Goal: 44379
Count of epochs trained: 43883	Goal: 44379
Count of epochs trained: 44008	Goal: 44379
Count of epochs trained: 44132	Goal: 44379
Count of epochs trained: 44256	Goal: 44379
Saved model from episode 93. Count of epochs trained: 44381
  2%|1         | 14/921 [1:48:03<153:46:18, 610.34s/episode]
Started episode 94 of 1000
Finished episode 94 of 1000
Count of epochs trained: 44388	Goal: 45620
Count of epochs trained: 44512	Goal: 45620
Count of epochs trained: 44637	Goal: 45620
Count of epochs trained: 44761	Goal: 45620
Count of epochs trained: 44885	Goal: 45620
Count of epochs trained: 45010	Goal: 45620
Count of epochs trained: 45134	Goal: 45620
Count of epochs trained: 45258	Goal: 45620
Count of epochs trained: 45382	Goal: 45620
Count of epochs trained: 45507	Goal: 45620
Saved model from episode 94. Count of epochs trained: 45632
  2%|1         | 15/921 [1:58:08<153:12:51, 608.80s/episode]
Started episode 95 of 1000
Finished episode 95 of 1000
Count of epochs trained: 45689	Goal: 46871
Count of epochs trained: 45813	Goal: 46871
Count of epochs trained: 45937	Goal: 46871
Count of epochs trained: 46062	Goal: 46871
Count of epochs trained: 46187	Goal: 46871
Count of epochs trained: 46311	Goal: 46871
Count of epochs trained: 46436	Goal: 46871
Count of epochs trained: 46560	Goal: 46871
Count of epochs trained: 46684	Goal: 46871
Count of epochs trained: 46809	Goal: 46871
Saved model from episode 95. Count of epochs trained: 46935
  2%|1         | 16/921 [2:08:41<154:52:43, 616.09s/episode]
Started episode 96 of 1000
Finished episode 96 of 1000
Count of epochs trained: 47223	Goal: 48174
Count of epochs trained: 47348	Goal: 48174
Count of epochs trained: 47473	Goal: 48174
Count of epochs trained: 47597	Goal: 48174
Count of epochs trained: 47721	Goal: 48174
Count of epochs trained: 47845	Goal: 48174
Count of epochs trained: 47970	Goal: 48174
Count of epochs trained: 48093	Goal: 48174
Saved model from episode 96. Count of epochs trained: 48218
  2%|1         | 17/921 [2:19:23<156:39:24, 623.85s/episode]
Started episode 97 of 1000
Finished episode 97 of 1000
Count of epochs trained: 48489	Goal: 49457
Count of epochs trained: 48613	Goal: 49457
Count of epochs trained: 48737	Goal: 49457
Count of epochs trained: 48861	Goal: 49457
Count of epochs trained: 48985	Goal: 49457
Count of epochs trained: 49110	Goal: 49457
Count of epochs trained: 49234	Goal: 49457
Count of epochs trained: 49358	Goal: 49457
Saved model from episode 97. Count of epochs trained: 49484
  2%|1         | 18/921 [2:29:57<157:14:19, 626.87s/episode]
Started episode 98 of 1000
Finished episode 98 of 1000
Count of epochs trained: 49507	Goal: 50723
Count of epochs trained: 49631	Goal: 50723
Count of epochs trained: 49756	Goal: 50723
Count of epochs trained: 49880	Goal: 50723
Count of epochs trained: 50005	Goal: 50723
Count of epochs trained: 50129	Goal: 50723
Count of epochs trained: 50253	Goal: 50723
Count of epochs trained: 50377	Goal: 50723
Count of epochs trained: 50501	Goal: 50723
Count of epochs trained: 50625	Goal: 50723
Saved model from episode 98. Count of epochs trained: 50749
  2%|2         | 19/921 [2:40:11<156:07:32, 623.12s/episode]
Started episode 99 of 1000
Finished episode 99 of 1000
Count of epochs trained: 50767	Goal: 51988
Count of epochs trained: 50890	Goal: 51988
Count of epochs trained: 51014	Goal: 51988
Count of epochs trained: 51138	Goal: 51988
Count of epochs trained: 51261	Goal: 51988
Count of epochs trained: 51385	Goal: 51988
Count of epochs trained: 51508	Goal: 51988
Count of epochs trained: 51632	Goal: 51988
Count of epochs trained: 51755	Goal: 51988
Count of epochs trained: 51878	Goal: 51988
Saved model from episode 99. Count of epochs trained: 52002
  2%|2         | 20/921 [2:50:22<155:02:41, 619.49s/episode]
Started episode 100 of 1000
Finished episode 100 of 1000
Count of epochs trained: 52253	Goal: 53241
Count of epochs trained: 52376	Goal: 53241
Count of epochs trained: 52502	Goal: 53241
Count of epochs trained: 52626	Goal: 53241
Count of epochs trained: 52750	Goal: 53241
Count of epochs trained: 52873	Goal: 53241
Count of epochs trained: 52997	Goal: 53241
Count of epochs trained: 53121	Goal: 53241
Saved model from episode 100. Count of epochs trained: 53245
  2%|2         | 21/921 [3:00:49<155:24:37, 621.64s/episode]
Started episode 101 of 1000
Finished episode 101 of 1000
Count of epochs trained: 53254	Goal: 54484
Count of epochs trained: 53366	Goal: 54484
Count of epochs trained: 53481	Goal: 54484
Count of epochs trained: 53605	Goal: 54484
Count of epochs trained: 53726	Goal: 54484
Count of epochs trained: 53848	Goal: 54484
Count of epochs trained: 53973	Goal: 54484
Count of epochs trained: 54096	Goal: 54484
Count of epochs trained: 54221	Goal: 54484
Count of epochs trained: 54345	Goal: 54484
Count of epochs trained: 54469	Goal: 54484
Saved model from episode 101. Count of epochs trained: 54593
  2%|2         | 22/921 [3:11:57<158:40:27, 635.40s/episode]
Started episode 102 of 1000
Finished episode 102 of 1000
Count of epochs trained: 54855	Goal: 55832
Count of epochs trained: 54978	Goal: 55832
Count of epochs trained: 55102	Goal: 55832
Count of epochs trained: 55226	Goal: 55832
Count of epochs trained: 55349	Goal: 55832
Count of epochs trained: 55472	Goal: 55832
Count of epochs trained: 55595	Goal: 55832
Count of epochs trained: 55718	Goal: 55832
Saved model from episode 102. Count of epochs trained: 55843
  2%|2         | 23/921 [3:22:32<158:29:37, 635.39s/episode]
Started episode 103 of 1000
Finished episode 103 of 1000
Count of epochs trained: 55851	Goal: 57082
Count of epochs trained: 55975	Goal: 57082
Count of epochs trained: 56100	Goal: 57082
Count of epochs trained: 56225	Goal: 57082
Count of epochs trained: 56350	Goal: 57082
Count of epochs trained: 56475	Goal: 57082
Count of epochs trained: 56600	Goal: 57082
Count of epochs trained: 56724	Goal: 57082
Count of epochs trained: 56849	Goal: 57082
Count of epochs trained: 56974	Goal: 57082
Saved model from episode 103. Count of epochs trained: 57099
  3%|2         | 24/921 [3:32:38<156:06:10, 626.50s/episode]
Started episode 104 of 1000
Finished episode 104 of 1000
Count of epochs trained: 57251	Goal: 58338
Count of epochs trained: 57375	Goal: 58338
Count of epochs trained: 57500	Goal: 58338
Count of epochs trained: 57625	Goal: 58338
Count of epochs trained: 57750	Goal: 58338
Count of epochs trained: 57875	Goal: 58338
Count of epochs trained: 57999	Goal: 58338
Count of epochs trained: 58124	Goal: 58338
Count of epochs trained: 58250	Goal: 58338
Saved model from episode 104. Count of epochs trained: 58375
  3%|2         | 25/921 [3:43:07<156:07:22, 627.28s/episode]
Started episode 105 of 1000
Finished episode 105 of 1000
Count of epochs trained: 58390	Goal: 59614
Count of epochs trained: 58515	Goal: 59614
Count of epochs trained: 58639	Goal: 59614
Count of epochs trained: 58764	Goal: 59614
Count of epochs trained: 58889	Goal: 59614
Count of epochs trained: 59014	Goal: 59614
Count of epochs trained: 59138	Goal: 59614
Count of epochs trained: 59263	Goal: 59614
Count of epochs trained: 59388	Goal: 59614
Count of epochs trained: 59512	Goal: 59614
Saved model from episode 105. Count of epochs trained: 59638
  3%|2         | 26/921 [3:53:17<154:40:09, 622.13s/episode]
Started episode 106 of 1000
Finished episode 106 of 1000
Count of epochs trained: 59647	Goal: 60877
Count of epochs trained: 59772	Goal: 60877
Count of epochs trained: 59897	Goal: 60877
Count of epochs trained: 60022	Goal: 60877
Count of epochs trained: 60147	Goal: 60877
Count of epochs trained: 60272	Goal: 60877
Count of epochs trained: 60397	Goal: 60877
Count of epochs trained: 60522	Goal: 60877
Count of epochs trained: 60647	Goal: 60877
Count of epochs trained: 60771	Goal: 60877
Saved model from episode 106. Count of epochs trained: 60897
  3%|2         | 27/921 [4:03:24<153:20:43, 617.50s/episode]
Started episode 107 of 1000
Finished episode 107 of 1000
Count of epochs trained: 61160	Goal: 62136
Count of epochs trained: 61285	Goal: 62136
Count of epochs trained: 61409	Goal: 62136
Count of epochs trained: 61534	Goal: 62136
Count of epochs trained: 61658	Goal: 62136
Count of epochs trained: 61783	Goal: 62136
Count of epochs trained: 61906	Goal: 62136
Count of epochs trained: 62031	Goal: 62136
Saved model from episode 107. Count of epochs trained: 62155
  3%|3         | 28/921 [4:14:00<154:32:52, 623.04s/episode]
Started episode 108 of 1000
Finished episode 108 of 1000
Count of epochs trained: 62164	Goal: 63394
Count of epochs trained: 62288	Goal: 63394
Count of epochs trained: 62412	Goal: 63394
Count of epochs trained: 62536	Goal: 63394
Count of epochs trained: 62660	Goal: 63394
Count of epochs trained: 62784	Goal: 63394
Count of epochs trained: 62908	Goal: 63394
Count of epochs trained: 63032	Goal: 63394
Count of epochs trained: 63156	Goal: 63394
Count of epochs trained: 63281	Goal: 63394
Saved model from episode 108. Count of epochs trained: 63406
  3%|3         | 29/921 [4:24:06<153:08:45, 618.08s/episode]
Started episode 109 of 1000
Finished episode 109 of 1000
Count of epochs trained: 63427	Goal: 64645
Count of epochs trained: 63552	Goal: 64645
Count of epochs trained: 63676	Goal: 64645
Count of epochs trained: 63800	Goal: 64645
Count of epochs trained: 63923	Goal: 64645
Count of epochs trained: 64041	Goal: 64645
Count of epochs trained: 64165	Goal: 64645
Count of epochs trained: 64287	Goal: 64645
Count of epochs trained: 64411	Goal: 64645
Count of epochs trained: 64535	Goal: 64645
Saved model from episode 109. Count of epochs trained: 64659
  3%|3         | 30/921 [4:34:20<152:41:20, 616.92s/episode]
Started episode 110 of 1000
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 459, in train_in_loop
    self.train()
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 379, in train
    current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE, verbose=0)
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/nsambhu/anaconda3/envs/carla_py3.9/lib/python3.9/site-packages/keras/src/engine/training.py", line 2655, in predict
    tmp_batch_outputs = self.predict_function(iterator)
TypeError: 'NoneType' object is not callable
Finished episode 110 of 1000
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
Count of epochs trained: 64918	Goal: 65898
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 387
    new_current_states = np.array([transition[3] for transition in minibatch])
IndentationError: unexpected unindent
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 387
    new_current_states = np.array([transition[3] for transition in minibatch])
IndentationError: unexpected unindent
2023-12-30 22:17:13.453224: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 22:17:13.475206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 22:17:13.475222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 22:17:13.475765: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 22:17:13.479118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 22:17:13.921194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 22:17:14.243745: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:14.276981: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:14.277212: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:14.280286: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:14.280475: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:14.280990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:15.085660: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:15.085788: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:15.085883: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:15.086021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4409 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Models in tmp ['tmp/079.31282.model', 'tmp/080.31529.model', 'tmp/081.31830.model', 'tmp/082.32126.model', 'tmp/083.32417.model', 'tmp/084.32703.model', 'tmp/085.33983.model', 'tmp/086.35278.model', 'tmp/087.36554.model', 'tmp/088.37897.model', 'tmp/089.39237.model', 'tmp/090.40564.model', 'tmp/091.41861.model', 'tmp/092.43139.model', 'tmp/093.44381.model', 'tmp/094.45631.model', 'tmp/095.46934.model', 'tmp/096.48218.model', 'tmp/097.49483.model', 'tmp/098.50749.model', 'tmp/099.52001.model', 'tmp/100.53244.model', 'tmp/101.54592.model', 'tmp/102.55842.model', 'tmp/103.57099.model', 'tmp/104.58374.model', 'tmp/105.59637.model', 'tmp/106.60896.model', 'tmp/107.62155.model', 'tmp/108.63405.model', 'tmp/109.64659.model']
Load model tmp/109.64659.model
Leftover images from failed episode: ['_out_16rl_custom2/110_047810.png', '_out_16rl_custom2/110_047812.png', '_out_16rl_custom2/110_047813.png', '_out_16rl_custom2/110_047815.png', '_out_16rl_custom2/110_047816.png', '_out_16rl_custom2/110_047817.png', '_out_16rl_custom2/110_047818.png', '_out_16rl_custom2/110_047819.png', '_out_16rl_custom2/110_047820.png', '_out_16rl_custom2/110_047822.png', '_out_16rl_custom2/110_047823.png', '_out_16rl_custom2/110_047824.png', '_out_16rl_custom2/110_047825.png', '_out_16rl_custom2/110_047826.png', '_out_16rl_custom2/110_047827.png', '_out_16rl_custom2/110_047828.png', '_out_16rl_custom2/110_047829.png', '_out_16rl_custom2/110_047830.png', '_out_16rl_custom2/110_047831.png', '_out_16rl_custom2/110_047832.png', '_out_16rl_custom2/110_047833.png', '_out_16rl_custom2/110_047834.png', '_out_16rl_custom2/110_047835.png', '_out_16rl_custom2/110_047836.png', '_out_16rl_custom2/110_047837.png', '_out_16rl_custom2/110_047838.png', '_out_16rl_custom2/110_047840.png', '_out_16rl_custom2/110_047841.png', '_out_16rl_custom2/110_047842.png', '_out_16rl_custom2/110_047843.png', '_out_16rl_custom2/110_047844.png', '_out_16rl_custom2/110_047845.png', '_out_16rl_custom2/110_047846.png', '_out_16rl_custom2/110_047847.png', '_out_16rl_custom2/110_047848.png', '_out_16rl_custom2/110_047850.png', '_out_16rl_custom2/110_047852.png', '_out_16rl_custom2/110_047853.png', '_out_16rl_custom2/110_047854.png', '_out_16rl_custom2/110_047855.png', '_out_16rl_custom2/110_047856.png', '_out_16rl_custom2/110_047857.png', '_out_16rl_custom2/110_047859.png', '_out_16rl_custom2/110_047860.png', '_out_16rl_custom2/110_047862.png', '_out_16rl_custom2/110_047863.png', '_out_16rl_custom2/110_047864.png', '_out_16rl_custom2/110_047865.png', '_out_16rl_custom2/110_047867.png', '_out_16rl_custom2/110_047868.png', '_out_16rl_custom2/110_047869.png', '_out_16rl_custom2/110_047871.png', '_out_16rl_custom2/110_047872.png', '_out_16rl_custom2/110_047873.png', '_out_16rl_custom2/110_047874.png', '_out_16rl_custom2/110_047875.png', '_out_16rl_custom2/110_047876.png', '_out_16rl_custom2/110_047878.png', '_out_16rl_custom2/110_047879.png', '_out_16rl_custom2/110_047880.png', '_out_16rl_custom2/110_047881.png', '_out_16rl_custom2/110_047882.png', '_out_16rl_custom2/110_047883.png', '_out_16rl_custom2/110_047884.png', '_out_16rl_custom2/110_047885.png', '_out_16rl_custom2/110_047886.png', '_out_16rl_custom2/110_047887.png', '_out_16rl_custom2/110_047888.png', '_out_16rl_custom2/110_047889.png', '_out_16rl_custom2/110_047890.png', '_out_16rl_custom2/110_047891.png', '_out_16rl_custom2/110_047892.png', '_out_16rl_custom2/110_047893.png', '_out_16rl_custom2/110_047894.png', '_out_16rl_custom2/110_047896.png', '_out_16rl_custom2/110_047897.png', '_out_16rl_custom2/110_047898.png', '_out_16rl_custom2/110_047899.png', '_out_16rl_custom2/110_047900.png', '_out_16rl_custom2/110_047901.png', '_out_16rl_custom2/110_047902.png', '_out_16rl_custom2/110_047903.png', '_out_16rl_custom2/110_047905.png', '_out_16rl_custom2/110_047906.png', '_out_16rl_custom2/110_047907.png', '_out_16rl_custom2/110_047908.png', '_out_16rl_custom2/110_047909.png', '_out_16rl_custom2/110_047910.png', '_out_16rl_custom2/110_047911.png', '_out_16rl_custom2/110_047912.png', '_out_16rl_custom2/110_047913.png', '_out_16rl_custom2/110_047914.png', '_out_16rl_custom2/110_047916.png', '_out_16rl_custom2/110_047917.png', '_out_16rl_custom2/110_047918.png', '_out_16rl_custom2/110_047919.png', '_out_16rl_custom2/110_047920.png', '_out_16rl_custom2/110_047922.png', '_out_16rl_custom2/110_047923.png', '_out_16rl_custom2/110_047925.png', '_out_16rl_custom2/110_047926.png', '_out_16rl_custom2/110_047928.png', '_out_16rl_custom2/110_047929.png', '_out_16rl_custom2/110_047930.png', '_out_16rl_custom2/110_047931.png', '_out_16rl_custom2/110_047932.png', '_out_16rl_custom2/110_047933.png', '_out_16rl_custom2/110_047934.png', '_out_16rl_custom2/110_047935.png', '_out_16rl_custom2/110_047936.png', '_out_16rl_custom2/110_047937.png', '_out_16rl_custom2/110_047938.png', '_out_16rl_custom2/110_047939.png', '_out_16rl_custom2/110_047940.png', '_out_16rl_custom2/110_047941.png', '_out_16rl_custom2/110_047942.png', '_out_16rl_custom2/110_047943.png', '_out_16rl_custom2/110_047944.png', '_out_16rl_custom2/110_047945.png', '_out_16rl_custom2/110_047946.png', '_out_16rl_custom2/110_047947.png', '_out_16rl_custom2/110_047948.png', '_out_16rl_custom2/110_047949.png', '_out_16rl_custom2/110_047950.png', '_out_16rl_custom2/110_047951.png', '_out_16rl_custom2/110_047952.png', '_out_16rl_custom2/110_047953.png', '_out_16rl_custom2/110_047954.png', '_out_16rl_custom2/110_047955.png', '_out_16rl_custom2/110_047956.png', '_out_16rl_custom2/110_047957.png', '_out_16rl_custom2/110_047958.png', '_out_16rl_custom2/110_047959.png', '_out_16rl_custom2/110_047960.png', '_out_16rl_custom2/110_047961.png', '_out_16rl_custom2/110_047962.png', '_out_16rl_custom2/110_047963.png', '_out_16rl_custom2/110_047964.png', '_out_16rl_custom2/110_047966.png', '_out_16rl_custom2/110_047967.png', '_out_16rl_custom2/110_047968.png', '_out_16rl_custom2/110_047969.png', '_out_16rl_custom2/110_047970.png', '_out_16rl_custom2/110_047971.png', '_out_16rl_custom2/110_047972.png', '_out_16rl_custom2/110_047974.png', '_out_16rl_custom2/110_047976.png', '_out_16rl_custom2/110_047977.png', '_out_16rl_custom2/110_047978.png', '_out_16rl_custom2/110_047980.png', '_out_16rl_custom2/110_047981.png', '_out_16rl_custom2/110_047982.png', '_out_16rl_custom2/110_047983.png', '_out_16rl_custom2/110_047984.png', '_out_16rl_custom2/110_047985.png', '_out_16rl_custom2/110_047986.png', '_out_16rl_custom2/110_047987.png', '_out_16rl_custom2/110_047988.png', '_out_16rl_custom2/110_047989.png', '_out_16rl_custom2/110_047990.png', '_out_16rl_custom2/110_047991.png', '_out_16rl_custom2/110_047992.png', '_out_16rl_custom2/110_047993.png', '_out_16rl_custom2/110_047995.png', '_out_16rl_custom2/110_047996.png', '_out_16rl_custom2/110_047997.png', '_out_16rl_custom2/110_047999.png', '_out_16rl_custom2/110_048000.png', '_out_16rl_custom2/110_048001.png', '_out_16rl_custom2/110_048002.png', '_out_16rl_custom2/110_048003.png', '_out_16rl_custom2/110_048004.png', '_out_16rl_custom2/110_048005.png', '_out_16rl_custom2/110_048006.png', '_out_16rl_custom2/110_048007.png', '_out_16rl_custom2/110_048008.png', '_out_16rl_custom2/110_048009.png', '_out_16rl_custom2/110_048010.png', '_out_16rl_custom2/110_048011.png', '_out_16rl_custom2/110_048012.png', '_out_16rl_custom2/110_048013.png', '_out_16rl_custom2/110_048014.png', '_out_16rl_custom2/110_048016.png', '_out_16rl_custom2/110_048017.png', '_out_16rl_custom2/110_048018.png', '_out_16rl_custom2/110_048020.png', '_out_16rl_custom2/110_048021.png', '_out_16rl_custom2/110_048022.png', '_out_16rl_custom2/110_048023.png', '_out_16rl_custom2/110_048024.png', '_out_16rl_custom2/110_048025.png', '_out_16rl_custom2/110_048026.png', '_out_16rl_custom2/110_048027.png', '_out_16rl_custom2/110_048028.png', '_out_16rl_custom2/110_048029.png', '_out_16rl_custom2/110_048030.png', '_out_16rl_custom2/110_048031.png', '_out_16rl_custom2/110_048032.png', '_out_16rl_custom2/110_048033.png', '_out_16rl_custom2/110_048035.png', '_out_16rl_custom2/110_048036.png', '_out_16rl_custom2/110_048037.png', '_out_16rl_custom2/110_048039.png', '_out_16rl_custom2/110_048040.png', '_out_16rl_custom2/110_048041.png', '_out_16rl_custom2/110_048043.png', '_out_16rl_custom2/110_048044.png', '_out_16rl_custom2/110_048045.png', '_out_16rl_custom2/110_048046.png', '_out_16rl_custom2/110_048047.png', '_out_16rl_custom2/110_048048.png', '_out_16rl_custom2/110_048050.png', '_out_16rl_custom2/110_048051.png', '_out_16rl_custom2/110_048053.png', '_out_16rl_custom2/110_048054.png', '_out_16rl_custom2/110_048056.png', '_out_16rl_custom2/110_048057.png', '_out_16rl_custom2/110_048058.png', '_out_16rl_custom2/110_048059.png', '_out_16rl_custom2/110_048060.png', '_out_16rl_custom2/110_048061.png', '_out_16rl_custom2/110_048062.png', '_out_16rl_custom2/110_048064.png', '_out_16rl_custom2/110_048065.png', '_out_16rl_custom2/110_048066.png', '_out_16rl_custom2/110_048068.png', '_out_16rl_custom2/110_048070.png', '_out_16rl_custom2/110_048071.png', '_out_16rl_custom2/110_048072.png', '_out_16rl_custom2/110_048074.png', '_out_16rl_custom2/110_048075.png', '_out_16rl_custom2/110_048076.png', '_out_16rl_custom2/110_048078.png', '_out_16rl_custom2/110_048079.png', '_out_16rl_custom2/110_048081.png', '_out_16rl_custom2/110_048082.png', '_out_16rl_custom2/110_048083.png', '_out_16rl_custom2/110_048084.png', '_out_16rl_custom2/110_048085.png', '_out_16rl_custom2/110_048086.png', '_out_16rl_custom2/110_048087.png', '_out_16rl_custom2/110_048088.png', '_out_16rl_custom2/110_048090.png', '_out_16rl_custom2/110_048091.png', '_out_16rl_custom2/110_048092.png', '_out_16rl_custom2/110_048093.png', '_out_16rl_custom2/110_048094.png', '_out_16rl_custom2/110_048095.png', '_out_16rl_custom2/110_048096.png', '_out_16rl_custom2/110_048097.png', '_out_16rl_custom2/110_048098.png', '_out_16rl_custom2/110_048099.png', '_out_16rl_custom2/110_048100.png', '_out_16rl_custom2/110_048101.png', '_out_16rl_custom2/110_048102.png', '_out_16rl_custom2/110_048103.png', '_out_16rl_custom2/110_048104.png', '_out_16rl_custom2/110_048105.png', '_out_16rl_custom2/110_048106.png', '_out_16rl_custom2/110_048107.png', '_out_16rl_custom2/110_048108.png', '_out_16rl_custom2/110_048109.png', '_out_16rl_custom2/110_048110.png', '_out_16rl_custom2/110_048111.png', '_out_16rl_custom2/110_048112.png', '_out_16rl_custom2/110_048113.png', '_out_16rl_custom2/110_048114.png', '_out_16rl_custom2/110_048115.png', '_out_16rl_custom2/110_048116.png', '_out_16rl_custom2/110_048118.png', '_out_16rl_custom2/110_048119.png', '_out_16rl_custom2/110_048120.png', '_out_16rl_custom2/110_048122.png', '_out_16rl_custom2/110_048123.png', '_out_16rl_custom2/110_048124.png', '_out_16rl_custom2/110_048125.png', '_out_16rl_custom2/110_048126.png', '_out_16rl_custom2/110_048127.png', '_out_16rl_custom2/110_048128.png', '_out_16rl_custom2/110_048129.png', '_out_16rl_custom2/110_048131.png', '_out_16rl_custom2/110_048132.png', '_out_16rl_custom2/110_048133.png', '_out_16rl_custom2/110_048134.png', '_out_16rl_custom2/110_048135.png', '_out_16rl_custom2/110_048136.png', '_out_16rl_custom2/110_048137.png', '_out_16rl_custom2/110_048138.png', '_out_16rl_custom2/110_048139.png', '_out_16rl_custom2/110_048140.png', '_out_16rl_custom2/110_048141.png', '_out_16rl_custom2/110_048142.png', '_out_16rl_custom2/110_048143.png', '_out_16rl_custom2/110_048144.png', '_out_16rl_custom2/110_048146.png', '_out_16rl_custom2/110_048147.png', '_out_16rl_custom2/110_048148.png', '_out_16rl_custom2/110_048149.png', '_out_16rl_custom2/110_048150.png', '_out_16rl_custom2/110_048151.png', '_out_16rl_custom2/110_048152.png', '_out_16rl_custom2/110_048153.png', '_out_16rl_custom2/110_048154.png', '_out_16rl_custom2/110_048155.png', '_out_16rl_custom2/110_048156.png', '_out_16rl_custom2/110_048158.png', '_out_16rl_custom2/110_048159.png', '_out_16rl_custom2/110_048161.png', '_out_16rl_custom2/110_048162.png', '_out_16rl_custom2/110_048163.png', '_out_16rl_custom2/110_048164.png', '_out_16rl_custom2/110_048165.png', '_out_16rl_custom2/110_048166.png', '_out_16rl_custom2/110_048167.png', '_out_16rl_custom2/110_048168.png', '_out_16rl_custom2/110_048169.png', '_out_16rl_custom2/110_048170.png', '_out_16rl_custom2/110_048171.png', '_out_16rl_custom2/110_048172.png', '_out_16rl_custom2/110_048173.png', '_out_16rl_custom2/110_048174.png', '_out_16rl_custom2/110_048175.png', '_out_16rl_custom2/110_048176.png', '_out_16rl_custom2/110_048177.png', '_out_16rl_custom2/110_048178.png', '_out_16rl_custom2/110_048179.png', '_out_16rl_custom2/110_048180.png', '_out_16rl_custom2/110_048181.png', '_out_16rl_custom2/110_048182.png', '_out_16rl_custom2/110_048183.png', '_out_16rl_custom2/110_048184.png', '_out_16rl_custom2/110_048185.png', '_out_16rl_custom2/110_048186.png', '_out_16rl_custom2/110_048187.png', '_out_16rl_custom2/110_048189.png', '_out_16rl_custom2/110_048190.png', '_out_16rl_custom2/110_048192.png', '_out_16rl_custom2/110_048193.png', '_out_16rl_custom2/110_048194.png', '_out_16rl_custom2/110_048195.png', '_out_16rl_custom2/110_048196.png', '_out_16rl_custom2/110_048197.png', '_out_16rl_custom2/110_048198.png', '_out_16rl_custom2/110_048199.png', '_out_16rl_custom2/110_048200.png', '_out_16rl_custom2/110_048201.png', '_out_16rl_custom2/110_048202.png', '_out_16rl_custom2/110_048203.png', '_out_16rl_custom2/110_048204.png', '_out_16rl_custom2/110_048205.png', '_out_16rl_custom2/110_048206.png', '_out_16rl_custom2/110_048208.png', '_out_16rl_custom2/110_048209.png', '_out_16rl_custom2/110_048210.png', '_out_16rl_custom2/110_048211.png', '_out_16rl_custom2/110_048212.png', '_out_16rl_custom2/110_048213.png', '_out_16rl_custom2/110_048214.png', '_out_16rl_custom2/110_048215.png', '_out_16rl_custom2/110_048216.png', '_out_16rl_custom2/110_048217.png', '_out_16rl_custom2/110_048218.png', '_out_16rl_custom2/110_048219.png', '_out_16rl_custom2/110_048221.png', '_out_16rl_custom2/110_048222.png', '_out_16rl_custom2/110_048223.png', '_out_16rl_custom2/110_048224.png', '_out_16rl_custom2/110_048225.png', '_out_16rl_custom2/110_048226.png', '_out_16rl_custom2/110_048227.png', '_out_16rl_custom2/110_048228.png', '_out_16rl_custom2/110_048229.png', '_out_16rl_custom2/110_048230.png', '_out_16rl_custom2/110_048231.png', '_out_16rl_custom2/110_048232.png', '_out_16rl_custom2/110_048233.png', '_out_16rl_custom2/110_048234.png', '_out_16rl_custom2/110_048235.png', '_out_16rl_custom2/110_048236.png', '_out_16rl_custom2/110_048237.png', '_out_16rl_custom2/110_048238.png', '_out_16rl_custom2/110_048239.png', '_out_16rl_custom2/110_048240.png', '_out_16rl_custom2/110_048242.png', '_out_16rl_custom2/110_048243.png', '_out_16rl_custom2/110_048244.png', '_out_16rl_custom2/110_048245.png', '_out_16rl_custom2/110_048246.png', '_out_16rl_custom2/110_048247.png', '_out_16rl_custom2/110_048248.png', '_out_16rl_custom2/110_048249.png', '_out_16rl_custom2/110_048250.png', '_out_16rl_custom2/110_048251.png', '_out_16rl_custom2/110_048252.png', '_out_16rl_custom2/110_048253.png', '_out_16rl_custom2/110_048254.png', '_out_16rl_custom2/110_048255.png', '_out_16rl_custom2/110_048256.png', '_out_16rl_custom2/110_048257.png', '_out_16rl_custom2/110_048259.png', '_out_16rl_custom2/110_048260.png', '_out_16rl_custom2/110_048261.png', '_out_16rl_custom2/110_048262.png', '_out_16rl_custom2/110_048263.png', '_out_16rl_custom2/110_048264.png', '_out_16rl_custom2/110_048265.png', '_out_16rl_custom2/110_048266.png', '_out_16rl_custom2/110_048267.png', '_out_16rl_custom2/110_048268.png', '_out_16rl_custom2/110_048269.png', '_out_16rl_custom2/110_048270.png', '_out_16rl_custom2/110_048272.png', '_out_16rl_custom2/110_048273.png', '_out_16rl_custom2/110_048274.png', '_out_16rl_custom2/110_048275.png', '_out_16rl_custom2/110_048276.png', '_out_16rl_custom2/110_048277.png', '_out_16rl_custom2/110_048278.png', '_out_16rl_custom2/110_048280.png', '_out_16rl_custom2/110_048281.png', '_out_16rl_custom2/110_048282.png', '_out_16rl_custom2/110_048283.png', '_out_16rl_custom2/110_048284.png', '_out_16rl_custom2/110_048285.png', '_out_16rl_custom2/110_048286.png', '_out_16rl_custom2/110_048287.png', '_out_16rl_custom2/110_048289.png', '_out_16rl_custom2/110_048290.png', '_out_16rl_custom2/110_048291.png', '_out_16rl_custom2/110_048292.png', '_out_16rl_custom2/110_048293.png', '_out_16rl_custom2/110_048294.png', '_out_16rl_custom2/110_048295.png', '_out_16rl_custom2/110_048296.png', '_out_16rl_custom2/110_048297.png', '_out_16rl_custom2/110_048298.png', '_out_16rl_custom2/110_048299.png', '_out_16rl_custom2/110_048300.png', '_out_16rl_custom2/110_048301.png', '_out_16rl_custom2/110_048302.png', '_out_16rl_custom2/110_048303.png', '_out_16rl_custom2/110_048304.png', '_out_16rl_custom2/110_048305.png', '_out_16rl_custom2/110_048306.png', '_out_16rl_custom2/110_048308.png', '_out_16rl_custom2/110_048309.png', '_out_16rl_custom2/110_048310.png', '_out_16rl_custom2/110_048312.png', '_out_16rl_custom2/110_048313.png', '_out_16rl_custom2/110_048314.png', '_out_16rl_custom2/110_048315.png', '_out_16rl_custom2/110_048317.png', '_out_16rl_custom2/110_048319.png', '_out_16rl_custom2/110_048320.png', '_out_16rl_custom2/110_048321.png', '_out_16rl_custom2/110_048322.png', '_out_16rl_custom2/110_048323.png', '_out_16rl_custom2/110_048324.png', '_out_16rl_custom2/110_048325.png', '_out_16rl_custom2/110_048327.png', '_out_16rl_custom2/110_048329.png', '_out_16rl_custom2/110_048330.png', '_out_16rl_custom2/110_048331.png', '_out_16rl_custom2/110_048332.png', '_out_16rl_custom2/110_048333.png', '_out_16rl_custom2/110_048334.png', '_out_16rl_custom2/110_048335.png', '_out_16rl_custom2/110_048336.png', '_out_16rl_custom2/110_048337.png', '_out_16rl_custom2/110_048338.png', '_out_16rl_custom2/110_048339.png', '_out_16rl_custom2/110_048340.png', '_out_16rl_custom2/110_048341.png', '_out_16rl_custom2/110_048342.png', '_out_16rl_custom2/110_048343.png', '_out_16rl_custom2/110_048345.png', '_out_16rl_custom2/110_048346.png', '_out_16rl_custom2/110_048347.png', '_out_16rl_custom2/110_048348.png', '_out_16rl_custom2/110_048349.png', '_out_16rl_custom2/110_048350.png', '_out_16rl_custom2/110_048351.png', '_out_16rl_custom2/110_048352.png', '_out_16rl_custom2/110_048353.png', '_out_16rl_custom2/110_048354.png', '_out_16rl_custom2/110_048355.png', '_out_16rl_custom2/110_048356.png', '_out_16rl_custom2/110_048357.png', '_out_16rl_custom2/110_048358.png', '_out_16rl_custom2/110_048359.png', '_out_16rl_custom2/110_048360.png', '_out_16rl_custom2/110_048361.png', '_out_16rl_custom2/110_048362.png', '_out_16rl_custom2/110_048363.png', '_out_16rl_custom2/110_048364.png', '_out_16rl_custom2/110_048365.png', '_out_16rl_custom2/110_048366.png', '_out_16rl_custom2/110_048367.png', '_out_16rl_custom2/110_048368.png', '_out_16rl_custom2/110_048369.png', '_out_16rl_custom2/110_048370.png', '_out_16rl_custom2/110_048371.png', '_out_16rl_custom2/110_048372.png', '_out_16rl_custom2/110_048373.png', '_out_16rl_custom2/110_048374.png', '_out_16rl_custom2/110_048375.png', '_out_16rl_custom2/110_048376.png', '_out_16rl_custom2/110_048377.png', '_out_16rl_custom2/110_048379.png', '_out_16rl_custom2/110_048380.png', '_out_16rl_custom2/110_048381.png', '_out_16rl_custom2/110_048382.png', '_out_16rl_custom2/110_048383.png', '_out_16rl_custom2/110_048384.png', '_out_16rl_custom2/110_048385.png', '_out_16rl_custom2/110_048386.png', '_out_16rl_custom2/110_048387.png', '_out_16rl_custom2/110_048388.png', '_out_16rl_custom2/110_048389.png', '_out_16rl_custom2/110_048390.png', '_out_16rl_custom2/110_048391.png', '_out_16rl_custom2/110_048393.png', '_out_16rl_custom2/110_048394.png', '_out_16rl_custom2/110_048395.png', '_out_16rl_custom2/110_048396.png', '_out_16rl_custom2/110_048397.png', '_out_16rl_custom2/110_048398.png', '_out_16rl_custom2/110_048399.png', '_out_16rl_custom2/110_048400.png', '_out_16rl_custom2/110_048401.png', '_out_16rl_custom2/110_048402.png', '_out_16rl_custom2/110_048403.png', '_out_16rl_custom2/110_048404.png', '_out_16rl_custom2/110_048405.png', '_out_16rl_custom2/110_048406.png', '_out_16rl_custom2/110_048407.png', '_out_16rl_custom2/110_048409.png', '_out_16rl_custom2/110_048410.png', '_out_16rl_custom2/110_048411.png', '_out_16rl_custom2/110_048412.png', '_out_16rl_custom2/110_048413.png', '_out_16rl_custom2/110_048415.png', '_out_16rl_custom2/110_048416.png', '_out_16rl_custom2/110_048417.png', '_out_16rl_custom2/110_048418.png', '_out_16rl_custom2/110_048419.png', '_out_16rl_custom2/110_048420.png', '_out_16rl_custom2/110_048421.png', '_out_16rl_custom2/110_048422.png', '_out_16rl_custom2/110_048423.png', '_out_16rl_custom2/110_048424.png', '_out_16rl_custom2/110_048425.png', '_out_16rl_custom2/110_048426.png', '_out_16rl_custom2/110_048427.png', '_out_16rl_custom2/110_048428.png', '_out_16rl_custom2/110_048429.png', '_out_16rl_custom2/110_048430.png', '_out_16rl_custom2/110_048432.png', '_out_16rl_custom2/110_048433.png', '_out_16rl_custom2/110_048434.png', '_out_16rl_custom2/110_048435.png', '_out_16rl_custom2/110_048436.png', '_out_16rl_custom2/110_048437.png', '_out_16rl_custom2/110_048438.png', '_out_16rl_custom2/110_048439.png', '_out_16rl_custom2/110_048440.png', '_out_16rl_custom2/110_048441.png', '_out_16rl_custom2/110_048442.png', '_out_16rl_custom2/110_048443.png', '_out_16rl_custom2/110_048444.png', '_out_16rl_custom2/110_048445.png', '_out_16rl_custom2/110_048446.png', '_out_16rl_custom2/110_048447.png', '_out_16rl_custom2/110_048448.png', '_out_16rl_custom2/110_048449.png', '_out_16rl_custom2/110_048450.png', '_out_16rl_custom2/110_048451.png', '_out_16rl_custom2/110_048453.png', '_out_16rl_custom2/110_048454.png', '_out_16rl_custom2/110_048455.png', '_out_16rl_custom2/110_048456.png', '_out_16rl_custom2/110_048458.png', '_out_16rl_custom2/110_048459.png', '_out_16rl_custom2/110_048460.png', '_out_16rl_custom2/110_048462.png', '_out_16rl_custom2/110_048463.png', '_out_16rl_custom2/110_048464.png', '_out_16rl_custom2/110_048465.png', '_out_16rl_custom2/110_048466.png', '_out_16rl_custom2/110_048467.png', '_out_16rl_custom2/110_048468.png', '_out_16rl_custom2/110_048469.png', '_out_16rl_custom2/110_048470.png', '_out_16rl_custom2/110_048471.png', '_out_16rl_custom2/110_048472.png', '_out_16rl_custom2/110_048473.png', '_out_16rl_custom2/110_048474.png', '_out_16rl_custom2/110_048475.png', '_out_16rl_custom2/110_048476.png', '_out_16rl_custom2/110_048478.png', '_out_16rl_custom2/110_048480.png', '_out_16rl_custom2/110_048481.png', '_out_16rl_custom2/110_048482.png', '_out_16rl_custom2/110_048483.png', '_out_16rl_custom2/110_048484.png', '_out_16rl_custom2/110_048485.png', '_out_16rl_custom2/110_048486.png', '_out_16rl_custom2/110_048487.png', '_out_16rl_custom2/110_048489.png', '_out_16rl_custom2/110_048490.png', '_out_16rl_custom2/110_048491.png', '_out_16rl_custom2/110_048492.png', '_out_16rl_custom2/110_048493.png', '_out_16rl_custom2/110_048494.png', '_out_16rl_custom2/110_048495.png', '_out_16rl_custom2/110_048496.png', '_out_16rl_custom2/110_048497.png', '_out_16rl_custom2/110_048498.png', '_out_16rl_custom2/110_048499.png', '_out_16rl_custom2/110_048500.png', '_out_16rl_custom2/110_048501.png', '_out_16rl_custom2/110_048502.png', '_out_16rl_custom2/110_048503.png', '_out_16rl_custom2/110_048504.png', '_out_16rl_custom2/110_048505.png', '_out_16rl_custom2/110_048507.png', '_out_16rl_custom2/110_048508.png', '_out_16rl_custom2/110_048509.png', '_out_16rl_custom2/110_048510.png', '_out_16rl_custom2/110_048511.png', '_out_16rl_custom2/110_048512.png', '_out_16rl_custom2/110_048513.png', '_out_16rl_custom2/110_048514.png', '_out_16rl_custom2/110_048515.png', '_out_16rl_custom2/110_048516.png', '_out_16rl_custom2/110_048517.png', '_out_16rl_custom2/110_048518.png', '_out_16rl_custom2/110_048519.png', '_out_16rl_custom2/110_048520.png', '_out_16rl_custom2/110_048521.png', '_out_16rl_custom2/110_048522.png', '_out_16rl_custom2/110_048523.png', '_out_16rl_custom2/110_048524.png', '_out_16rl_custom2/110_048525.png', '_out_16rl_custom2/110_048526.png', '_out_16rl_custom2/110_048527.png', '_out_16rl_custom2/110_048528.png', '_out_16rl_custom2/110_048529.png', '_out_16rl_custom2/110_048531.png', '_out_16rl_custom2/110_048532.png', '_out_16rl_custom2/110_048533.png', '_out_16rl_custom2/110_048534.png', '_out_16rl_custom2/110_048535.png', '_out_16rl_custom2/110_048536.png', '_out_16rl_custom2/110_048537.png', '_out_16rl_custom2/110_048538.png', '_out_16rl_custom2/110_048540.png', '_out_16rl_custom2/110_048541.png', '_out_16rl_custom2/110_048542.png', '_out_16rl_custom2/110_048543.png', '_out_16rl_custom2/110_048544.png', '_out_16rl_custom2/110_048545.png', '_out_16rl_custom2/110_048546.png', '_out_16rl_custom2/110_048547.png', '_out_16rl_custom2/110_048548.png', '_out_16rl_custom2/110_048549.png', '_out_16rl_custom2/110_048550.png', '_out_16rl_custom2/110_048551.png', '_out_16rl_custom2/110_048552.png', '_out_16rl_custom2/110_048553.png', '_out_16rl_custom2/110_048554.png', '_out_16rl_custom2/110_048555.png', '_out_16rl_custom2/110_048556.png', '_out_16rl_custom2/110_048557.png', '_out_16rl_custom2/110_048558.png', '_out_16rl_custom2/110_048559.png', '_out_16rl_custom2/110_048560.png', '_out_16rl_custom2/110_048561.png', '_out_16rl_custom2/110_048562.png', '_out_16rl_custom2/110_048563.png', '_out_16rl_custom2/110_048565.png', '_out_16rl_custom2/110_048566.png', '_out_16rl_custom2/110_048567.png', '_out_16rl_custom2/110_048568.png', '_out_16rl_custom2/110_048569.png', '_out_16rl_custom2/110_048570.png', '_out_16rl_custom2/110_048571.png', '_out_16rl_custom2/110_048572.png', '_out_16rl_custom2/110_048574.png', '_out_16rl_custom2/110_048575.png', '_out_16rl_custom2/110_048576.png', '_out_16rl_custom2/110_048577.png', '_out_16rl_custom2/110_048578.png', '_out_16rl_custom2/110_048579.png', '_out_16rl_custom2/110_048581.png', '_out_16rl_custom2/110_048582.png', '_out_16rl_custom2/110_048583.png', '_out_16rl_custom2/110_048585.png', '_out_16rl_custom2/110_048587.png', '_out_16rl_custom2/110_048588.png', '_out_16rl_custom2/110_048589.png', '_out_16rl_custom2/110_048591.png', '_out_16rl_custom2/110_048592.png', '_out_16rl_custom2/110_048593.png', '_out_16rl_custom2/110_048594.png', '_out_16rl_custom2/110_048595.png', '_out_16rl_custom2/110_048596.png', '_out_16rl_custom2/110_048597.png', '_out_16rl_custom2/110_048598.png', '_out_16rl_custom2/110_048599.png', '_out_16rl_custom2/110_048600.png', '_out_16rl_custom2/110_048601.png', '_out_16rl_custom2/110_048602.png', '_out_16rl_custom2/110_048603.png', '_out_16rl_custom2/110_048604.png', '_out_16rl_custom2/110_048605.png', '_out_16rl_custom2/110_048606.png', '_out_16rl_custom2/110_048607.png', '_out_16rl_custom2/110_048608.png', '_out_16rl_custom2/110_048610.png', '_out_16rl_custom2/110_048611.png', '_out_16rl_custom2/110_048613.png', '_out_16rl_custom2/110_048614.png', '_out_16rl_custom2/110_048615.png', '_out_16rl_custom2/110_048616.png', '_out_16rl_custom2/110_048617.png', '_out_16rl_custom2/110_048618.png', '_out_16rl_custom2/110_048619.png', '_out_16rl_custom2/110_048620.png', '_out_16rl_custom2/110_048621.png', '_out_16rl_custom2/110_048622.png', '_out_16rl_custom2/110_048623.png', '_out_16rl_custom2/110_048624.png', '_out_16rl_custom2/110_048625.png', '_out_16rl_custom2/110_048626.png', '_out_16rl_custom2/110_048627.png', '_out_16rl_custom2/110_048628.png', '_out_16rl_custom2/110_048629.png', '_out_16rl_custom2/110_048630.png', '_out_16rl_custom2/110_048631.png', '_out_16rl_custom2/110_048632.png', '_out_16rl_custom2/110_048633.png', '_out_16rl_custom2/110_048634.png', '_out_16rl_custom2/110_048636.png', '_out_16rl_custom2/110_048637.png', '_out_16rl_custom2/110_048638.png', '_out_16rl_custom2/110_048639.png', '_out_16rl_custom2/110_048640.png', '_out_16rl_custom2/110_048641.png', '_out_16rl_custom2/110_048642.png', '_out_16rl_custom2/110_048643.png', '_out_16rl_custom2/110_048645.png', '_out_16rl_custom2/110_048647.png', '_out_16rl_custom2/110_048648.png', '_out_16rl_custom2/110_048649.png', '_out_16rl_custom2/110_048650.png', '_out_16rl_custom2/110_048651.png', '_out_16rl_custom2/110_048652.png', '_out_16rl_custom2/110_048653.png', '_out_16rl_custom2/110_048655.png', '_out_16rl_custom2/110_048656.png', '_out_16rl_custom2/110_048657.png', '_out_16rl_custom2/110_048658.png', '_out_16rl_custom2/110_048659.png', '_out_16rl_custom2/110_048660.png', '_out_16rl_custom2/110_048662.png', '_out_16rl_custom2/110_048663.png', '_out_16rl_custom2/110_048664.png', '_out_16rl_custom2/110_048665.png', '_out_16rl_custom2/110_048666.png', '_out_16rl_custom2/110_048667.png', '_out_16rl_custom2/110_048669.png', '_out_16rl_custom2/110_048670.png', '_out_16rl_custom2/110_048671.png', '_out_16rl_custom2/110_048672.png', '_out_16rl_custom2/110_048673.png', '_out_16rl_custom2/110_048674.png', '_out_16rl_custom2/110_048675.png', '_out_16rl_custom2/110_048677.png', '_out_16rl_custom2/110_048678.png', '_out_16rl_custom2/110_048679.png', '_out_16rl_custom2/110_048681.png', '_out_16rl_custom2/110_048682.png', '_out_16rl_custom2/110_048683.png', '_out_16rl_custom2/110_048684.png', '_out_16rl_custom2/110_048685.png', '_out_16rl_custom2/110_048686.png', '_out_16rl_custom2/110_048687.png', '_out_16rl_custom2/110_048688.png', '_out_16rl_custom2/110_048689.png', '_out_16rl_custom2/110_048691.png', '_out_16rl_custom2/110_048692.png', '_out_16rl_custom2/110_048693.png', '_out_16rl_custom2/110_048694.png', '_out_16rl_custom2/110_048695.png', '_out_16rl_custom2/110_048696.png', '_out_16rl_custom2/110_048697.png', '_out_16rl_custom2/110_048698.png', '_out_16rl_custom2/110_048699.png', '_out_16rl_custom2/110_048700.png', '_out_16rl_custom2/110_048701.png', '_out_16rl_custom2/110_048702.png', '_out_16rl_custom2/110_048703.png', '_out_16rl_custom2/110_048704.png', '_out_16rl_custom2/110_048705.png', '_out_16rl_custom2/110_048706.png', '_out_16rl_custom2/110_048707.png', '_out_16rl_custom2/110_048708.png', '_out_16rl_custom2/110_048710.png', '_out_16rl_custom2/110_048711.png', '_out_16rl_custom2/110_048712.png', '_out_16rl_custom2/110_048713.png', '_out_16rl_custom2/110_048714.png', '_out_16rl_custom2/110_048715.png', '_out_16rl_custom2/110_048716.png', '_out_16rl_custom2/110_048717.png', '_out_16rl_custom2/110_048718.png', '_out_16rl_custom2/110_048720.png', '_out_16rl_custom2/110_048721.png', '_out_16rl_custom2/110_048722.png', '_out_16rl_custom2/110_048723.png', '_out_16rl_custom2/110_048725.png', '_out_16rl_custom2/110_048726.png', '_out_16rl_custom2/110_048727.png', '_out_16rl_custom2/110_048728.png', '_out_16rl_custom2/110_048729.png', '_out_16rl_custom2/110_048731.png', '_out_16rl_custom2/110_048733.png', '_out_16rl_custom2/110_048734.png', '_out_16rl_custom2/110_048735.png', '_out_16rl_custom2/110_048737.png', '_out_16rl_custom2/110_048738.png', '_out_16rl_custom2/110_048739.png', '_out_16rl_custom2/110_048740.png', '_out_16rl_custom2/110_048742.png', '_out_16rl_custom2/110_048743.png', '_out_16rl_custom2/110_048744.png', '_out_16rl_custom2/110_048745.png', '_out_16rl_custom2/110_048746.png', '_out_16rl_custom2/110_048747.png', '_out_16rl_custom2/110_048749.png', '_out_16rl_custom2/110_048750.png', '_out_16rl_custom2/110_048751.png', '_out_16rl_custom2/110_048752.png', '_out_16rl_custom2/110_048753.png', '_out_16rl_custom2/110_048754.png', '_out_16rl_custom2/110_048755.png', '_out_16rl_custom2/110_048756.png', '_out_16rl_custom2/110_048758.png', '_out_16rl_custom2/110_048759.png', '_out_16rl_custom2/110_048760.png', '_out_16rl_custom2/110_048761.png', '_out_16rl_custom2/110_048763.png', '_out_16rl_custom2/110_048764.png', '_out_16rl_custom2/110_048765.png', '_out_16rl_custom2/110_048766.png', '_out_16rl_custom2/110_048767.png', '_out_16rl_custom2/110_048768.png', '_out_16rl_custom2/110_048769.png', '_out_16rl_custom2/110_048770.png', '_out_16rl_custom2/110_048771.png', '_out_16rl_custom2/110_048772.png', '_out_16rl_custom2/110_048774.png', '_out_16rl_custom2/110_048776.png', '_out_16rl_custom2/110_048777.png', '_out_16rl_custom2/110_048778.png', '_out_16rl_custom2/110_048779.png', '_out_16rl_custom2/110_048781.png', '_out_16rl_custom2/110_048782.png', '_out_16rl_custom2/110_048783.png', '_out_16rl_custom2/110_048785.png', '_out_16rl_custom2/110_048786.png', '_out_16rl_custom2/110_048787.png', '_out_16rl_custom2/110_048788.png', '_out_16rl_custom2/110_048789.png', '_out_16rl_custom2/110_048791.png', '_out_16rl_custom2/110_048792.png', '_out_16rl_custom2/110_048793.png', '_out_16rl_custom2/110_048795.png', '_out_16rl_custom2/110_048796.png', '_out_16rl_custom2/110_048797.png', '_out_16rl_custom2/110_048798.png', '_out_16rl_custom2/110_048799.png', '_out_16rl_custom2/110_048800.png', '_out_16rl_custom2/110_048801.png', '_out_16rl_custom2/110_048802.png', '_out_16rl_custom2/110_048803.png', '_out_16rl_custom2/110_048804.png', '_out_16rl_custom2/110_048805.png', '_out_16rl_custom2/110_048807.png', '_out_16rl_custom2/110_048808.png', '_out_16rl_custom2/110_048809.png', '_out_16rl_custom2/110_048810.png', '_out_16rl_custom2/110_048811.png', '_out_16rl_custom2/110_048812.png', '_out_16rl_custom2/110_048813.png', '_out_16rl_custom2/110_048814.png', '_out_16rl_custom2/110_048815.png', '_out_16rl_custom2/110_048816.png', '_out_16rl_custom2/110_048817.png', '_out_16rl_custom2/110_048818.png', '_out_16rl_custom2/110_048819.png', '_out_16rl_custom2/110_048821.png', '_out_16rl_custom2/110_048822.png', '_out_16rl_custom2/110_048823.png', '_out_16rl_custom2/110_048824.png', '_out_16rl_custom2/110_048826.png', '_out_16rl_custom2/110_048827.png', '_out_16rl_custom2/110_048828.png', '_out_16rl_custom2/110_048829.png', '_out_16rl_custom2/110_048830.png', '_out_16rl_custom2/110_048831.png', '_out_16rl_custom2/110_048832.png', '_out_16rl_custom2/110_048833.png', '_out_16rl_custom2/110_048834.png', '_out_16rl_custom2/110_048835.png', '_out_16rl_custom2/110_048836.png', '_out_16rl_custom2/110_048837.png', '_out_16rl_custom2/110_048838.png', '_out_16rl_custom2/110_048839.png', '_out_16rl_custom2/110_048840.png', '_out_16rl_custom2/110_048841.png', '_out_16rl_custom2/110_048842.png', '_out_16rl_custom2/110_048843.png', '_out_16rl_custom2/110_048844.png', '_out_16rl_custom2/110_048845.png', '_out_16rl_custom2/110_048846.png', '_out_16rl_custom2/110_048847.png', '_out_16rl_custom2/110_048848.png', '_out_16rl_custom2/110_048849.png', '_out_16rl_custom2/110_048850.png', '_out_16rl_custom2/110_048851.png', '_out_16rl_custom2/110_048852.png', '_out_16rl_custom2/110_048853.png', '_out_16rl_custom2/110_048855.png', '_out_16rl_custom2/110_048856.png', '_out_16rl_custom2/110_048858.png', '_out_16rl_custom2/110_048859.png', '_out_16rl_custom2/110_048860.png', '_out_16rl_custom2/110_048861.png', '_out_16rl_custom2/110_048862.png', '_out_16rl_custom2/110_048863.png', '_out_16rl_custom2/110_048864.png', '_out_16rl_custom2/110_048865.png', '_out_16rl_custom2/110_048867.png', '_out_16rl_custom2/110_048868.png', '_out_16rl_custom2/110_048869.png', '_out_16rl_custom2/110_048870.png', '_out_16rl_custom2/110_048872.png', '_out_16rl_custom2/110_048874.png', '_out_16rl_custom2/110_048875.png', '_out_16rl_custom2/110_048876.png', '_out_16rl_custom2/110_048877.png', '_out_16rl_custom2/110_048878.png', '_out_16rl_custom2/110_048880.png', '_out_16rl_custom2/110_048881.png', '_out_16rl_custom2/110_048882.png', '_out_16rl_custom2/110_048883.png', '_out_16rl_custom2/110_048884.png', '_out_16rl_custom2/110_048885.png', '_out_16rl_custom2/110_048886.png', '_out_16rl_custom2/110_048887.png', '_out_16rl_custom2/110_048888.png', '_out_16rl_custom2/110_048889.png', '_out_16rl_custom2/110_048890.png', '_out_16rl_custom2/110_048891.png', '_out_16rl_custom2/110_048892.png', '_out_16rl_custom2/110_048893.png', '_out_16rl_custom2/110_048894.png', '_out_16rl_custom2/110_048895.png', '_out_16rl_custom2/110_048896.png', '_out_16rl_custom2/110_048898.png', '_out_16rl_custom2/110_048899.png', '_out_16rl_custom2/110_048900.png', '_out_16rl_custom2/110_048902.png', '_out_16rl_custom2/110_048903.png', '_out_16rl_custom2/110_048904.png', '_out_16rl_custom2/110_048905.png', '_out_16rl_custom2/110_048906.png', '_out_16rl_custom2/110_048907.png', '_out_16rl_custom2/110_048908.png', '_out_16rl_custom2/110_048910.png', '_out_16rl_custom2/110_048911.png', '_out_16rl_custom2/110_048912.png', '_out_16rl_custom2/110_048913.png', '_out_16rl_custom2/110_048914.png', '_out_16rl_custom2/110_048915.png', '_out_16rl_custom2/110_048916.png', '_out_16rl_custom2/110_048917.png', '_out_16rl_custom2/110_048919.png', '_out_16rl_custom2/110_048920.png', '_out_16rl_custom2/110_048921.png', '_out_16rl_custom2/110_048923.png', '_out_16rl_custom2/110_048924.png', '_out_16rl_custom2/110_048926.png', '_out_16rl_custom2/110_048927.png', '_out_16rl_custom2/110_048928.png', '_out_16rl_custom2/110_048929.png', '_out_16rl_custom2/110_048930.png', '_out_16rl_custom2/110_048931.png', '_out_16rl_custom2/110_048932.png', '_out_16rl_custom2/110_048933.png', '_out_16rl_custom2/110_048934.png', '_out_16rl_custom2/110_048935.png', '_out_16rl_custom2/110_048936.png', '_out_16rl_custom2/110_048937.png', '_out_16rl_custom2/110_048939.png', '_out_16rl_custom2/110_048940.png', '_out_16rl_custom2/110_048941.png', '_out_16rl_custom2/110_048942.png', '_out_16rl_custom2/110_048944.png', '_out_16rl_custom2/110_048945.png', '_out_16rl_custom2/110_048946.png', '_out_16rl_custom2/110_048947.png', '_out_16rl_custom2/110_048949.png', '_out_16rl_custom2/110_048950.png', '_out_16rl_custom2/110_048951.png', '_out_16rl_custom2/110_048952.png', '_out_16rl_custom2/110_048953.png', '_out_16rl_custom2/110_048955.png', '_out_16rl_custom2/110_048956.png', '_out_16rl_custom2/110_048957.png', '_out_16rl_custom2/110_048959.png', '_out_16rl_custom2/110_048960.png', '_out_16rl_custom2/110_048961.png', '_out_16rl_custom2/110_048963.png', '_out_16rl_custom2/110_048964.png', '_out_16rl_custom2/110_048965.png', '_out_16rl_custom2/110_048966.png', '_out_16rl_custom2/110_048967.png', '_out_16rl_custom2/110_048968.png', '_out_16rl_custom2/110_048969.png', '_out_16rl_custom2/110_048970.png', '_out_16rl_custom2/110_048971.png', '_out_16rl_custom2/110_048972.png', '_out_16rl_custom2/110_048973.png', '_out_16rl_custom2/110_048974.png', '_out_16rl_custom2/110_048975.png', '_out_16rl_custom2/110_048976.png', '_out_16rl_custom2/110_048977.png', '_out_16rl_custom2/110_048978.png', '_out_16rl_custom2/110_048979.png', '_out_16rl_custom2/110_048980.png', '_out_16rl_custom2/110_048982.png', '_out_16rl_custom2/110_048983.png', '_out_16rl_custom2/110_048984.png', '_out_16rl_custom2/110_048985.png', '_out_16rl_custom2/110_048986.png', '_out_16rl_custom2/110_048987.png', '_out_16rl_custom2/110_048988.png', '_out_16rl_custom2/110_048990.png', '_out_16rl_custom2/110_048991.png', '_out_16rl_custom2/110_048992.png', '_out_16rl_custom2/110_048993.png', '_out_16rl_custom2/110_048994.png', '_out_16rl_custom2/110_048995.png', '_out_16rl_custom2/110_048996.png', '_out_16rl_custom2/110_048997.png', '_out_16rl_custom2/110_048998.png', '_out_16rl_custom2/110_048999.png', '_out_16rl_custom2/110_049000.png', '_out_16rl_custom2/110_049001.png', '_out_16rl_custom2/110_049002.png', '_out_16rl_custom2/110_049004.png', '_out_16rl_custom2/110_049005.png', '_out_16rl_custom2/110_049006.png', '_out_16rl_custom2/110_049007.png', '_out_16rl_custom2/110_049008.png', '_out_16rl_custom2/110_049009.png', '_out_16rl_custom2/110_049010.png', '_out_16rl_custom2/110_049012.png', '_out_16rl_custom2/110_049014.png', '_out_16rl_custom2/110_049015.png', '_out_16rl_custom2/110_049016.png', '_out_16rl_custom2/110_049017.png', '_out_16rl_custom2/110_049018.png', '_out_16rl_custom2/110_049019.png', '_out_16rl_custom2/110_049021.png', '_out_16rl_custom2/110_049022.png', '_out_16rl_custom2/110_049023.png', '_out_16rl_custom2/110_049024.png', '_out_16rl_custom2/110_049025.png', '_out_16rl_custom2/110_049026.png', '_out_16rl_custom2/110_049027.png', '_out_16rl_custom2/110_049028.png', '_out_16rl_custom2/110_049029.png', '_out_16rl_custom2/110_049030.png', '_out_16rl_custom2/110_049031.png', '_out_16rl_custom2/110_049032.png', '_out_16rl_custom2/110_049033.png', '_out_16rl_custom2/110_049034.png', '_out_16rl_custom2/110_049035.png', '_out_16rl_custom2/110_049036.png', '_out_16rl_custom2/110_049037.png', '_out_16rl_custom2/110_049038.png', '_out_16rl_custom2/110_049039.png', '_out_16rl_custom2/110_049040.png', '_out_16rl_custom2/110_049041.png', '_out_16rl_custom2/110_049042.png', '_out_16rl_custom2/110_049043.png', '_out_16rl_custom2/110_049044.png', '_out_16rl_custom2/110_049045.png', '_out_16rl_custom2/110_049046.png', '_out_16rl_custom2/110_049047.png', '_out_16rl_custom2/110_049048.png', '_out_16rl_custom2/110_049050.png', '_out_16rl_custom2/110_049051.png', '_out_16rl_custom2/110_049052.png', '_out_16rl_custom2/110_049053.png', '_out_16rl_custom2/110_049054.png', '_out_16rl_custom2/110_049055.png', '_out_16rl_custom2/110_049056.png', '_out_16rl_custom2/110_049057.png', '_out_16rl_custom2/110_049058.png', '_out_16rl_custom2/110_049059.png', '_out_16rl_custom2/110_049060.png', '_out_16rl_custom2/110_049061.png', '_out_16rl_custom2/110_049062.png', '_out_16rl_custom2/110_049063.png', '_out_16rl_custom2/110_049064.png', '_out_16rl_custom2/110_049065.png', '_out_16rl_custom2/110_049066.png', '_out_16rl_custom2/110_049067.png', '_out_16rl_custom2/110_049069.png', '_out_16rl_custom2/110_049070.png', '_out_16rl_custom2/110_049071.png', '_out_16rl_custom2/110_049072.png', '_out_16rl_custom2/110_049074.png', '_out_16rl_custom2/110_049075.png', '_out_16rl_custom2/110_049076.png', '_out_16rl_custom2/110_049077.png', '_out_16rl_custom2/110_049078.png', '_out_16rl_custom2/110_049079.png', '_out_16rl_custom2/110_049080.png', '_out_16rl_custom2/110_049081.png', '_out_16rl_custom2/110_049082.png', '_out_16rl_custom2/110_049084.png', '_out_16rl_custom2/110_049085.png', '_out_16rl_custom2/110_049086.png', '_out_16rl_custom2/110_049087.png', '_out_16rl_custom2/110_049088.png', '_out_16rl_custom2/110_049089.png', '_out_16rl_custom2/110_049090.png', '_out_16rl_custom2/110_049091.png', '_out_16rl_custom2/110_049092.png', '_out_16rl_custom2/110_049093.png', '_out_16rl_custom2/110_049095.png', '_out_16rl_custom2/110_049096.png', '_out_16rl_custom2/110_049097.png', '_out_16rl_custom2/110_049098.png', '_out_16rl_custom2/110_049099.png', '_out_16rl_custom2/110_049101.png', '_out_16rl_custom2/110_049102.png', '_out_16rl_custom2/110_049103.png', '_out_16rl_custom2/110_049104.png', '_out_16rl_custom2/110_049105.png', '_out_16rl_custom2/110_049106.png', '_out_16rl_custom2/110_049107.png', '_out_16rl_custom2/110_049108.png', '_out_16rl_custom2/110_049109.png', '_out_16rl_custom2/110_049110.png', '_out_16rl_custom2/110_049111.png', '_out_16rl_custom2/110_049112.png', '_out_16rl_custom2/110_049113.png', '_out_16rl_custom2/110_049114.png', '_out_16rl_custom2/110_049115.png', '_out_16rl_custom2/110_049116.png', '_out_16rl_custom2/110_049117.png', '_out_16rl_custom2/110_049118.png', '_out_16rl_custom2/110_049119.png', '_out_16rl_custom2/110_049120.png', '_out_16rl_custom2/110_049121.png', '_out_16rl_custom2/110_049122.png', '_out_16rl_custom2/110_049123.png', '_out_16rl_custom2/110_049124.png', '_out_16rl_custom2/110_049125.png', '_out_16rl_custom2/110_049126.png', '_out_16rl_custom2/110_049127.png', '_out_16rl_custom2/110_049128.png', '_out_16rl_custom2/110_049129.png', '_out_16rl_custom2/110_049130.png', '_out_16rl_custom2/110_049131.png', '_out_16rl_custom2/110_049132.png', '_out_16rl_custom2/110_049133.png', '_out_16rl_custom2/110_049134.png', '_out_16rl_custom2/110_049135.png', '_out_16rl_custom2/110_049136.png', '_out_16rl_custom2/110_049137.png', '_out_16rl_custom2/110_049138.png', '_out_16rl_custom2/110_049139.png', '_out_16rl_custom2/110_049140.png', '_out_16rl_custom2/110_049142.png', '_out_16rl_custom2/110_049144.png', '_out_16rl_custom2/110_049145.png', '_out_16rl_custom2/110_049146.png', '_out_16rl_custom2/110_049147.png', '_out_16rl_custom2/110_049148.png', '_out_16rl_custom2/110_049149.png', '_out_16rl_custom2/110_049150.png', '_out_16rl_custom2/110_049151.png', '_out_16rl_custom2/110_049152.png', '_out_16rl_custom2/110_049153.png', '_out_16rl_custom2/110_049154.png', '_out_16rl_custom2/110_049155.png', '_out_16rl_custom2/110_049156.png', '_out_16rl_custom2/110_049157.png', '_out_16rl_custom2/110_049158.png', '_out_16rl_custom2/110_049159.png', '_out_16rl_custom2/110_049161.png', '_out_16rl_custom2/110_049162.png', '_out_16rl_custom2/110_049163.png', '_out_16rl_custom2/110_049164.png', '_out_16rl_custom2/110_049165.png', '_out_16rl_custom2/110_049166.png', '_out_16rl_custom2/110_049167.png', '_out_16rl_custom2/110_049168.png', '_out_16rl_custom2/110_049169.png', '_out_16rl_custom2/110_049170.png', '_out_16rl_custom2/110_049171.png', '_out_16rl_custom2/110_049172.png', '_out_16rl_custom2/110_049173.png', '_out_16rl_custom2/110_049174.png', '_out_16rl_custom2/110_049175.png', '_out_16rl_custom2/110_049176.png', '_out_16rl_custom2/110_049178.png', '_out_16rl_custom2/110_049179.png', '_out_16rl_custom2/110_049181.png', '_out_16rl_custom2/110_049182.png', '_out_16rl_custom2/110_049183.png', '_out_16rl_custom2/110_049184.png', '_out_16rl_custom2/110_049185.png', '_out_16rl_custom2/110_049186.png', '_out_16rl_custom2/110_049187.png', '_out_16rl_custom2/110_049189.png', '_out_16rl_custom2/110_049190.png', '_out_16rl_custom2/110_049192.png', '_out_16rl_custom2/110_049194.png', '_out_16rl_custom2/110_049195.png', '_out_16rl_custom2/110_049196.png', '_out_16rl_custom2/110_049197.png', '_out_16rl_custom2/110_049198.png', '_out_16rl_custom2/110_049199.png', '_out_16rl_custom2/110_049200.png', '_out_16rl_custom2/110_049201.png', '_out_16rl_custom2/110_049202.png', '_out_16rl_custom2/110_049203.png', '_out_16rl_custom2/110_049204.png', '_out_16rl_custom2/110_049205.png', '_out_16rl_custom2/110_049206.png', '_out_16rl_custom2/110_049207.png', '_out_16rl_custom2/110_049208.png', '_out_16rl_custom2/110_049210.png', '_out_16rl_custom2/110_049211.png', '_out_16rl_custom2/110_049212.png', '_out_16rl_custom2/110_049213.png', '_out_16rl_custom2/110_049214.png', '_out_16rl_custom2/110_049215.png', '_out_16rl_custom2/110_049216.png', '_out_16rl_custom2/110_049217.png', '_out_16rl_custom2/110_049218.png', '_out_16rl_custom2/110_049219.png', '_out_16rl_custom2/110_049220.png', '_out_16rl_custom2/110_049221.png', '_out_16rl_custom2/110_049222.png', '_out_16rl_custom2/110_049223.png', '_out_16rl_custom2/110_049224.png', '_out_16rl_custom2/110_049225.png', '_out_16rl_custom2/110_049226.png', '_out_16rl_custom2/110_049227.png', '_out_16rl_custom2/110_049229.png', '_out_16rl_custom2/110_049230.png', '_out_16rl_custom2/110_049231.png', '_out_16rl_custom2/110_049232.png', '_out_16rl_custom2/110_049233.png', '_out_16rl_custom2/110_049234.png', '_out_16rl_custom2/110_049235.png', '_out_16rl_custom2/110_049236.png', '_out_16rl_custom2/110_049237.png', '_out_16rl_custom2/110_049238.png', '_out_16rl_custom2/110_049239.png', '_out_16rl_custom2/110_049240.png', '_out_16rl_custom2/110_049241.png', '_out_16rl_custom2/110_049242.png', '_out_16rl_custom2/110_049244.png', '_out_16rl_custom2/110_049245.png', '_out_16rl_custom2/110_049246.png', '_out_16rl_custom2/110_049247.png', '_out_16rl_custom2/110_049248.png', '_out_16rl_custom2/110_049250.png', '_out_16rl_custom2/110_049252.png', '_out_16rl_custom2/110_049254.png', '_out_16rl_custom2/110_049256.png', '_out_16rl_custom2/110_049257.png', '_out_16rl_custom2/110_049258.png', '_out_16rl_custom2/110_049259.png', '_out_16rl_custom2/110_049260.png', '_out_16rl_custom2/110_049261.png', '_out_16rl_custom2/110_049262.png', '_out_16rl_custom2/110_049263.png', '_out_16rl_custom2/110_049264.png', '_out_16rl_custom2/110_049265.png', '_out_16rl_custom2/110_049266.png', '_out_16rl_custom2/110_049267.png', '_out_16rl_custom2/110_049269.png', '_out_16rl_custom2/110_049270.png', '_out_16rl_custom2/110_049271.png', '_out_16rl_custom2/110_049272.png', '_out_16rl_custom2/110_049273.png', '_out_16rl_custom2/110_049274.png', '_out_16rl_custom2/110_049275.png', '_out_16rl_custom2/110_049277.png', '_out_16rl_custom2/110_049279.png', '_out_16rl_custom2/110_049280.png', '_out_16rl_custom2/110_049281.png', '_out_16rl_custom2/110_049282.png', '_out_16rl_custom2/110_049283.png', '_out_16rl_custom2/110_049284.png', '_out_16rl_custom2/110_049285.png', '_out_16rl_custom2/110_049286.png', '_out_16rl_custom2/110_049287.png', '_out_16rl_custom2/110_049288.png', '_out_16rl_custom2/110_049289.png', '_out_16rl_custom2/110_049290.png', '_out_16rl_custom2/110_049292.png', '_out_16rl_custom2/110_049293.png', '_out_16rl_custom2/110_049295.png', '_out_16rl_custom2/110_049296.png', '_out_16rl_custom2/110_049297.png', '_out_16rl_custom2/110_049298.png', '_out_16rl_custom2/110_049299.png', '_out_16rl_custom2/110_049300.png', '_out_16rl_custom2/110_049301.png', '_out_16rl_custom2/110_049302.png', '_out_16rl_custom2/110_049303.png', '_out_16rl_custom2/110_049304.png', '_out_16rl_custom2/110_049305.png', '_out_16rl_custom2/110_049306.png', '_out_16rl_custom2/110_049307.png', '_out_16rl_custom2/110_049308.png', '_out_16rl_custom2/110_049309.png', '_out_16rl_custom2/110_049310.png', '_out_16rl_custom2/110_049311.png', '_out_16rl_custom2/110_049312.png', '_out_16rl_custom2/110_049313.png', '_out_16rl_custom2/110_049315.png', '_out_16rl_custom2/110_049316.png', '_out_16rl_custom2/110_049318.png', '_out_16rl_custom2/110_049319.png', '_out_16rl_custom2/110_049320.png', '_out_16rl_custom2/110_049321.png', '_out_16rl_custom2/110_049322.png', '_out_16rl_custom2/110_049323.png', '_out_16rl_custom2/110_049325.png', '_out_16rl_custom2/110_049326.png', '_out_16rl_custom2/110_049327.png', '_out_16rl_custom2/110_049328.png', '_out_16rl_custom2/110_049329.png', '_out_16rl_custom2/110_049330.png', '_out_16rl_custom2/110_049332.png', '_out_16rl_custom2/110_049334.png', '_out_16rl_custom2/110_049335.png', '_out_16rl_custom2/110_049336.png', '_out_16rl_custom2/110_049337.png', '_out_16rl_custom2/110_049338.png', '_out_16rl_custom2/110_049339.png', '_out_16rl_custom2/110_049340.png', '_out_16rl_custom2/110_049341.png', '_out_16rl_custom2/110_049343.png', '_out_16rl_custom2/110_049344.png', '_out_16rl_custom2/110_049345.png', '_out_16rl_custom2/110_049347.png', '_out_16rl_custom2/110_049348.png', '_out_16rl_custom2/110_049349.png', '_out_16rl_custom2/110_049350.png', '_out_16rl_custom2/110_049351.png', '_out_16rl_custom2/110_049352.png', '_out_16rl_custom2/110_049353.png', '_out_16rl_custom2/110_049354.png', '_out_16rl_custom2/110_049355.png', '_out_16rl_custom2/110_049356.png', '_out_16rl_custom2/110_049357.png', '_out_16rl_custom2/110_049358.png', '_out_16rl_custom2/110_049359.png', '_out_16rl_custom2/110_049360.png', '_out_16rl_custom2/110_049361.png', '_out_16rl_custom2/110_049362.png', '_out_16rl_custom2/110_049363.png', '_out_16rl_custom2/110_049364.png', '_out_16rl_custom2/110_049366.png', '_out_16rl_custom2/110_049367.png', '_out_16rl_custom2/110_049368.png', '_out_16rl_custom2/110_049369.png', '_out_16rl_custom2/110_049370.png', '_out_16rl_custom2/110_049371.png', '_out_16rl_custom2/110_049373.png', '_out_16rl_custom2/110_049374.png', '_out_16rl_custom2/110_049375.png', '_out_16rl_custom2/110_049377.png', '_out_16rl_custom2/110_049378.png', '_out_16rl_custom2/110_049380.png', '_out_16rl_custom2/110_049381.png', '_out_16rl_custom2/110_049383.png', '_out_16rl_custom2/110_049384.png', '_out_16rl_custom2/110_049385.png', '_out_16rl_custom2/110_049386.png', '_out_16rl_custom2/110_049387.png', '_out_16rl_custom2/110_049388.png', '_out_16rl_custom2/110_049389.png', '_out_16rl_custom2/110_049390.png', '_out_16rl_custom2/110_049391.png', '_out_16rl_custom2/110_049392.png', '_out_16rl_custom2/110_049393.png', '_out_16rl_custom2/110_049394.png', '_out_16rl_custom2/110_049395.png', '_out_16rl_custom2/110_049396.png', '_out_16rl_custom2/110_049397.png', '_out_16rl_custom2/110_049398.png', '_out_16rl_custom2/110_049399.png', '_out_16rl_custom2/110_049401.png', '_out_16rl_custom2/110_049402.png', '_out_16rl_custom2/110_049403.png', '_out_16rl_custom2/110_049404.png', '_out_16rl_custom2/110_049405.png', '_out_16rl_custom2/110_049406.png', '_out_16rl_custom2/110_049408.png', '_out_16rl_custom2/110_049409.png', '_out_16rl_custom2/110_049410.png', '_out_16rl_custom2/110_049411.png', '_out_16rl_custom2/110_049412.png', '_out_16rl_custom2/110_049413.png', '_out_16rl_custom2/110_049414.png', '_out_16rl_custom2/110_049415.png', '_out_16rl_custom2/110_049416.png', '_out_16rl_custom2/110_049417.png', '_out_16rl_custom2/110_049418.png', '_out_16rl_custom2/110_049419.png', '_out_16rl_custom2/110_049420.png', '_out_16rl_custom2/110_049421.png', '_out_16rl_custom2/110_049422.png', '_out_16rl_custom2/110_049424.png', '_out_16rl_custom2/110_049425.png', '_out_16rl_custom2/110_049426.png', '_out_16rl_custom2/110_049428.png', '_out_16rl_custom2/110_049430.png', '_out_16rl_custom2/110_049431.png', '_out_16rl_custom2/110_049432.png', '_out_16rl_custom2/110_049433.png', '_out_16rl_custom2/110_049434.png', '_out_16rl_custom2/110_049435.png', '_out_16rl_custom2/110_049436.png', '_out_16rl_custom2/110_049437.png', '_out_16rl_custom2/110_049438.png', '_out_16rl_custom2/110_049439.png', '_out_16rl_custom2/110_049440.png', '_out_16rl_custom2/110_049441.png', '_out_16rl_custom2/110_049442.png', '_out_16rl_custom2/110_049443.png', '_out_16rl_custom2/110_049445.png', '_out_16rl_custom2/110_049446.png', '_out_16rl_custom2/110_049447.png', '_out_16rl_custom2/110_049448.png', '_out_16rl_custom2/110_049449.png', '_out_16rl_custom2/110_049450.png', '_out_16rl_custom2/110_049451.png', '_out_16rl_custom2/110_049452.png', '_out_16rl_custom2/110_049453.png', '_out_16rl_custom2/110_049454.png', '_out_16rl_custom2/110_049455.png', '_out_16rl_custom2/110_049456.png', '_out_16rl_custom2/110_049457.png', '_out_16rl_custom2/110_049458.png', '_out_16rl_custom2/110_049459.png', '_out_16rl_custom2/110_049461.png', '_out_16rl_custom2/110_049462.png', '_out_16rl_custom2/110_049463.png', '_out_16rl_custom2/110_049464.png', '_out_16rl_custom2/110_049465.png', '_out_16rl_custom2/110_049467.png', '_out_16rl_custom2/110_049468.png', '_out_16rl_custom2/110_049469.png', '_out_16rl_custom2/110_049470.png', '_out_16rl_custom2/110_049471.png', '_out_16rl_custom2/110_049472.png', '_out_16rl_custom2/110_049473.png', '_out_16rl_custom2/110_049474.png', '_out_16rl_custom2/110_049475.png', '_out_16rl_custom2/110_049476.png', '_out_16rl_custom2/110_049477.png', '_out_16rl_custom2/110_049479.png', '_out_16rl_custom2/110_049480.png', '_out_16rl_custom2/110_049481.png', '_out_16rl_custom2/110_049482.png', '_out_16rl_custom2/110_049483.png', '_out_16rl_custom2/110_049484.png', '_out_16rl_custom2/110_049485.png', '_out_16rl_custom2/110_049486.png', '_out_16rl_custom2/110_049487.png', '_out_16rl_custom2/110_049488.png', '_out_16rl_custom2/110_049489.png', '_out_16rl_custom2/110_049490.png', '_out_16rl_custom2/110_049491.png', '_out_16rl_custom2/110_049492.png', '_out_16rl_custom2/110_049494.png', '_out_16rl_custom2/110_049495.png', '_out_16rl_custom2/110_049496.png', '_out_16rl_custom2/110_049497.png', '_out_16rl_custom2/110_049498.png', '_out_16rl_custom2/110_049499.png', '_out_16rl_custom2/110_049500.png', '_out_16rl_custom2/110_049501.png', '_out_16rl_custom2/110_049502.png', '_out_16rl_custom2/110_049503.png', '_out_16rl_custom2/110_049504.png', '_out_16rl_custom2/110_049505.png', '_out_16rl_custom2/110_049506.png', '_out_16rl_custom2/110_049508.png', '_out_16rl_custom2/110_049509.png', '_out_16rl_custom2/110_049510.png', '_out_16rl_custom2/110_049511.png', '_out_16rl_custom2/110_049512.png', '_out_16rl_custom2/110_049513.png', '_out_16rl_custom2/110_049514.png', '_out_16rl_custom2/110_049515.png', '_out_16rl_custom2/110_049516.png', '_out_16rl_custom2/110_049517.png', '_out_16rl_custom2/110_049519.png', '_out_16rl_custom2/110_049520.png', '_out_16rl_custom2/110_049521.png', '_out_16rl_custom2/110_049522.png', '_out_16rl_custom2/110_049523.png', '_out_16rl_custom2/110_049524.png', '_out_16rl_custom2/110_049525.png', '_out_16rl_custom2/110_049526.png', '_out_16rl_custom2/110_049527.png', '_out_16rl_custom2/110_049528.png', '_out_16rl_custom2/110_049529.png', '_out_16rl_custom2/110_049530.png', '_out_16rl_custom2/110_049531.png', '_out_16rl_custom2/110_049532.png', '_out_16rl_custom2/110_049533.png', '_out_16rl_custom2/110_049534.png', '_out_16rl_custom2/110_049535.png', '_out_16rl_custom2/110_049536.png', '_out_16rl_custom2/110_049537.png', '_out_16rl_custom2/110_049538.png', '_out_16rl_custom2/110_049539.png', '_out_16rl_custom2/110_049540.png', '_out_16rl_custom2/110_049541.png', '_out_16rl_custom2/110_049542.png', '_out_16rl_custom2/110_049543.png', '_out_16rl_custom2/110_049544.png', '_out_16rl_custom2/110_049545.png', '_out_16rl_custom2/110_049546.png', '_out_16rl_custom2/110_049547.png', '_out_16rl_custom2/110_049548.png', '_out_16rl_custom2/110_049550.png', '_out_16rl_custom2/110_049552.png', '_out_16rl_custom2/110_049553.png', '_out_16rl_custom2/110_049554.png', '_out_16rl_custom2/110_049555.png', '_out_16rl_custom2/110_049556.png', '_out_16rl_custom2/110_049557.png', '_out_16rl_custom2/110_049558.png', '_out_16rl_custom2/110_049559.png', '_out_16rl_custom2/110_049560.png', '_out_16rl_custom2/110_049561.png', '_out_16rl_custom2/110_049562.png', '_out_16rl_custom2/110_049563.png', '_out_16rl_custom2/110_049564.png', '_out_16rl_custom2/110_049565.png', '_out_16rl_custom2/110_049566.png', '_out_16rl_custom2/110_049567.png', '_out_16rl_custom2/110_049568.png', '_out_16rl_custom2/110_049569.png', '_out_16rl_custom2/110_049571.png', '_out_16rl_custom2/110_049573.png', '_out_16rl_custom2/110_049574.png', '_out_16rl_custom2/110_049575.png', '_out_16rl_custom2/110_049576.png', '_out_16rl_custom2/110_049577.png', '_out_16rl_custom2/110_049578.png', '_out_16rl_custom2/110_049579.png', '_out_16rl_custom2/110_049580.png', '_out_16rl_custom2/110_049581.png', '_out_16rl_custom2/110_049582.png', '_out_16rl_custom2/110_049583.png', '_out_16rl_custom2/110_049584.png', '_out_16rl_custom2/110_049586.png', '_out_16rl_custom2/110_049587.png', '_out_16rl_custom2/110_049588.png', '_out_16rl_custom2/110_049589.png', '_out_16rl_custom2/110_049590.png', '_out_16rl_custom2/110_049591.png', '_out_16rl_custom2/110_049592.png', '_out_16rl_custom2/110_049594.png', '_out_16rl_custom2/110_049595.png', '_out_16rl_custom2/110_049596.png', '_out_16rl_custom2/110_049597.png', '_out_16rl_custom2/110_049598.png', '_out_16rl_custom2/110_049599.png', '_out_16rl_custom2/110_049600.png', '_out_16rl_custom2/110_049601.png', '_out_16rl_custom2/110_049602.png', '_out_16rl_custom2/110_049603.png', '_out_16rl_custom2/110_049604.png', '_out_16rl_custom2/110_049605.png', '_out_16rl_custom2/110_049606.png', '_out_16rl_custom2/110_049607.png', '_out_16rl_custom2/110_049608.png', '_out_16rl_custom2/110_049609.png', '_out_16rl_custom2/110_049610.png', '_out_16rl_custom2/110_049611.png', '_out_16rl_custom2/110_049612.png', '_out_16rl_custom2/110_049613.png', '_out_16rl_custom2/110_049614.png', '_out_16rl_custom2/110_049615.png', '_out_16rl_custom2/110_049616.png', '_out_16rl_custom2/110_049617.png', '_out_16rl_custom2/110_049618.png', '_out_16rl_custom2/110_049619.png', '_out_16rl_custom2/110_049620.png', '_out_16rl_custom2/110_049621.png', '_out_16rl_custom2/110_049622.png', '_out_16rl_custom2/110_049623.png', '_out_16rl_custom2/110_049625.png', '_out_16rl_custom2/110_049626.png', '_out_16rl_custom2/110_049627.png', '_out_16rl_custom2/110_049628.png', '_out_16rl_custom2/110_049629.png', '_out_16rl_custom2/110_049630.png', '_out_16rl_custom2/110_049631.png', '_out_16rl_custom2/110_049632.png', '_out_16rl_custom2/110_049634.png', '_out_16rl_custom2/110_049636.png', '_out_16rl_custom2/110_049638.png', '_out_16rl_custom2/110_049639.png', '_out_16rl_custom2/110_049640.png', '_out_16rl_custom2/110_049641.png', '_out_16rl_custom2/110_049642.png', '_out_16rl_custom2/110_049643.png', '_out_16rl_custom2/110_049645.png', '_out_16rl_custom2/110_049646.png', '_out_16rl_custom2/110_049647.png', '_out_16rl_custom2/110_049648.png', '_out_16rl_custom2/110_049649.png', '_out_16rl_custom2/110_049650.png', '_out_16rl_custom2/110_049651.png', '_out_16rl_custom2/110_049652.png', '_out_16rl_custom2/110_049653.png', '_out_16rl_custom2/110_049654.png', '_out_16rl_custom2/110_049655.png', '_out_16rl_custom2/110_049656.png', '_out_16rl_custom2/110_049657.png', '_out_16rl_custom2/110_049658.png', '_out_16rl_custom2/110_049659.png', '_out_16rl_custom2/110_049661.png', '_out_16rl_custom2/110_049662.png', '_out_16rl_custom2/110_049663.png', '_out_16rl_custom2/110_049664.png', '_out_16rl_custom2/110_049665.png', '_out_16rl_custom2/110_049666.png', '_out_16rl_custom2/110_049667.png', '_out_16rl_custom2/110_049669.png', '_out_16rl_custom2/110_049670.png', '_out_16rl_custom2/110_049671.png', '_out_16rl_custom2/110_049672.png', '_out_16rl_custom2/110_049673.png', '_out_16rl_custom2/110_049674.png', '_out_16rl_custom2/110_049675.png', '_out_16rl_custom2/110_049676.png', '_out_16rl_custom2/110_049677.png', '_out_16rl_custom2/110_049678.png', '_out_16rl_custom2/110_049680.png', '_out_16rl_custom2/110_049681.png', '_out_16rl_custom2/110_049682.png', '_out_16rl_custom2/110_049683.png', '_out_16rl_custom2/110_049684.png', '_out_16rl_custom2/110_049686.png', '_out_16rl_custom2/110_049687.png', '_out_16rl_custom2/110_049688.png', '_out_16rl_custom2/110_049689.png', '_out_16rl_custom2/110_049690.png', '_out_16rl_custom2/110_049691.png', '_out_16rl_custom2/110_049692.png', '_out_16rl_custom2/110_049693.png', '_out_16rl_custom2/110_049694.png', '_out_16rl_custom2/110_049695.png', '_out_16rl_custom2/110_049696.png', '_out_16rl_custom2/110_049698.png', '_out_16rl_custom2/110_049700.png', '_out_16rl_custom2/110_049701.png', '_out_16rl_custom2/110_049702.png', '_out_16rl_custom2/110_049703.png', '_out_16rl_custom2/110_049704.png', '_out_16rl_custom2/110_049705.png', '_out_16rl_custom2/110_049706.png', '_out_16rl_custom2/110_049707.png', '_out_16rl_custom2/110_049708.png', '_out_16rl_custom2/110_049709.png', '_out_16rl_custom2/110_049710.png', '_out_16rl_custom2/110_049711.png', '_out_16rl_custom2/110_049712.png', '_out_16rl_custom2/110_049714.png', '_out_16rl_custom2/110_049715.png', '_out_16rl_custom2/110_049716.png', '_out_16rl_custom2/110_049717.png', '_out_16rl_custom2/110_049719.png', '_out_16rl_custom2/110_049720.png', '_out_16rl_custom2/110_049722.png', '_out_16rl_custom2/110_049723.png', '_out_16rl_custom2/110_049724.png', '_out_16rl_custom2/110_049726.png', '_out_16rl_custom2/110_049728.png', '_out_16rl_custom2/110_049729.png', '_out_16rl_custom2/110_049730.png', '_out_16rl_custom2/110_049731.png', '_out_16rl_custom2/110_049732.png', '_out_16rl_custom2/110_049734.png', '_out_16rl_custom2/110_049736.png', '_out_16rl_custom2/110_049737.png', '_out_16rl_custom2/110_049738.png', '_out_16rl_custom2/110_049739.png', '_out_16rl_custom2/110_049741.png', '_out_16rl_custom2/110_049742.png', '_out_16rl_custom2/110_049743.png', '_out_16rl_custom2/110_049745.png', '_out_16rl_custom2/110_049746.png', '_out_16rl_custom2/110_049748.png', '_out_16rl_custom2/110_049749.png', '_out_16rl_custom2/110_049751.png', '_out_16rl_custom2/110_049752.png', '_out_16rl_custom2/110_049753.png', '_out_16rl_custom2/110_049754.png', '_out_16rl_custom2/110_049755.png', '_out_16rl_custom2/110_049756.png', '_out_16rl_custom2/110_049757.png', '_out_16rl_custom2/110_049758.png', '_out_16rl_custom2/110_049759.png', '_out_16rl_custom2/110_049760.png', '_out_16rl_custom2/110_049761.png', '_out_16rl_custom2/110_049762.png', '_out_16rl_custom2/110_049763.png', '_out_16rl_custom2/110_049764.png', '_out_16rl_custom2/110_049765.png', '_out_16rl_custom2/110_049767.png', '_out_16rl_custom2/110_049768.png', '_out_16rl_custom2/110_049769.png', '_out_16rl_custom2/110_049770.png', '_out_16rl_custom2/110_049771.png', '_out_16rl_custom2/110_049772.png', '_out_16rl_custom2/110_049773.png', '_out_16rl_custom2/110_049774.png', '_out_16rl_custom2/110_049775.png', '_out_16rl_custom2/110_049776.png', '_out_16rl_custom2/110_049778.png', '_out_16rl_custom2/110_049779.png', '_out_16rl_custom2/110_049780.png', '_out_16rl_custom2/110_049781.png', '_out_16rl_custom2/110_049783.png', '_out_16rl_custom2/110_049784.png', '_out_16rl_custom2/110_049785.png', '_out_16rl_custom2/110_049786.png', '_out_16rl_custom2/110_049787.png', '_out_16rl_custom2/110_049788.png', '_out_16rl_custom2/110_049789.png', '_out_16rl_custom2/110_049790.png', '_out_16rl_custom2/110_049791.png', '_out_16rl_custom2/110_049793.png', '_out_16rl_custom2/110_049794.png', '_out_16rl_custom2/110_049795.png', '_out_16rl_custom2/110_049796.png', '_out_16rl_custom2/110_049797.png', '_out_16rl_custom2/110_049798.png', '_out_16rl_custom2/110_049799.png', '_out_16rl_custom2/110_049800.png', '_out_16rl_custom2/110_049801.png', '_out_16rl_custom2/110_049802.png', '_out_16rl_custom2/110_049803.png', '_out_16rl_custom2/110_049804.png', '_out_16rl_custom2/110_049805.png', '_out_16rl_custom2/110_049806.png', '_out_16rl_custom2/110_049807.png', '_out_16rl_custom2/110_049808.png', '_out_16rl_custom2/110_049809.png', '_out_16rl_custom2/110_049810.png', '_out_16rl_custom2/110_049811.png', '_out_16rl_custom2/110_049812.png', '_out_16rl_custom2/110_049813.png', '_out_16rl_custom2/110_049815.png', '_out_16rl_custom2/110_049816.png', '_out_16rl_custom2/110_049817.png', '_out_16rl_custom2/110_049818.png', '_out_16rl_custom2/110_049819.png', '_out_16rl_custom2/110_049820.png', '_out_16rl_custom2/110_049821.png', '_out_16rl_custom2/110_049823.png', '_out_16rl_custom2/110_049824.png', '_out_16rl_custom2/110_049825.png', '_out_16rl_custom2/110_049826.png', '_out_16rl_custom2/110_049827.png', '_out_16rl_custom2/110_049828.png', '_out_16rl_custom2/110_049829.png', '_out_16rl_custom2/110_049830.png', '_out_16rl_custom2/110_049831.png', '_out_16rl_custom2/110_049832.png', '_out_16rl_custom2/110_049833.png', '_out_16rl_custom2/110_049834.png', '_out_16rl_custom2/110_049835.png', '_out_16rl_custom2/110_049836.png', '_out_16rl_custom2/110_049837.png', '_out_16rl_custom2/110_049838.png', '_out_16rl_custom2/110_049839.png', '_out_16rl_custom2/110_049840.png', '_out_16rl_custom2/110_049842.png', '_out_16rl_custom2/110_049844.png', '_out_16rl_custom2/110_049845.png', '_out_16rl_custom2/110_049847.png', '_out_16rl_custom2/110_049848.png', '_out_16rl_custom2/110_049849.png', '_out_16rl_custom2/110_049850.png', '_out_16rl_custom2/110_049852.png', '_out_16rl_custom2/110_049853.png', '_out_16rl_custom2/110_049855.png', '_out_16rl_custom2/110_049856.png', '_out_16rl_custom2/110_049857.png', '_out_16rl_custom2/110_049858.png', '_out_16rl_custom2/110_049859.png', '_out_16rl_custom2/110_049860.png', '_out_16rl_custom2/110_049861.png', '_out_16rl_custom2/110_049862.png', '_out_16rl_custom2/110_049863.png', '_out_16rl_custom2/110_049864.png', '_out_16rl_custom2/110_049865.png', '_out_16rl_custom2/110_049866.png', '_out_16rl_custom2/110_049867.png', '_out_16rl_custom2/110_049869.png', '_out_16rl_custom2/110_049870.png', '_out_16rl_custom2/110_049872.png', '_out_16rl_custom2/110_049874.png', '_out_16rl_custom2/110_049875.png', '_out_16rl_custom2/110_049877.png', '_out_16rl_custom2/110_049878.png', '_out_16rl_custom2/110_049879.png', '_out_16rl_custom2/110_049880.png', '_out_16rl_custom2/110_049881.png', '_out_16rl_custom2/110_049882.png', '_out_16rl_custom2/110_049884.png', '_out_16rl_custom2/110_049885.png', '_out_16rl_custom2/110_049886.png', '_out_16rl_custom2/110_049888.png', '_out_16rl_custom2/110_049889.png', '_out_16rl_custom2/110_049890.png', '_out_16rl_custom2/110_049891.png', '_out_16rl_custom2/110_049892.png', '_out_16rl_custom2/110_049893.png', '_out_16rl_custom2/110_049895.png', '_out_16rl_custom2/110_049896.png', '_out_16rl_custom2/110_049897.png', '_out_16rl_custom2/110_049898.png', '_out_16rl_custom2/110_049899.png', '_out_16rl_custom2/110_049900.png', '_out_16rl_custom2/110_049901.png', '_out_16rl_custom2/110_049902.png', '_out_16rl_custom2/110_049903.png', '_out_16rl_custom2/110_049904.png', '_out_16rl_custom2/110_049905.png', '_out_16rl_custom2/110_049906.png', '_out_16rl_custom2/110_049907.png', '_out_16rl_custom2/110_049908.png', '_out_16rl_custom2/110_049910.png', '_out_16rl_custom2/110_049911.png', '_out_16rl_custom2/110_049912.png', '_out_16rl_custom2/110_049913.png', '_out_16rl_custom2/110_049914.png', '_out_16rl_custom2/110_049915.png', '_out_16rl_custom2/110_049917.png', '_out_16rl_custom2/110_049919.png', '_out_16rl_custom2/110_049920.png', '_out_16rl_custom2/110_049921.png', '_out_16rl_custom2/110_049922.png', '_out_16rl_custom2/110_049923.png', '_out_16rl_custom2/110_049924.png', '_out_16rl_custom2/110_049926.png', '_out_16rl_custom2/110_049927.png', '_out_16rl_custom2/110_049928.png', '_out_16rl_custom2/110_049929.png', '_out_16rl_custom2/110_049930.png', '_out_16rl_custom2/110_049931.png', '_out_16rl_custom2/110_049932.png', '_out_16rl_custom2/110_049933.png', '_out_16rl_custom2/110_049934.png', '_out_16rl_custom2/110_049936.png', '_out_16rl_custom2/110_049938.png', '_out_16rl_custom2/110_049940.png', '_out_16rl_custom2/110_049941.png', '_out_16rl_custom2/110_049942.png', '_out_16rl_custom2/110_049943.png', '_out_16rl_custom2/110_049944.png', '_out_16rl_custom2/110_049945.png', '_out_16rl_custom2/110_049946.png', '_out_16rl_custom2/110_049947.png', '_out_16rl_custom2/110_049948.png', '_out_16rl_custom2/110_049949.png', '_out_16rl_custom2/110_049951.png', '_out_16rl_custom2/110_049952.png', '_out_16rl_custom2/110_049953.png', '_out_16rl_custom2/110_049954.png', '_out_16rl_custom2/110_049955.png', '_out_16rl_custom2/110_049956.png', '_out_16rl_custom2/110_049957.png', '_out_16rl_custom2/110_049958.png', '_out_16rl_custom2/110_049959.png', '_out_16rl_custom2/110_049960.png', '_out_16rl_custom2/110_049961.png', '_out_16rl_custom2/110_049962.png', '_out_16rl_custom2/110_049963.png', '_out_16rl_custom2/110_049964.png', '_out_16rl_custom2/110_049965.png', '_out_16rl_custom2/110_049966.png', '_out_16rl_custom2/110_049967.png', '_out_16rl_custom2/110_049968.png', '_out_16rl_custom2/110_049969.png', '_out_16rl_custom2/110_049970.png', '_out_16rl_custom2/110_049972.png', '_out_16rl_custom2/110_049973.png', '_out_16rl_custom2/110_049974.png', '_out_16rl_custom2/110_049976.png', '_out_16rl_custom2/110_049977.png', '_out_16rl_custom2/110_049978.png', '_out_16rl_custom2/110_049979.png', '_out_16rl_custom2/110_049980.png', '_out_16rl_custom2/110_049981.png', '_out_16rl_custom2/110_049983.png', '_out_16rl_custom2/110_049984.png', '_out_16rl_custom2/110_049985.png', '_out_16rl_custom2/110_049986.png', '_out_16rl_custom2/110_049987.png', '_out_16rl_custom2/110_049988.png', '_out_16rl_custom2/110_049989.png', '_out_16rl_custom2/110_049990.png', '_out_16rl_custom2/110_049991.png', '_out_16rl_custom2/110_049992.png', '_out_16rl_custom2/110_049993.png', '_out_16rl_custom2/110_049994.png', '_out_16rl_custom2/110_049995.png', '_out_16rl_custom2/110_049996.png', '_out_16rl_custom2/110_049997.png', '_out_16rl_custom2/110_049998.png', '_out_16rl_custom2/110_049999.png', '_out_16rl_custom2/110_050000.png', '_out_16rl_custom2/110_050001.png', '_out_16rl_custom2/110_050002.png', '_out_16rl_custom2/110_050003.png', '_out_16rl_custom2/110_050004.png', '_out_16rl_custom2/110_050005.png', '_out_16rl_custom2/110_050006.png', '_out_16rl_custom2/110_050008.png', '_out_16rl_custom2/110_050010.png', '_out_16rl_custom2/110_050011.png', '_out_16rl_custom2/110_050012.png', '_out_16rl_custom2/110_050014.png', '_out_16rl_custom2/110_050015.png', '_out_16rl_custom2/110_050016.png', '_out_16rl_custom2/110_050017.png', '_out_16rl_custom2/110_050018.png', '_out_16rl_custom2/110_050019.png', '_out_16rl_custom2/110_050020.png', '_out_16rl_custom2/110_050021.png', '_out_16rl_custom2/110_050022.png', '_out_16rl_custom2/110_050023.png', '_out_16rl_custom2/110_050024.png', '_out_16rl_custom2/110_050025.png', '_out_16rl_custom2/110_050026.png', '_out_16rl_custom2/110_050027.png', '_out_16rl_custom2/110_050028.png', '_out_16rl_custom2/110_050029.png', '_out_16rl_custom2/110_050030.png', '_out_16rl_custom2/110_050031.png', '_out_16rl_custom2/110_050032.png', '_out_16rl_custom2/110_050033.png', '_out_16rl_custom2/110_050035.png', '_out_16rl_custom2/110_050036.png', '_out_16rl_custom2/110_050038.png', '_out_16rl_custom2/110_050039.png', '_out_16rl_custom2/110_050040.png', '_out_16rl_custom2/110_050041.png', '_out_16rl_custom2/110_050042.png', '_out_16rl_custom2/110_050043.png', '_out_16rl_custom2/110_050044.png', '_out_16rl_custom2/110_050046.png', '_out_16rl_custom2/110_050047.png', '_out_16rl_custom2/110_050048.png', '_out_16rl_custom2/110_050050.png', '_out_16rl_custom2/110_050051.png', '_out_16rl_custom2/110_050052.png', '_out_16rl_custom2/110_050053.png', '_out_16rl_custom2/110_050054.png', '_out_16rl_custom2/110_050055.png', '_out_16rl_custom2/110_050056.png', '_out_16rl_custom2/110_050057.png', '_out_16rl_custom2/110_050059.png', '_out_16rl_custom2/110_050061.png', '_out_16rl_custom2/110_050062.png', '_out_16rl_custom2/110_050063.png', '_out_16rl_custom2/110_050064.png', '_out_16rl_custom2/110_050065.png', '_out_16rl_custom2/110_050066.png', '_out_16rl_custom2/110_050068.png', '_out_16rl_custom2/110_050069.png', '_out_16rl_custom2/110_050070.png', '_out_16rl_custom2/110_050071.png', '_out_16rl_custom2/110_050072.png', '_out_16rl_custom2/110_050073.png', '_out_16rl_custom2/110_050074.png', '_out_16rl_custom2/110_050075.png', '_out_16rl_custom2/110_050076.png', '_out_16rl_custom2/110_050077.png', '_out_16rl_custom2/110_050078.png', '_out_16rl_custom2/110_050079.png', '_out_16rl_custom2/110_050080.png', '_out_16rl_custom2/110_050082.png', '_out_16rl_custom2/110_050083.png', '_out_16rl_custom2/110_050085.png', '_out_16rl_custom2/110_050086.png', '_out_16rl_custom2/110_050087.png', '_out_16rl_custom2/110_050088.png', '_out_16rl_custom2/110_050089.png', '_out_16rl_custom2/110_050090.png', '_out_16rl_custom2/110_050091.png', '_out_16rl_custom2/110_050093.png', '_out_16rl_custom2/110_050094.png', '_out_16rl_custom2/110_050095.png', '_out_16rl_custom2/110_050096.png', '_out_16rl_custom2/110_050098.png', '_out_16rl_custom2/110_050099.png', '_out_16rl_custom2/110_050100.png', '_out_16rl_custom2/110_050101.png', '_out_16rl_custom2/110_050102.png', '_out_16rl_custom2/110_050103.png', '_out_16rl_custom2/110_050104.png', '_out_16rl_custom2/110_050105.png', '_out_16rl_custom2/110_050106.png', '_out_16rl_custom2/110_050107.png', '_out_16rl_custom2/110_050108.png', '_out_16rl_custom2/110_050109.png', '_out_16rl_custom2/110_050110.png', '_out_16rl_custom2/110_050111.png', '_out_16rl_custom2/110_050112.png', '_out_16rl_custom2/110_050113.png', '_out_16rl_custom2/110_050114.png', '_out_16rl_custom2/110_050115.png', '_out_16rl_custom2/110_050116.png', '_out_16rl_custom2/110_050117.png', '_out_16rl_custom2/110_050119.png', '_out_16rl_custom2/110_050120.png', '_out_16rl_custom2/110_050121.png', '_out_16rl_custom2/110_050122.png', '_out_16rl_custom2/110_050124.png', '_out_16rl_custom2/110_050125.png', '_out_16rl_custom2/110_050126.png', '_out_16rl_custom2/110_050127.png', '_out_16rl_custom2/110_050128.png', '_out_16rl_custom2/110_050129.png', '_out_16rl_custom2/110_050130.png', '_out_16rl_custom2/110_050131.png', '_out_16rl_custom2/110_050132.png', '_out_16rl_custom2/110_050133.png', '_out_16rl_custom2/110_050134.png', '_out_16rl_custom2/110_050135.png', '_out_16rl_custom2/110_050136.png', '_out_16rl_custom2/110_050138.png', '_out_16rl_custom2/110_050139.png', '_out_16rl_custom2/110_050140.png', '_out_16rl_custom2/110_050141.png', '_out_16rl_custom2/110_050142.png', '_out_16rl_custom2/110_050143.png', '_out_16rl_custom2/110_050144.png', '_out_16rl_custom2/110_050145.png', '_out_16rl_custom2/110_050146.png', '_out_16rl_custom2/110_050147.png', '_out_16rl_custom2/110_050148.png', '_out_16rl_custom2/110_050149.png', '_out_16rl_custom2/110_050150.png', '_out_16rl_custom2/110_050151.png', '_out_16rl_custom2/110_050152.png', '_out_16rl_custom2/110_050154.png', '_out_16rl_custom2/110_050155.png', '_out_16rl_custom2/110_050156.png', '_out_16rl_custom2/110_050157.png', '_out_16rl_custom2/110_050158.png', '_out_16rl_custom2/110_050159.png', '_out_16rl_custom2/110_050160.png', '_out_16rl_custom2/110_050162.png', '_out_16rl_custom2/110_050163.png', '_out_16rl_custom2/110_050164.png', '_out_16rl_custom2/110_050165.png', '_out_16rl_custom2/110_050166.png', '_out_16rl_custom2/110_050167.png', '_out_16rl_custom2/110_050168.png', '_out_16rl_custom2/110_050169.png', '_out_16rl_custom2/110_050170.png', '_out_16rl_custom2/110_050172.png', '_out_16rl_custom2/110_050174.png', '_out_16rl_custom2/110_050176.png', '_out_16rl_custom2/110_050177.png', '_out_16rl_custom2/110_050179.png', '_out_16rl_custom2/110_050180.png', '_out_16rl_custom2/110_050182.png', '_out_16rl_custom2/110_050183.png', '_out_16rl_custom2/110_050184.png', '_out_16rl_custom2/110_050185.png', '_out_16rl_custom2/110_050186.png', '_out_16rl_custom2/110_050187.png', '_out_16rl_custom2/110_050188.png', '_out_16rl_custom2/110_050190.png', '_out_16rl_custom2/110_050191.png', '_out_16rl_custom2/110_050192.png', '_out_16rl_custom2/110_050193.png', '_out_16rl_custom2/110_050195.png', '_out_16rl_custom2/110_050196.png', '_out_16rl_custom2/110_050197.png', '_out_16rl_custom2/110_050199.png', '_out_16rl_custom2/110_050200.png', '_out_16rl_custom2/110_050201.png', '_out_16rl_custom2/110_050202.png', '_out_16rl_custom2/110_050203.png', '_out_16rl_custom2/110_050204.png', '_out_16rl_custom2/110_050206.png', '_out_16rl_custom2/110_050207.png', '_out_16rl_custom2/110_050208.png', '_out_16rl_custom2/110_050209.png', '_out_16rl_custom2/110_050210.png', '_out_16rl_custom2/110_050211.png', '_out_16rl_custom2/110_050212.png', '_out_16rl_custom2/110_050214.png', '_out_16rl_custom2/110_050215.png', '_out_16rl_custom2/110_050217.png', '_out_16rl_custom2/110_050218.png', '_out_16rl_custom2/110_050219.png', '_out_16rl_custom2/110_050220.png', '_out_16rl_custom2/110_050221.png', '_out_16rl_custom2/110_050222.png', '_out_16rl_custom2/110_050224.png', '_out_16rl_custom2/110_050225.png', '_out_16rl_custom2/110_050227.png', '_out_16rl_custom2/110_050228.png', '_out_16rl_custom2/110_050229.png', '_out_16rl_custom2/110_050230.png', '_out_16rl_custom2/110_050231.png', '_out_16rl_custom2/110_050232.png', '_out_16rl_custom2/110_050233.png', '_out_16rl_custom2/110_050234.png', '_out_16rl_custom2/110_050235.png', '_out_16rl_custom2/110_050236.png', '_out_16rl_custom2/110_050237.png', '_out_16rl_custom2/110_050238.png', '_out_16rl_custom2/110_050239.png', '_out_16rl_custom2/110_050240.png', '_out_16rl_custom2/110_050241.png', '_out_16rl_custom2/110_050242.png', '_out_16rl_custom2/110_050243.png', '_out_16rl_custom2/110_050244.png', '_out_16rl_custom2/110_050245.png', '_out_16rl_custom2/110_050246.png', '_out_16rl_custom2/110_050247.png', '_out_16rl_custom2/110_050248.png', '_out_16rl_custom2/110_050249.png', '_out_16rl_custom2/110_050250.png', '_out_16rl_custom2/110_050251.png', '_out_16rl_custom2/110_050252.png', '_out_16rl_custom2/110_050253.png', '_out_16rl_custom2/110_050254.png', '_out_16rl_custom2/110_050255.png', '_out_16rl_custom2/110_050256.png', '_out_16rl_custom2/110_050257.png', '_out_16rl_custom2/110_050258.png', '_out_16rl_custom2/110_050259.png', '_out_16rl_custom2/110_050261.png', '_out_16rl_custom2/110_050262.png', '_out_16rl_custom2/110_050263.png', '_out_16rl_custom2/110_050264.png', '_out_16rl_custom2/110_050265.png', '_out_16rl_custom2/110_050266.png', '_out_16rl_custom2/110_050267.png', '_out_16rl_custom2/110_050268.png', '_out_16rl_custom2/110_050269.png', '_out_16rl_custom2/110_050270.png', '_out_16rl_custom2/110_050271.png', '_out_16rl_custom2/110_050272.png', '_out_16rl_custom2/110_050273.png', '_out_16rl_custom2/110_050274.png', '_out_16rl_custom2/110_050275.png', '_out_16rl_custom2/110_050276.png', '_out_16rl_custom2/110_050277.png', '_out_16rl_custom2/110_050278.png', '_out_16rl_custom2/110_050279.png', '_out_16rl_custom2/110_050280.png', '_out_16rl_custom2/110_050282.png', '_out_16rl_custom2/110_050283.png', '_out_16rl_custom2/110_050284.png', '_out_16rl_custom2/110_050286.png', '_out_16rl_custom2/110_050287.png', '_out_16rl_custom2/110_050288.png', '_out_16rl_custom2/110_050289.png', '_out_16rl_custom2/110_050290.png', '_out_16rl_custom2/110_050291.png', '_out_16rl_custom2/110_050292.png', '_out_16rl_custom2/110_050293.png', '_out_16rl_custom2/110_050294.png', '_out_16rl_custom2/110_050295.png', '_out_16rl_custom2/110_050296.png', '_out_16rl_custom2/110_050297.png', '_out_16rl_custom2/110_050299.png', '_out_16rl_custom2/110_050300.png', '_out_16rl_custom2/110_050301.png', '_out_16rl_custom2/110_050302.png', '_out_16rl_custom2/110_050303.png', '_out_16rl_custom2/110_050305.png', '_out_16rl_custom2/110_050306.png', '_out_16rl_custom2/110_050307.png', '_out_16rl_custom2/110_050308.png', '_out_16rl_custom2/110_050310.png', '_out_16rl_custom2/110_050311.png', '_out_16rl_custom2/110_050312.png', '_out_16rl_custom2/110_050314.png', '_out_16rl_custom2/110_050315.png', '_out_16rl_custom2/110_050316.png', '_out_16rl_custom2/110_050318.png', '_out_16rl_custom2/110_050319.png', '_out_16rl_custom2/110_050320.png', '_out_16rl_custom2/110_050322.png', '_out_16rl_custom2/110_050324.png', '_out_16rl_custom2/110_050325.png', '_out_16rl_custom2/110_050326.png', '_out_16rl_custom2/110_050327.png', '_out_16rl_custom2/110_050328.png', '_out_16rl_custom2/110_050329.png', '_out_16rl_custom2/110_050330.png', '_out_16rl_custom2/110_050331.png', '_out_16rl_custom2/110_050332.png', '_out_16rl_custom2/110_050333.png', '_out_16rl_custom2/110_050334.png', '_out_16rl_custom2/110_050335.png', '_out_16rl_custom2/110_050336.png', '_out_16rl_custom2/110_050337.png', '_out_16rl_custom2/110_050338.png', '_out_16rl_custom2/110_050339.png', '_out_16rl_custom2/110_050340.png', '_out_16rl_custom2/110_050341.png', '_out_16rl_custom2/110_050342.png', '_out_16rl_custom2/110_050343.png', '_out_16rl_custom2/110_050344.png', '_out_16rl_custom2/110_050345.png', '_out_16rl_custom2/110_050346.png', '_out_16rl_custom2/110_050347.png', '_out_16rl_custom2/110_050348.png', '_out_16rl_custom2/110_050349.png', '_out_16rl_custom2/110_050350.png', '_out_16rl_custom2/110_050351.png', '_out_16rl_custom2/110_050352.png', '_out_16rl_custom2/110_050353.png', '_out_16rl_custom2/110_050354.png', '_out_16rl_custom2/110_050355.png', '_out_16rl_custom2/110_050356.png', '_out_16rl_custom2/110_050357.png', '_out_16rl_custom2/110_050358.png', '_out_16rl_custom2/110_050360.png', '_out_16rl_custom2/110_050361.png', '_out_16rl_custom2/110_050362.png', '_out_16rl_custom2/110_050363.png', '_out_16rl_custom2/110_050365.png', '_out_16rl_custom2/110_050366.png', '_out_16rl_custom2/110_050368.png', '_out_16rl_custom2/110_050369.png', '_out_16rl_custom2/110_050370.png', '_out_16rl_custom2/110_050371.png', '_out_16rl_custom2/110_050372.png', '_out_16rl_custom2/110_050373.png', '_out_16rl_custom2/110_050374.png', '_out_16rl_custom2/110_050375.png', '_out_16rl_custom2/110_050376.png', '_out_16rl_custom2/110_050377.png', '_out_16rl_custom2/110_050378.png', '_out_16rl_custom2/110_050379.png', '_out_16rl_custom2/110_050380.png', '_out_16rl_custom2/110_050381.png', '_out_16rl_custom2/110_050382.png', '_out_16rl_custom2/110_050383.png', '_out_16rl_custom2/110_050384.png', '_out_16rl_custom2/110_050385.png', '_out_16rl_custom2/110_050386.png', '_out_16rl_custom2/110_050387.png', '_out_16rl_custom2/110_050388.png', '_out_16rl_custom2/110_050389.png', '_out_16rl_custom2/110_050390.png', '_out_16rl_custom2/110_050392.png', '_out_16rl_custom2/110_050393.png', '_out_16rl_custom2/110_050394.png', '_out_16rl_custom2/110_050395.png', '_out_16rl_custom2/110_050396.png', '_out_16rl_custom2/110_050397.png', '_out_16rl_custom2/110_050398.png', '_out_16rl_custom2/110_050399.png', '_out_16rl_custom2/110_050401.png', '_out_16rl_custom2/110_050402.png', '_out_16rl_custom2/110_050403.png', '_out_16rl_custom2/110_050405.png', '_out_16rl_custom2/110_050406.png', '_out_16rl_custom2/110_050407.png', '_out_16rl_custom2/110_050409.png', '_out_16rl_custom2/110_050410.png', '_out_16rl_custom2/110_050411.png', '_out_16rl_custom2/110_050412.png', '_out_16rl_custom2/110_050413.png', '_out_16rl_custom2/110_050415.png', '_out_16rl_custom2/110_050416.png', '_out_16rl_custom2/110_050418.png', '_out_16rl_custom2/110_050419.png', '_out_16rl_custom2/110_050420.png', '_out_16rl_custom2/110_050421.png', '_out_16rl_custom2/110_050422.png', '_out_16rl_custom2/110_050424.png', '_out_16rl_custom2/110_050425.png', '_out_16rl_custom2/110_050426.png', '_out_16rl_custom2/110_050428.png', '_out_16rl_custom2/110_050430.png', '_out_16rl_custom2/110_050431.png', '_out_16rl_custom2/110_050433.png', '_out_16rl_custom2/110_050434.png', '_out_16rl_custom2/110_050435.png', '_out_16rl_custom2/110_050436.png', '_out_16rl_custom2/110_050437.png', '_out_16rl_custom2/110_050439.png', '_out_16rl_custom2/110_050441.png', '_out_16rl_custom2/110_050442.png', '_out_16rl_custom2/110_050444.png', '_out_16rl_custom2/110_050446.png', '_out_16rl_custom2/110_050447.png', '_out_16rl_custom2/110_050448.png', '_out_16rl_custom2/110_050450.png', '_out_16rl_custom2/110_050451.png', '_out_16rl_custom2/110_050452.png', '_out_16rl_custom2/110_050453.png', '_out_16rl_custom2/110_050454.png', '_out_16rl_custom2/110_050455.png', '_out_16rl_custom2/110_050456.png', '_out_16rl_custom2/110_050457.png', '_out_16rl_custom2/110_050458.png', '_out_16rl_custom2/110_050459.png', '_out_16rl_custom2/110_050460.png', '_out_16rl_custom2/110_050461.png', '_out_16rl_custom2/110_050462.png', '_out_16rl_custom2/110_050464.png', '_out_16rl_custom2/110_050465.png', '_out_16rl_custom2/110_050466.png', '_out_16rl_custom2/110_050468.png', '_out_16rl_custom2/110_050469.png', '_out_16rl_custom2/110_050470.png', '_out_16rl_custom2/110_050471.png', '_out_16rl_custom2/110_050472.png', '_out_16rl_custom2/110_050473.png', '_out_16rl_custom2/110_050474.png', '_out_16rl_custom2/110_050475.png', '_out_16rl_custom2/110_050476.png', '_out_16rl_custom2/110_050477.png', '_out_16rl_custom2/110_050479.png', '_out_16rl_custom2/110_050480.png', '_out_16rl_custom2/110_050481.png', '_out_16rl_custom2/110_050482.png', '_out_16rl_custom2/110_050483.png', '_out_16rl_custom2/110_050484.png', '_out_16rl_custom2/110_050485.png', '_out_16rl_custom2/110_050486.png', '_out_16rl_custom2/110_050487.png', '_out_16rl_custom2/110_050489.png', '_out_16rl_custom2/110_050490.png', '_out_16rl_custom2/110_050491.png', '_out_16rl_custom2/110_050492.png', '_out_16rl_custom2/110_050493.png', '_out_16rl_custom2/110_050494.png', '_out_16rl_custom2/110_050495.png', '_out_16rl_custom2/110_050496.png', '_out_16rl_custom2/110_050497.png', '_out_16rl_custom2/110_050498.png', '_out_16rl_custom2/110_050499.png', '_out_16rl_custom2/110_050500.png', '_out_16rl_custom2/110_050501.png', '_out_16rl_custom2/110_050502.png', '_out_16rl_custom2/110_050503.png', '_out_16rl_custom2/110_050504.png', '_out_16rl_custom2/110_050505.png', '_out_16rl_custom2/110_050506.png', '_out_16rl_custom2/110_050507.png', '_out_16rl_custom2/110_050508.png', '_out_16rl_custom2/110_050510.png', '_out_16rl_custom2/110_050511.png', '_out_16rl_custom2/110_050512.png', '_out_16rl_custom2/110_050513.png', '_out_16rl_custom2/110_050514.png', '_out_16rl_custom2/110_050515.png', '_out_16rl_custom2/110_050516.png', '_out_16rl_custom2/110_050518.png', '_out_16rl_custom2/110_050519.png', '_out_16rl_custom2/110_050521.png', '_out_16rl_custom2/110_050522.png', '_out_16rl_custom2/110_050523.png', '_out_16rl_custom2/110_050524.png', '_out_16rl_custom2/110_050525.png', '_out_16rl_custom2/110_050526.png', '_out_16rl_custom2/110_050527.png', '_out_16rl_custom2/110_050528.png', '_out_16rl_custom2/110_050530.png', '_out_16rl_custom2/110_050532.png', '_out_16rl_custom2/110_050533.png', '_out_16rl_custom2/110_050534.png', '_out_16rl_custom2/110_050536.png', '_out_16rl_custom2/110_050537.png', '_out_16rl_custom2/110_050539.png', '_out_16rl_custom2/110_050540.png', '_out_16rl_custom2/110_050542.png', '_out_16rl_custom2/110_050543.png', '_out_16rl_custom2/110_050544.png', '_out_16rl_custom2/110_050545.png', '_out_16rl_custom2/110_050546.png', '_out_16rl_custom2/110_050547.png', '_out_16rl_custom2/110_050548.png', '_out_16rl_custom2/110_050549.png', '_out_16rl_custom2/110_050550.png', '_out_16rl_custom2/110_050551.png', '_out_16rl_custom2/110_050552.png', '_out_16rl_custom2/110_050554.png', '_out_16rl_custom2/110_050555.png', '_out_16rl_custom2/110_050556.png', '_out_16rl_custom2/110_050557.png', '_out_16rl_custom2/110_050558.png', '_out_16rl_custom2/110_050559.png', '_out_16rl_custom2/110_050560.png', '_out_16rl_custom2/110_050561.png', '_out_16rl_custom2/110_050562.png', '_out_16rl_custom2/110_050563.png', '_out_16rl_custom2/110_050565.png', '_out_16rl_custom2/110_050567.png', '_out_16rl_custom2/110_050568.png', '_out_16rl_custom2/110_050570.png', '_out_16rl_custom2/110_050571.png', '_out_16rl_custom2/110_050572.png', '_out_16rl_custom2/110_050573.png', '_out_16rl_custom2/110_050574.png', '_out_16rl_custom2/110_050575.png', '_out_16rl_custom2/110_050576.png', '_out_16rl_custom2/110_050577.png', '_out_16rl_custom2/110_050578.png', '_out_16rl_custom2/110_050579.png', '_out_16rl_custom2/110_050580.png', '_out_16rl_custom2/110_050581.png', '_out_16rl_custom2/110_050583.png', '_out_16rl_custom2/110_050584.png', '_out_16rl_custom2/110_050585.png', '_out_16rl_custom2/110_050586.png', '_out_16rl_custom2/110_050587.png', '_out_16rl_custom2/110_050588.png', '_out_16rl_custom2/110_050589.png', '_out_16rl_custom2/110_050590.png', '_out_16rl_custom2/110_050591.png', '_out_16rl_custom2/110_050592.png', '_out_16rl_custom2/110_050593.png', '_out_16rl_custom2/110_050594.png', '_out_16rl_custom2/110_050595.png', '_out_16rl_custom2/110_050596.png', '_out_16rl_custom2/110_050597.png', '_out_16rl_custom2/110_050598.png', '_out_16rl_custom2/110_050599.png', '_out_16rl_custom2/110_050600.png', '_out_16rl_custom2/110_050601.png', '_out_16rl_custom2/110_050602.png', '_out_16rl_custom2/110_050603.png', '_out_16rl_custom2/110_050604.png', '_out_16rl_custom2/110_050605.png', '_out_16rl_custom2/110_050606.png', '_out_16rl_custom2/110_050607.png', '_out_16rl_custom2/110_050608.png', '_out_16rl_custom2/110_050609.png', '_out_16rl_custom2/110_050611.png', '_out_16rl_custom2/110_050613.png', '_out_16rl_custom2/110_050614.png', '_out_16rl_custom2/110_050615.png', '_out_16rl_custom2/110_050616.png', '_out_16rl_custom2/110_050617.png', '_out_16rl_custom2/110_050618.png', '_out_16rl_custom2/110_050619.png', '_out_16rl_custom2/110_050620.png', '_out_16rl_custom2/110_050621.png', '_out_16rl_custom2/110_050623.png', '_out_16rl_custom2/110_050625.png', '_out_16rl_custom2/110_050626.png', '_out_16rl_custom2/110_050627.png', '_out_16rl_custom2/110_050628.png', '_out_16rl_custom2/110_050629.png', '_out_16rl_custom2/110_050630.png', '_out_16rl_custom2/110_050631.png', '_out_16rl_custom2/110_050633.png', '_out_16rl_custom2/110_050634.png', '_out_16rl_custom2/110_050635.png', '_out_16rl_custom2/110_050636.png', '_out_16rl_custom2/110_050637.png', '_out_16rl_custom2/110_050638.png', '_out_16rl_custom2/110_050639.png', '_out_16rl_custom2/110_050640.png', '_out_16rl_custom2/110_050641.png', '_out_16rl_custom2/110_050642.png', '_out_16rl_custom2/110_050643.png', '_out_16rl_custom2/110_050644.png', '_out_16rl_custom2/110_050645.png', '_out_16rl_custom2/110_050646.png', '_out_16rl_custom2/110_050647.png', '_out_16rl_custom2/110_050648.png', '_out_16rl_custom2/110_050649.png', '_out_16rl_custom2/110_050650.png', '_out_16rl_custom2/110_050651.png', '_out_16rl_custom2/110_050652.png', '_out_16rl_custom2/110_050653.png', '_out_16rl_custom2/110_050654.png', '_out_16rl_custom2/110_050655.png', '_out_16rl_custom2/110_050656.png', '_out_16rl_custom2/110_050657.png', '_out_16rl_custom2/110_050659.png', '_out_16rl_custom2/110_050660.png', '_out_16rl_custom2/110_050661.png', '_out_16rl_custom2/110_050662.png', '_out_16rl_custom2/110_050663.png', '_out_16rl_custom2/110_050664.png', '_out_16rl_custom2/110_050665.png', '_out_16rl_custom2/110_050666.png', '_out_16rl_custom2/110_050668.png', '_out_16rl_custom2/110_050669.png', '_out_16rl_custom2/110_050670.png', '_out_16rl_custom2/110_050671.png', '_out_16rl_custom2/110_050672.png', '_out_16rl_custom2/110_050673.png', '_out_16rl_custom2/110_050674.png', '_out_16rl_custom2/110_050675.png', '_out_16rl_custom2/110_050676.png', '_out_16rl_custom2/110_050677.png', '_out_16rl_custom2/110_050678.png', '_out_16rl_custom2/110_050679.png', '_out_16rl_custom2/110_050680.png', '_out_16rl_custom2/110_050681.png', '_out_16rl_custom2/110_050682.png', '_out_16rl_custom2/110_050683.png', '_out_16rl_custom2/110_050684.png', '_out_16rl_custom2/110_050685.png', '_out_16rl_custom2/110_050686.png', '_out_16rl_custom2/110_050688.png', '_out_16rl_custom2/110_050689.png', '_out_16rl_custom2/110_050690.png', '_out_16rl_custom2/110_050691.png', '_out_16rl_custom2/110_050692.png', '_out_16rl_custom2/110_050693.png', '_out_16rl_custom2/110_050694.png', '_out_16rl_custom2/110_050695.png', '_out_16rl_custom2/110_050696.png', '_out_16rl_custom2/110_050698.png', '_out_16rl_custom2/110_050699.png', '_out_16rl_custom2/110_050700.png', '_out_16rl_custom2/110_050701.png', '_out_16rl_custom2/110_050702.png', '_out_16rl_custom2/110_050704.png', '_out_16rl_custom2/110_050705.png', '_out_16rl_custom2/110_050706.png', '_out_16rl_custom2/110_050707.png', '_out_16rl_custom2/110_050708.png', '_out_16rl_custom2/110_050709.png', '_out_16rl_custom2/110_050710.png', '_out_16rl_custom2/110_050711.png', '_out_16rl_custom2/110_050712.png', '_out_16rl_custom2/110_050713.png', '_out_16rl_custom2/110_050714.png', '_out_16rl_custom2/110_050715.png', '_out_16rl_custom2/110_050717.png', '_out_16rl_custom2/110_050718.png', '_out_16rl_custom2/110_050719.png', '_out_16rl_custom2/110_050720.png', '_out_16rl_custom2/110_050721.png', '_out_16rl_custom2/110_050722.png', '_out_16rl_custom2/110_050724.png', '_out_16rl_custom2/110_050725.png', '_out_16rl_custom2/110_050726.png', '_out_16rl_custom2/110_050727.png', '_out_16rl_custom2/110_050728.png', '_out_16rl_custom2/110_050729.png', '_out_16rl_custom2/110_050730.png', '_out_16rl_custom2/110_050731.png', '_out_16rl_custom2/110_050732.png', '_out_16rl_custom2/110_050733.png', '_out_16rl_custom2/110_050734.png', '_out_16rl_custom2/110_050735.png', '_out_16rl_custom2/110_050736.png', '_out_16rl_custom2/110_050737.png', '_out_16rl_custom2/110_050739.png', '_out_16rl_custom2/110_050740.png', '_out_16rl_custom2/110_050741.png', '_out_16rl_custom2/110_050742.png', '_out_16rl_custom2/110_050743.png', '_out_16rl_custom2/110_050744.png', '_out_16rl_custom2/110_050745.png', '_out_16rl_custom2/110_050746.png', '_out_16rl_custom2/110_050747.png', '_out_16rl_custom2/110_050748.png', '_out_16rl_custom2/110_050749.png', '_out_16rl_custom2/110_050750.png', '_out_16rl_custom2/110_050751.png', '_out_16rl_custom2/110_050752.png', '_out_16rl_custom2/110_050754.png', '_out_16rl_custom2/110_050756.png', '_out_16rl_custom2/110_050757.png', '_out_16rl_custom2/110_050758.png', '_out_16rl_custom2/110_050760.png', '_out_16rl_custom2/110_050761.png', '_out_16rl_custom2/110_050762.png', '_out_16rl_custom2/110_050763.png', '_out_16rl_custom2/110_050764.png', '_out_16rl_custom2/110_050766.png', '_out_16rl_custom2/110_050767.png', '_out_16rl_custom2/110_050768.png', '_out_16rl_custom2/110_050770.png', '_out_16rl_custom2/110_050771.png', '_out_16rl_custom2/110_050772.png', '_out_16rl_custom2/110_050774.png', '_out_16rl_custom2/110_050775.png', '_out_16rl_custom2/110_050776.png', '_out_16rl_custom2/110_050777.png', '_out_16rl_custom2/110_050778.png', '_out_16rl_custom2/110_050779.png', '_out_16rl_custom2/110_050780.png', '_out_16rl_custom2/110_050781.png', '_out_16rl_custom2/110_050782.png', '_out_16rl_custom2/110_050783.png', '_out_16rl_custom2/110_050784.png', '_out_16rl_custom2/110_050785.png', '_out_16rl_custom2/110_050786.png', '_out_16rl_custom2/110_050787.png', '_out_16rl_custom2/110_050788.png', '_out_16rl_custom2/110_050789.png', '_out_16rl_custom2/110_050791.png', '_out_16rl_custom2/110_050793.png', '_out_16rl_custom2/110_050794.png', '_out_16rl_custom2/110_050796.png', '_out_16rl_custom2/110_050797.png', '_out_16rl_custom2/110_050798.png', '_out_16rl_custom2/110_050799.png', '_out_16rl_custom2/110_050800.png', '_out_16rl_custom2/110_050801.png', '_out_16rl_custom2/110_050802.png', '_out_16rl_custom2/110_050803.png', '_out_16rl_custom2/110_050804.png', '_out_16rl_custom2/110_050805.png', '_out_16rl_custom2/110_050806.png', '_out_16rl_custom2/110_050807.png', '_out_16rl_custom2/110_050808.png', '_out_16rl_custom2/110_050809.png', '_out_16rl_custom2/110_050810.png', '_out_16rl_custom2/110_050811.png', '_out_16rl_custom2/110_050813.png', '_out_16rl_custom2/110_050814.png', '_out_16rl_custom2/110_050815.png', '_out_16rl_custom2/110_050816.png', '_out_16rl_custom2/110_050817.png', '_out_16rl_custom2/110_050818.png', '_out_16rl_custom2/110_050819.png', '_out_16rl_custom2/110_050820.png', '_out_16rl_custom2/110_050821.png', '_out_16rl_custom2/110_050822.png', '_out_16rl_custom2/110_050823.png', '_out_16rl_custom2/110_050824.png', '_out_16rl_custom2/110_050825.png', '_out_16rl_custom2/110_050826.png', '_out_16rl_custom2/110_050827.png', '_out_16rl_custom2/110_050828.png', '_out_16rl_custom2/110_050829.png', '_out_16rl_custom2/110_050830.png', '_out_16rl_custom2/110_050831.png', '_out_16rl_custom2/110_050832.png', '_out_16rl_custom2/110_050833.png', '_out_16rl_custom2/110_050835.png', '_out_16rl_custom2/110_050837.png', '_out_16rl_custom2/110_050838.png', '_out_16rl_custom2/110_050840.png', '_out_16rl_custom2/110_050841.png', '_out_16rl_custom2/110_050842.png', '_out_16rl_custom2/110_050843.png', '_out_16rl_custom2/110_050844.png', '_out_16rl_custom2/110_050845.png', '_out_16rl_custom2/110_050846.png', '_out_16rl_custom2/110_050847.png', '_out_16rl_custom2/110_050848.png', '_out_16rl_custom2/110_050849.png', '_out_16rl_custom2/110_050850.png', '_out_16rl_custom2/110_050851.png', '_out_16rl_custom2/110_050852.png', '_out_16rl_custom2/110_050853.png', '_out_16rl_custom2/110_050855.png', '_out_16rl_custom2/110_050856.png', '_out_16rl_custom2/110_050857.png', '_out_16rl_custom2/110_050859.png', '_out_16rl_custom2/110_050860.png', '_out_16rl_custom2/110_050861.png', '_out_16rl_custom2/110_050862.png', '_out_16rl_custom2/110_050863.png', '_out_16rl_custom2/110_050864.png', '_out_16rl_custom2/110_050865.png', '_out_16rl_custom2/110_050866.png', '_out_16rl_custom2/110_050867.png', '_out_16rl_custom2/110_050868.png', '_out_16rl_custom2/110_050869.png', '_out_16rl_custom2/110_050870.png', '_out_16rl_custom2/110_050871.png', '_out_16rl_custom2/110_050872.png', '_out_16rl_custom2/110_050873.png', '_out_16rl_custom2/110_050874.png', '_out_16rl_custom2/110_050875.png', '_out_16rl_custom2/110_050876.png', '_out_16rl_custom2/110_050877.png', '_out_16rl_custom2/110_050878.png', '_out_16rl_custom2/110_050879.png', '_out_16rl_custom2/110_050880.png', '_out_16rl_custom2/110_050881.png', '_out_16rl_custom2/110_050882.png', '_out_16rl_custom2/110_050883.png', '_out_16rl_custom2/110_050884.png', '_out_16rl_custom2/110_050885.png', '_out_16rl_custom2/110_050886.png', '_out_16rl_custom2/110_050887.png', '_out_16rl_custom2/110_050888.png', '_out_16rl_custom2/110_050889.png', '_out_16rl_custom2/110_050890.png', '_out_16rl_custom2/110_050891.png', '_out_16rl_custom2/110_050892.png', '_out_16rl_custom2/110_050893.png', '_out_16rl_custom2/110_050894.png', '_out_16rl_custom2/110_050895.png', '_out_16rl_custom2/110_050897.png', '_out_16rl_custom2/110_050898.png', '_out_16rl_custom2/110_050900.png', '_out_16rl_custom2/110_050901.png', '_out_16rl_custom2/110_050902.png', '_out_16rl_custom2/110_050903.png', '_out_16rl_custom2/110_050904.png', '_out_16rl_custom2/110_050905.png', '_out_16rl_custom2/110_050906.png', '_out_16rl_custom2/110_050907.png', '_out_16rl_custom2/110_050909.png', '_out_16rl_custom2/110_050911.png', '_out_16rl_custom2/110_050913.png', '_out_16rl_custom2/110_050914.png', '_out_16rl_custom2/110_050915.png', '_out_16rl_custom2/110_050916.png', '_out_16rl_custom2/110_050917.png', '_out_16rl_custom2/110_050918.png', '_out_16rl_custom2/110_050919.png', '_out_16rl_custom2/110_050920.png', '_out_16rl_custom2/110_050921.png', '_out_16rl_custom2/110_050922.png', '_out_16rl_custom2/110_050923.png', '_out_16rl_custom2/110_050924.png', '_out_16rl_custom2/110_050926.png', '_out_16rl_custom2/110_050927.png', '_out_16rl_custom2/110_050928.png', '_out_16rl_custom2/110_050929.png', '_out_16rl_custom2/110_050930.png', '_out_16rl_custom2/110_050931.png', '_out_16rl_custom2/110_050932.png', '_out_16rl_custom2/110_050933.png', '_out_16rl_custom2/110_050934.png', '_out_16rl_custom2/110_050935.png', '_out_16rl_custom2/110_050936.png', '_out_16rl_custom2/110_050937.png', '_out_16rl_custom2/110_050938.png', '_out_16rl_custom2/110_050939.png', '_out_16rl_custom2/110_050940.png', '_out_16rl_custom2/110_050941.png', '_out_16rl_custom2/110_050942.png', '_out_16rl_custom2/110_050943.png', '_out_16rl_custom2/110_050944.png', '_out_16rl_custom2/110_050945.png', '_out_16rl_custom2/110_050946.png', '_out_16rl_custom2/110_050947.png', '_out_16rl_custom2/110_050948.png', '_out_16rl_custom2/110_050949.png', '_out_16rl_custom2/110_050950.png', '_out_16rl_custom2/110_050951.png', '_out_16rl_custom2/110_050952.png', '_out_16rl_custom2/110_050953.png', '_out_16rl_custom2/110_050955.png', '_out_16rl_custom2/110_050956.png', '_out_16rl_custom2/110_050958.png', '_out_16rl_custom2/110_050959.png', '_out_16rl_custom2/110_050960.png', '_out_16rl_custom2/110_050961.png', '_out_16rl_custom2/110_050962.png', '_out_16rl_custom2/110_050963.png', '_out_16rl_custom2/110_050964.png', '_out_16rl_custom2/110_050965.png', '_out_16rl_custom2/110_050966.png', '_out_16rl_custom2/110_050967.png', '_out_16rl_custom2/110_050968.png', '_out_16rl_custom2/110_050969.png', '_out_16rl_custom2/110_050970.png', '_out_16rl_custom2/110_050971.png', '_out_16rl_custom2/110_050972.png', '_out_16rl_custom2/110_050973.png', '_out_16rl_custom2/110_050974.png', '_out_16rl_custom2/110_050975.png', '_out_16rl_custom2/110_050976.png', '_out_16rl_custom2/110_050977.png', '_out_16rl_custom2/110_050978.png', '_out_16rl_custom2/110_050979.png', '_out_16rl_custom2/110_050980.png', '_out_16rl_custom2/110_050981.png']
Traceback (most recent call last):
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 515, in <module>
    with open(f'tmp/{idx_episode_saved}.replay_memory', 'rb') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/109.replay_memory'
2023-12-30 22:17:46.980283: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 22:17:47.002397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 22:17:47.002414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 22:17:47.002958: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 22:17:47.006306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 22:17:47.426724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 22:17:47.740685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:47.774564: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:47.774799: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:47.778582: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:47.778795: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:47.778913: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:48.592978: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:48.593166: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:48.593444: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:17:48.593531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4409 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 128, 128, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 64, 64, 64)        256       
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_4 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_5 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_5 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_6 (Conv2D)           (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 16, 16, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_6 (Bat  (None, 16, 16, 64)        256       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 batch_normalization_7 (Bat  (None, 8, 8, 64)          256       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 8, 8, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_8_input (InputLayer  [(None, 128, 128, 3)]     0         
 )                                                               
                                                                 
 conv2d_8 (Conv2D)           (None, 128, 128, 64)      1792      
                                                                 
 max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_8 (Bat  (None, 64, 64, 64)        256       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 64, 64, 64)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 64, 64, 64)        36928     
                                                                 
 max_pooling2d_9 (MaxPoolin  (None, 32, 32, 64)        0         
 g2D)                                                            
                                                                 
 batch_normalization_9 (Bat  (None, 32, 32, 64)        256       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 32, 32, 64)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 64)        36928     
                                                                 
 max_pooling2d_10 (MaxPooli  (None, 16, 16, 64)        0         
 ng2D)                                                           
                                                                 
 batch_normalization_10 (Ba  (None, 16, 16, 64)        256       
 tchNormalization)                                               
                                                                 
 activation_10 (Activation)  (None, 16, 16, 64)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_11 (MaxPooli  (None, 8, 8, 64)          0         
 ng2D)                                                           
                                                                 
 batch_normalization_11 (Ba  (None, 8, 8, 64)          256       
 tchNormalization)                                               
                                                                 
 activation_11 (Activation)  (None, 8, 8, 64)          0         
                                                                 
 flatten_2 (Flatten)         (None, 4096)              0         
                                                                 
 dense_2 (Dense)             (None, 2541)              10410477  
                                                                 
=================================================================
Total params: 10524077 (40.15 MB)
Trainable params: 10523565 (40.14 MB)
Non-trainable params: 512 (2.00 KB)
_________________________________________________________________
None
Models in tmp ['tmp/109.64659.model']
Load model tmp/109.64659.model
Leftover images from failed episode: []
Traceback (most recent call last):
  File "/home/nsambhu/github/carla-race/run/2023_12_18_16rl_custom2.py", line 515, in <module>
    with open(f'tmp/{idx_episode_saved}.replay_memory', 'rb') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/109.replay_memory'
2023-12-30 22:18:20.771659: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-30 22:18:20.795120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-30 22:18:20.795139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-30 22:18:20.795720: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-30 22:18:20.799130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-30 22:18:21.269830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-12-30 22:18:21.573127: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:21.602265: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:21.602400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:21.604953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:21.605090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:21.605209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:22.438376: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:22.438508: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:22.438653: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-12-30 22:18:22.438744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4409 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5
